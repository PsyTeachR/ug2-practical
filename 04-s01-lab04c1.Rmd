## InClass Activity 1

Today we are going to continue our study of distributions and probability to help gain an understanding of how we can end up with an inference about a population from a sample. In the preclass activity, we focused a lot on basic probability and **binomial distributions**. In the lab today, we will focus more on the **normal distribution**; one of the key distributions in Psychology. 

It will be really beneficial to you when it comes to doing the assignment to have run through both the preclass and inclass activities. The assignment for this lab is formative and should not be submitted. This of course does not mean that you should not do it. As will all the formative labs, completing them and understanding them, will benefit you when it comes to completing the summative labs. You may also want to refer to the Lecture series for help. If you are unclear at any stage please do ask; probability is challenging to grasp at first.

`r hide("Portfolio Point - Always be adding to your knowledge in your own words")`
```{block, type ="info"}
Remember to add useful information to you Portfoilio! One of the main reasons we do this is because there is a wealth of research that says the more interactive you are with your learning, the deeper your understanding of the topic will be. This relates to the Craik and Lockhart (1972) levels of processing model you will hear about in lectures. Always make notes about what you are learning to really get an understanding of the concepts. And, most importantly, in your own words!

[Craik, F. I. M., & Lockhart, R. S. (1972). Levels of processing: A framework for memory research. Journal of Verbal Learning and Verbal behavior, 11, 671-684.](https://www.sciencedirect.com/science/article/pii/S002253717280001X)
```
`r unhide()`

### Continuous Data and the Normal Distribution

Many of the variables we will encounter in Psychology will:  

* be **continuous** as opposed to discrete. 
* tend to show a **normal distribution**. 
* look similar to below - the bell-shaped curve - when plotted.

But can you name any? Take a couple of minutes as a group to think of variables that we might encounter that are normally distributed. 

```{r normplot1, eval = TRUE, echo = FALSE, fig.cap='The Normal Distribution'}
# here we use the sequence function to generate our quantiles ranging from -4 to 4 in steps of 0.01
x<-seq(146,194,0.01) 
mean = 170
sd = 7
y = dnorm(x, mean, sd)
ymin<- min(y)
ymax<- max(y)

{plot(x,y, type = "l", col = "black", xlab = "Height (cm)", ylab = "Density (probability)", yaxt='n')
polygon(c(mean,mean), c( ymin, ymax), border ="green")
polygon(c(mean-sd, mean-sd), c(y[x==mean-sd], ymin), border ="blue")
polygon(c(mean-(2*sd),mean-(2*sd)), c( y[x==mean-(2*sd)], ymin), border ="orange")
polygon(c(mean-(3*sd),mean-(3*sd)), c( y[x==mean-(3*sd)], ymin), border ="red")
polygon(c(mean+sd,mean+sd), c( y[x==mean+sd], ymin), border ="blue")
polygon(c(mean+(2*sd),mean+(2*sd)), c( y[x==mean+(2*sd)], ymin), border ="orange")
polygon(c(mean+(3*sd),mean+(3*sd)), c( y[x==mean+(3*sd)], ymin), border ="red")
text(170+1, 0.01, expression(mu), col = "green")
text(mean-sd-2, 0.01, expression(-sigma), col = "blue")
text(mean+sd+2, 0.01, expression(+sigma), col = "blue")
text(mean-(2*sd)-2, 0.01, expression(-2*sigma), col = "orange")
text(mean+(2*sd)+2, 0.01, expression(+2*sigma), col = "orange")
text(mean-(3*sd)-2, 0.01, expression(-3*sigma), col = "red")
text(mean+(3*sd)+2, 0.01, expression(+3*sigma), col = "red")}
```

### Estimating from the Normal Distribution

We won't ask you to create a normal distribution as it is more complicated than the binomial distribution you estimated in the preclass. Unlike coin flips, the outcome in the normal distribution is not just 50/50. Instead, just as with the binomial distribution (and other distributions) there are functions that allow us to estimate the normal distribution and to ask questions about the distribution. These are:

* `dnorm()` - the density function
* `pnorm()` - the probability or distribution function
* `qnorm()` - the quantile function

They work in a similar way to their binomial counterparts. If you are unsure about how a function works you can call the help on it by typing in the console, for example, `?dnorm` or `?dnorm()`, but the brackets aren't essential for the help.
<br>
<span style="font-size: 22px; font-weight: bold; color: var(--green);">Quickfire Questions</span>

* Type in the box the binomial counterpart to `dnorm()`? `r fitb("dbinom()", width = 10, ignore_ws = TRUE)`

* Type in the box the binomial counterpart to `pnorm()`? `r fitb("pbinom()", width = 10, ignore_ws = TRUE)`

* Type in the box the binomial counterpart to `qnorm()`? `r fitb("qbinom()", width = 10, ignore_ws = TRUE)`

`r hide("Explain This - I don't get the answers")`
```{block, type ="info"}
The counterpart functions all start with the same letter, d, p, q, it is just the distribution name that changes, `binom`, `norm`, `t` - though we haven't quite come across the t-distribution yet.  

1. `dbinom()` is the binomial equivalent to `dnorm()`

2. `pbinom()` is the binomial equivalent to `pnorm()`

3. `qbinom()` is the binomial equivalent to `qnorm()`

There is also `rnorm()` and `rbinom()` but we will look at them another time.
```
`r unhide()`

### **`dnorm()`** - The Density Function for the Normal Distribution

Using `dnorm`, like we did with `dbinom`, we can plot a normal distribution. This time however we need: 

* `x`, a vector of quantiles (in other words, values on the x axis) 
* the `mean` of our data
* and standard deviation `sd` of our data. 

We will use IQ as an example. Many Psychologists are interested in studying IQ, perhaps in terms of heritability, or interested in controlling for IQ in their own studies to rule out any effect (e.g. clinical and autism studies).  

#### Task 1: Standard Deviations and IQ Score Distribution {#Ch4InClassQueT1}

1. Copy the below code into a new script and run it. Remember that you will need to call `tidyverse` to your library first. e.g. `library("tidyverse")`. 

This code creates the below plot showing a normal distribution of IQ scores (M = 100, SD = 15) ranging from 40 to 160. These are values considered typical for the general population.

``` {r normplot, message = FALSE, fig.cap = 'Distribution of IQ scores'}
# First we set up our range of values from 40 to 160
IQ_data <- tibble(IQ_range = c(40, 160))

# Then we plot the distribution of IQ_data, where we have M = 100 and SD = 15
ggplot(IQ_data, aes(IQ_range)) + 
  stat_function(fun = dnorm, args = list(mean = 100, sd = 15)) +
  labs(x = "IQ Score", y = "probability") +
  theme_classic()
```

* Which part of the code do you need to change to alter the SD of your plot? `r mcq(c("mean = 100",answer = "sd = 15","(40, 160)"))`

2. Now copy and edit the above code to plot a distribution with `mean = 100` and `sd = 10`, and compare the first plot to your own plot. 
<br>
<span style="font-size: 22px; font-weight: bold; color: var(--pink);">Group Discussion Point</span>

What does changing the `sd` do to the shape of the distribution? Spend a few minutes editing the code and discussing with your group to answer the following questions:

* What happens to the shape of the distribution if you change the `sd` from 10 to 20? `r mcq(c("the distribution gets narrower",answer = "the distribution gets wider"))`

* What happens to the shape of the distribution if you change the `sd` from 10 to 5? `r mcq(c(answer = "the distribution gets narrower","the distribution gets wider"))`

* What does a small or large standard deviation in your sample tell you about the data you have collected?

`r hide("Explain This - I don't get Standard Deviations!")`
```{block, type ="info"}
1. Changing the SD from 10 to 20 means a larger standard deviation so you will have a wider distribution.
2. Changing the SD from 10 to 5 means a smaller standard deviation so you will have a narrower distribution.
3. Smaller SD results in a narrower distribution meaning that the data is less spread out; larger SD results in a wider distribution meaning the data is more spread out.

**A note on the Standard Deviation**:

You will know from your lectures that you can estimate data in two ways: point-estimates and spread estimates. The mean is a point-estimate and condenses all your data down into one data point - it tells you the average value of all your data but tells you nothing about how spread out the data is.  The standard deviation however is a spread estimate and gives you an estimate of how spread out your data is from the mean - it is a measure of the standard deviation from the mean. 

So imagine we are looking at IQ scores and you test 100 people and get a mean of 100 and an SD of 5. This means that the vast majority of your sample will have an IQ around 100 - probably most will fall within 1 SD of the mean, meaning that most of your participants will have an IQ of between 95 and 105. 

Now if you test again and find a mean of 100 and an SD of 20, this means your data is much more spread out. If you take the 1 SD approach again then most of your participants will have an IQ of between 80 and 120. 

So one sample has a very tight range of IQs and the other sample has a very wide range of IQs. All in, from the point-estimate and spread estimate of your data you can tell the shape of your sample distribution.
```
`r unhide()`
<br>
So far so good! But in the above example we told `dnorm()` the values at the limit of our range and it did the rest; we said give us a range of 40 to 160 IQ scores. However, we could plot it another way by telling `dnorm()` the sequence, or range, of values we want and how much precision we want between them.  

#### Task 2: Changing Range and Step Size of The Normal Distribution {#Ch4InClassQueT2}

1. Copy the code below in to your script and run it. 
Here we plot the standard Normal Distribution from -4 to 4 in steps of 0.01. We have also stated a mean of 0 and an sd of 1.

```{r ND-example, include = TRUE, message=FALSE, fig.cap='The Normal Distribution with Mean = 0 and SD = 1'}
ND_data <- tibble(ND_range = seq(-4, 4, 0.01))
ggplot(ND_data, aes(ND_range)) + 
  stat_function(fun = dnorm, args = list(mean = 0, sd = 1)) +
  labs(x = "SD units", y = "probability", title = "The Normal Distribution") +
  theme_classic()
```
<br>
<span style="font-size: 22px; font-weight: bold; color: var(--green);">Quickfire Questions</span>

* Fill in the box to show what you would type to get a range and step size of `-10` to `10` in steps of `.05`:

**ND_data** <- `r fitb(c("tibble(ND_range = seq(-10, 10, 0.05))", "tibble(ND_range = seq(-10, 10, .05))"), width = 40, ignore_ws = TRUE)`

Now that you know what to change, plot the normal distribution with the range of -10 to 10, in steps of 0.05, with a mean of 0, and a standard deviation of 1.

* Compare your new plot the the original one we created. What change is there in the distribution? `r mcq(c("Distribution widens", answer = "No change in distribution", "Distribution narrows"))`

`r hide("Explain This - I don't understand the answer!")`
```{block, type ="info"}
To change the distribution you would write: `ND_data <- tibble(ND_range = seq(-10, 10, 0.05))`

However, when comparing the plots, whilst the plot itself may look thinner, the distribution has not changed. The change in appearance is due to the range of `sd` values which have been extended from -4 and 4 to -10 and 10. The density of values within those values has not changed however and you will see, more clearly in the second plot, that values beyond -3 and 3 are very unlikely.
```
`r unhide()`

### **`pnorm()`** - The Probability or Distribution Function

There is one important difference you should note between continuous and discrete probability distributions. With discrete probability distributions, there are usually a finite number of outcomes over which you take a probability. For instance, with 4 coin flips, there are 5 possible outcomes for the number of heads: 0, 1, 2, 3, 4, 5. We can use `dbinom()` to get the exact probability for each. However, with a truly continuous variable, the number of possible outcomes are infinite, because you not only have 0.01 but also .0000001 and .00000000001 to arbitrary levels of precision. So rather than asking for the probability of a single value, we ask the probability for a **range of values**, which is equal to the area under the curve for that range.

Just as `dnorm()` works like `dbinom()`, `pnorm()` works just like `pbinom()`. So, `pnorm()`, given the `mean` and `sd` of our data, returns the **cumulative density function (cumulative probability)** that a given probability (`p`) lies at a specified **cut-off point and below**, unless of course `lower.tail = FALSE` is specified in which case it is from the **cut-off point and above**.

We will use height to give a concrete example. Say that we test a sample of students (M = 170cm, SD = 7). If we want to calculate the probability that a given student is 150cm or shorter we would do the following:

``` {r pnorm1, results = 'hide'}
# lower.tail = TRUE means lower than and including the value of X
# TRUE is the default so we don't actually need to declare it
pnorm(150, 170, 7, lower.tail = TRUE)
```

This tells us that finding the probability of someone 150cm or shorter in our class is about **p = `r pnorm(150, 170, 7, lower.tail = TRUE)`**; stated differently, we would expect **`r round(pnorm(150, 170, 7, lower.tail = TRUE)*100, 2)`%** of students would be 150cm or shorter. This is a very small probability and suggests that it is pretty unlikely to find someone shorter than 150cm in our class. This is mainly because of how small the standard deviation of our distribution is. Think back to what we said earlier about narrow standard deviations round the mean! 

#### Task 3: Calculating Cumulative Probability of Height {#Ch4InClassQueT3}

1. Edit the `pnorm` code above to calculate the probability that a given student is 190cm **or taller**.

To three decimal places, as in Task 3, what is the probability of a student being 190cm or taller in this class? `r fitb(c("0.002",".002"), ignore_ws = TRUE)`

`r hide("Explain This - I don't understand the answer!")`
```{block, type ="info"}
The answer is .002. See the solution code at the bottom of the page to see how to complete Task 3.

There is a difference in where you need to specify the cut-off point in the `pbinom` (discussed in the preclass activity) and `pnorm` functions for values **above** `x`, i.e. when **`lower.tail = FALSE`**. 

If you had discrete data, say the number of coin flips that result in `heads`, and wanted to calculate the probability above `x`, you would apply `pbinom` and have to specify your cut-off point as **`x-1`** to include `x` in your calculation. For example, to calculate the probability of 4 or more 'heads' occuring in 10 coin flips, you would specify `pbinom(3, 10, 0.5, lower.tail = FALSE)` as `lower.tail` includes the value you state.

For continuous data, however, such as height, you would be applying `pnorm` and therefore can specify your cut-off point simply as **`x`**. In the above example, for the cut-off point of 190, a mean of 170 and standard deviation of 7, you can write `pnorm(190, 170, 7, lower.tail = FALSE)`. The way to think about this is that setting `x` as 189 on a continuous scale, when you only want all values greater than 190, would also include all the possible values between 189 and 190. Setting `x` at 190 starts it at 190.0000000...001. 

This is a tricky difference between `pbinom` and `pnorm` to recall easily, so best include this explanation point in your portfolio to help you carry out the correct analyses in the future!
```
`r unhide()`  

#### Task 4: Using Figures to Calculate Cumulative Probability {#Ch4InClassQueT4}

Have a look at the distribution below:

``` {r pnormplot1, echo = FALSE, fig.cap='The Normal Distribution of Height with the probability of people of 185cm highlighted in purple'}
# students dont see this code, just the shaded output.
x<-seq(146,194,0.01) 
mean = 170
sd = 7
y = dnorm(x, mean, sd)
ymin<- min(y)
ymax<- max(y)
xmin<- min(x)
xmax<- max(x)

{plot(x,y, type = "l", col = "black", xlab = "Height (cm)", ylab ="Probability")
polygon(c( x[x>=185], xmax),  c(ymin, y[x>=185]), col="purple")
text(185+1, 0.01, "185cm", col = "purple")}
```

1. Using the information in the figure, and the mean and SD as above, calculate the probability associated with the shaded area.

`r hide("Helpful Hint")`
```{block, type ="info"}
You already have your mean and standard deviations to input in `pnorm`, look at the shaded area to obtain your cut-off point. What should the `lower.tail` call be set to according to the shaded area?
```
`r unhide()`
<br>
<span style="font-size: 22px; font-weight: bold; color: var(--green);">Quickfire Question</span>

To three decimal places, what is the cumulative probability of the shaded area in Task 4?  `r fitb(c("0.016",".016"), ignore_ws = TRUE)`

`r hide("Explain This - I don't get this answer")`
```{block, type ="info"}
The answer should be .016. See the solution code at the end of the Chapter for Task 4. `lower.tail` is set to FALSE as you want the area to the right.
```
`r unhide()`  

### **`qnorm()`** - The Quantile Function

Using `qnorm()` we can do the inverse of `pnorm()`, and instead of finding out the cumulative probability from a given set of probabilities, we can find a cut-off value given a desired probability. For example, what is the maximum IQ a student would have if they were in the bottom 10% of the IQ distribution (`M = 100` & `SD = 15`)?

``` {r qnorm, results = 'hide'}
qnorm(0.1, 100, 15) 
# note that we need to convert 10% to a probability (0.1).
```

So anyone with an IQ of **`r round(qnorm(0.1, 100, 15), 1)`** or lower would be in the bottom 10%. To recap, we have calculated the **inverse cumulative density function (or inverse of the cumulative probability)** of the **lower tail** of the distribution, with a cut-off of a probability of 0.1 (10%), illustrated in purple below:

``` {r pnormplot2, echo = FALSE, fig.cap='The Normal Distribution of Height with the bottom 10% of heights highlighted in purple'}
x<-seq(40,160,0.01) # here we use the sequence function to generate our quantiles ranging from -4 to 4 in steps of 0.01
mean = 100
sd = 15
y = dnorm(x, mean, sd)
ymin<- min(y)
ymax<- max(y)

xmin<- min(x)
xmax<- max(x)

{plot(x,y, type = "l", col = "black", xlab = "IQ", ylab = "Probability" )
polygon(c( x[x<=80.77673], 80.77673),  c(y[x<=80.77673],ymin ), col="purple")
text(75, 0.0025, "10%", col = "White")}
```

#### Task 5: Using **`pnorm`** and **`qnorm`** to find probability and cut-off Values {#Ch4InClassQueT5}

1. Calculate the lowest IQ score a student must have to be in the top 5% of the above distribution.  

2. **More challenging**: Using the appropriate normal distribution function, calculate the probability that a given student will have an IQ between 105 and 110, on a normal distribution of mean = 100, sd = 15.  

`r hide("Helpful Hint")`
```{block, type ="info"}
Part 1: Remember to include the `lower.tail` call if required! If you are unsure, visualise what you are trying to find (i.e. the lowest IQ score you can have to be in top 5%) by sketching it out on a normal distribution curve. It may help to reverse the question to sound more like the previous example.

Part 2: For the second part, each function, not necessarily `qnorm`, gives one value, so you are looking to do separate calculations for each IQ. Then you have to combine these two values, but are you summing or subtracting them? Is it more or less likely for students to have an IQ that falls between this range than above or below a cut-off?
```
`r unhide()`
<br>
<span style="font-size: 22px; font-weight: bold; color: var(--green);">Quickfire Questions</span>

* To one decimal place, enter your answer for Task 5 part 1: What is the lowest IQ score a student must have to be in the top 5% of the distribution? `r fitb("124.7", ignore_ws = TRUE)`

* To two decimal places, enter your answer for Task 5 part 2: What is the probability that a student will have an IQ between 105 and 110, on a normal distribution of mean = 100, sd = 15? `r fitb(c("0.12",".12"), ignore_ws = TRUE)`
<br>
`r hide("Explain This - I dont get this answer")`
```{block, type ="info"}
1. The question can be rephrased as what value would give you 95% of the distribution - and the answer would be 124.7. See the solution code for Task 5 at the bottom of the page.

2. You could use `pnorm()` to establish the probability of an IQ of 110. And you could use `pnorm()` again to establish the probability of an IQ of 105. The answer is the difference between these two probabilities and should be .12. So a 12% probability. See the solution code for Task 5 at the bottom of the page.
```
`r unhide()`
<br>
<span style="font-size: 22px; font-weight: bold; color: var(--blue);">Job Done - Activity Complete!</span>

You have now recapped probability and the binomial and normal distributions. You should now be ready to complete the Homework Assignment for this lab. **The assignment for this Lab is FORMATIVE and is NOT to be submitted and will NOT count towards the overall grade for this module**. However you are strongly encouraged to do the assignment as it will continue to boost your skills which you will need in future assignments. If you have any questions, please post them on the slack forum under the channel #level2_2018. Finally, don't forget to add any useful information to your Portfolio before you leave it too long and forget.

Excellent work! You are Probably an expert! Now go try the assignment!
