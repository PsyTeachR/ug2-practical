[
["index.html", "ug2-practical Introduction", " ug2-practical psyTeachR 2019-05-10 Introduction Materials for the University of Glasgow School of Psychology Year 2 Research Methods and Statistics Practical Course "],
["starting-with-r-markdown.html", "Lab 1 Starting with R Markdown 1.1 Overview 1.2 PreClass Activiy 1.3 InClass Activiy 1.4 Assignment 1.5 Solutions to Questions", " Lab 1 Starting with R Markdown Welcome to the Level 2 Lab Series 1.1 Overview Over the course of this year's practical lab sessions we will help you learn a whole host of skills and methods based around being a Psychologist, starting right now! A key goal of any researcher is to carry out an experiment and to tell others about it. One of the main ways we as Psychologists do this is through publication of a journal article. There are numerous ways that people combine different software to create a journal article, but a more recent innovation in the field that we want you to know about is creating reports and articles through R Markdown. If you like, you can see an example from a research team in our school in this recent PLOS article. A link within the article methods section (this one - https://osf.io/eb9dq/) allows you to see the one file that creates the whole manuscript. Obviously you wont be writing full journal articles just yet but you will use R Markdown throughout this lab series to do assignments and you could even also use it in other subjects to write reports, or to even make yourself a portfolio of hints, tips, and study aids. We will start by showing you some of skills in using R Markdown efficiently. In this lab you will learn: What is R Markdown? How to create an R Markdown file and knit it. How to add code and edit rules in your R Markdown file. How to format your text. 1.1.1 What is R Markdown? R Markdown (abbreviated as Rmd) is a great way to create dynamic documents through embedded chunks of code. These documents are self-contained and fully reproducible which makes it very easy to share. For more information about R Markdown feel free to have a look at their main webpage sometime: The R Markdown Webpage. The key advantage of R Markdown is that it allows you to write code into a document, along with regular text, and then knit it using the package knitr() to create your document as either a webpage (HTML), a PDF, or Word document (.docx). Explain This - We Knit what? Throughout the labs you will see little tabs that give more information, answers to quick questoins, helpful hints, solutions to tasks, or suggestions for information you want to note down somewhere. You do not have to read them all and you will find they get less as the course progresses, but they might help you if you are stuck on something. Knit is what we say when we want to turn our R Markdown file into either a webpage, PDF, or a Word document. Often in the labs you will hear someone say, &quot;Have you tried knitting it?&quot; or &quot;What happens when you knit it?&quot;. This simply means what happens when you try turning your file into a pdf or webpage.&quot;) 1.1.2 Advantages of using R Markdown The output is one file that includes figures, text and citations. No additional files are needed so it's easy to keep all your work in one place. R code can be put directly into a R Markdown report so it is not necessary to keep your writing (e.g. a Word document) and your analysis (e.g. your R script) separate. Including the R code directly lets others see how you did your analysis - this is a good thing for science! It is both Reproducible and Open! You write your report in normal text so it's not necessary that you learn any new coding such as HTML. 1.1.3 Creating an R Markdown (.Rmd) File In this lab you're going to create your own R Markdown document. Knowing how to do this will: help you with submitting homework assignments, and help you be able to create your own reports using it. If at any point you are unsure about how to do something remember to think about where you can get help. There is an R Markdown Cheatsheet on the top menu under Help &gt;&gt; Cheatsheets or do what we do, Google it. For example, if I forget how to put words in bold, I could simply go to Google and type &quot;rmarkdown bold&quot; and no doubt get a lot of useful hints. Quickfire Questions We have put questions throughout to help you test your knowledge. When you type in or choose the correct answer, the dashed box will change color and become solid. From the following options, why are we creating an R Markdown document instead of simply using an R script? R Markdown can combine report writing and analysis R Scripts can't run code Reproducible Science! Explain This Answer! So there's more than one answer to this question! R Markdown can combine report writing and analysis, providing open access for others to examine data, and create more Reproducible Science. But what about the incorrect answer? R Scripts do in fact run R code as you may remember from Level 1 labs. The key difference is that R Scripts cannot really be used for documentation and creating reports - this is where R Markdown is used to ensure your code can be added to all the other information of your research and can be reproduced by others. 1.1.4 One last thing before beginning! Before working through the rest of this Lab you may want to watch the 20 minute video on the Lab 1 Moodle page {{{WHAT TO DO}}} by Dale Barr reminding you about R, RStudio, and various file formats you will be working with. 1.2 PreClass Activiy Having read the Overview for this Lab, and the reason behind using R, we are now going to show you how to make a reproducible code. If you have a laptop it is best to install R and RStudio on that for you to use. Dale Barr's video gives a reminder of how to install R and Rstudio. If you don't have it installed yet you can just read along today and try it out when you have access to a machine in the labs. 1.2.1 Let's Begin Create a new R Markdown file (.Rmd) by opening Rstudio, and then on the top menu, selecting File &gt;&gt; New File &gt;&gt; R Markdown.... You should now see the following dialog box: Figure 1.1: Starting an R Markdown file Click Document on the left-hand panel and then give your document a Title. This is your file so call it what you want but make sure it is informative to you and your reader. Put your name or your student ID in the Author field as you are the author. For now we will focus on making an HTML output, so make sure that is selected as shown in Figure 1.1 then hit OK when you have done so. You should now have an .Rmd file open in Rstudio. The first thing you will see in your R Markdown file is a header section enclosed at the top and bottom by ---. Technically called the yaml header, this section lists the title, author, date and output format. The layout of the header is very precise and will look like that shown in Figure 1.2, which is currently set to output as HTML. Figure 1.2: An Rmd yaml header By default the file header includes the info shown in Figure 1.2 but there are many other options available. You can learn more about this in your spare time if you like through these links: http://rmarkdown.rstudio.com/html_document_format.html for .html options or http://rmarkdown.rstudio.com/pdf_document_format.html for .pdf options. BUT WAIT!! What if you spelt your name wrong? How would you change this? Explain This - I spelt my name wrong! The long way would be to close the file and start again. The shorter way would be to just correct the info in the header - just remember to keep between the quotes. E.g. &quot;Si Cologe&quot; instead of &quot;Untitled&quot; 1.2.2 Code Chunks Immediately below the header information you will see the default setup code chunk as shown in Figure 1.3. Most of the time you will not edit the information in this chunk and you will add information, text, and code, below it. Figure 1.3: The defualt setup code chunk In RMarkdown you can type any text you want directly in the document just as you would in a word document. However, if you want to include code you need to include it in one of these code chunks similar to Figure 1.3. Code chunks start with a line that contains three backwards apostrophes ` (these are called grave accents - often in the top-left of keyboards), and then a set of curly brackets with the letter r inside: ```{r}``` You will always need both of these parts to create a code chunk: The three back ticks ` are the part of the Rmd file that says this is code being inserted into my document. The {r} part says that you are specifically including R code. The default setup code chunk provides some basic options for your RMarkdown file for when it knits your work. As above, for now, it is best to leave this particular code chunk alone. Instead we will show you how to use RMarkdown by editing the code chunks that come after this default chunk. The next code chunk in your file will look a bit like this: ```{r cars} summary(cars) ``` Within the curly brackets, on the first line of the chunk, the word cars is included after the letter r. This is simply the name or the label for the code chunk and it really could have been called anything. For example, you could have called this code chunk cars1 and a later chunk cars2 to show it was the first and second chunk relating to cars. Whilst it is always advisable you name your code chunks you do not need to name them, however if you do put in names for the chunks do not use the same name twice as this will cause your script to crash when you knit it, e.g. Do not use data and data; instead maybe use personality_data and participant_info or whatever makes sense to what you are doing in the chunk. Explain This - You can crash whilst knitting? Remember knitting just means converting or rendering your file as a pdf, webpage, etc. Crashing means that you had an error in your code that stopped your knitting from working or finishing. You can usually find the problem line of code from the error message you'll see. The second line in the above code chunk is the R code we have written: summary(cars). In this case, we are just asking for a summary() of the inbuilt dataset cars. R has a lot of inbuilt datasets for you to practice on; cars is one of these. The third line closes off the code chunk, again with the three backwards apostrophes. This means that whatever is contained between the first and third lines will be the code that is run. Quickfire Questions From the following options what was the name, or label, of the default setup code chunk (i.e. the first code chunk in an R Markdown file)? include r setup FALSE Explain This Answer If you look at the default setup code chunk you can see the code chunk has the name setup. include=FALSE is a rule which we will explain in a little bit. 1.2.3 Knitting Code Now would be a good time to try knitting your file to see what the code chunks do. You can do this using the Knit button at the top of the RStudio screen: Figure 1.4: The knit button. Clicking this will knit your file. When you click Knit it will ask you to save the file as an .Rmd file. Call the file L2Psych_Lab1_Preclass.Rmd and save it in a folder where you will keep all the information for this lab. When working in the Psychology labs or the University Library you need to save in a location or drive space that you have full access to and can save files to. The best one on campus is your M: drive. If using your own device then anywhere you can save the file should work. Helpful Hint - One folder for all your work cat(&quot;It would be very beneficial to create a folder in your M: drive that will contain all your practical lab work for the rest of Level 2. Maybe something like Psychology Level 2 Lab Work and then have folders within that for each lab, e.g Lab1. The clearer the structure of these folders the easier it will be to find and use your files again! This is important as one thing we will keep telling you to do is Look Back at what you previously did. A good way to think about this is if you have an exam, it isn't helpful to be told the location of your exam is 'Glasgow Uni' (i.e. a large folder of many locations). Instead you would need to be told the specific building (a folder within your larger folder), but more specifically the room number in the building where your exam is taking place (the folder which you are working from). After saving the file a webpage should appear. The first thing to notice is that some lines in the code chunks have disappeared: the ```{r} and the closing ``` in your code chunk have gone. Whenever you knit an RMarkdown file these lines will disappear leaving only the code within. You'll also notice that the output of the code is also now showing in your webpage. In the next section we will show you how to control showing the data or not through adding rules. Figure 1.5: The knitted summary output 1.2.4 Adding Rules to Code Chunks It can often be a good idea or even necessary to show the data or the outcome of a test in your report, for example if you were writing a report and wanted to include a table of results. But what if your code displayed a table that was 10,000 lines long? In that case we might want to not show the output and only show the code. You can do this by including a rule within the first line of your code chunk - your ```{r name, rule = option} line. You have already seen a rule before in the standard default chunk, the include rule, but there are a number of others. To hide the output but show the code we use the results = &quot;hide&quot; rule: Figure 1.6: The results Rule Add this rule into your example code chunk, as shown above, and knit the file again. What happens? Note that there is a comma separating the name of the chunk and the rule. You should now see the code only and not the data. Alternatively, we can Hide the code, but show the ouput by using the echo = FALSE rule: Figure 1.7: The echo Rule In your template Rmd file the rule echo is set to FALSE meaning to show the figure and not the code. Change the rule in your code to echo and set it as TRUE, then knit the file again. What happens? Explain This - Why would I hide my code? Remember from Level 1 where we called in libraries to our environment. The &quot;echo = FALSE&quot; option is useful for commands like library() when you are just calling a package into the library but don't necessarily want to display that in your final report or in your final HTML file. Another example might be if you wanted to make a plot but didn't want to include the code, you just want to show the plot in your report. You might want to hide both the code AND the output by using the include rule: Figure 1.8: The include Rule Change the rule to your example code chunk, as shown above, to include = FALSE and then knit the file again. What happens? Note that here the code still runs. It just does not show you anything. Finally, you can use the eval rule which specifies whether or not you want the code chunk you have written to be evaluated when you knit the RMarkdown file. Evaluated means to run or carry out the code. Here, the eval = FALSE rule will stop the code from being evaluated. The code will be shown because there is no rule stopping it but there will be no output because it won't get evaluated because of the eval rule being FALSE. Figure 1.9: The eval Rule This might be useful in cases where you want to show the code relating to how you programmed your stimuli for an experiment, but you don't necessarily want it to run as part of the RMarkdown file. Quickfire Questions You've got a large dataset of thousands of participant's personality and happiness scores that you want to analyse and present in RMarkdown. You want to show the code you are running in your analysis but not show the output as this would be too much to display. Note that you want the code to run. Type in the box (e.g. rule = set) how you would set the results rule to do this? You create a plot of happiness versus neuroticism scores but you want to hide the code and only show the output. How can you do this? echo = TRUE include = FALSE code = HIDE echo = FALSE Explain This - I don't understand these answers The first answer should be results = &quot;hide&quot; as you want to show the code and run the code but not necessarily show the output of the code. In the second question, include = FALSE technically would hide the code, but this also hides the output! echo = FALSE allows you to still see your plot while hiding the code you want hidden. code = HIDE - if only it were that simple! The aim of these questions aren't to help you memorise these codes (no one can do that!), they're to help you gain a better understanding of how to apply these codes when you come across them in the future. True or False, writing echo = TRUE has the same effect on the output of a code as if you had no echo rule at all: TRUE FALSE Explain This - Echo True or Not at all All of the rules have a default mode. For example, echo, include, and eval are usually by default set to TRUE. As a result, if you don't declare any echo rule, i.e. you don't declare echo = FALSE, then it is the same as declaring echo = TRUE. So no rule means that you are wanting that rule set as TRUE. True or False, there is no difference between the rules results = &quot;hide&quot; and eval = FALSE as they both hide the output: TRUE FALSE Explain This - What's the difference? In the first rule, results = &quot;hide&quot;, the code is evaluated and results are produced but the output is hidden. In the second rule, eval = FALSE, the code is not evaluated and therefor no results or output has been produced. If you need your output for a later part of the code then you would use the results rule. If you don't need the output and just want to show the code as an example then you would use the eval rule. 1.2.5 Adding Inline Code An alternative way to add code to a report is through what is called inline coding. Inline coding is slightly different to code chunks' you don't use a code chunk in fact. Inline coding can be inserted using a back-tick, then the letter r, followed by a space, the code you want to include, then another back-tick. For example, writing `r 2 + 2` would return the answer 4 when you knit the file instead of showing the code. Note that you do not do this inside a code chunk, you do this in line with your text, e.g.: &quot;We ran `r 2+2` people&quot;. Which when knitted becomes: &quot;We ran 4 people&quot;. So inline coding is really useful if you want to do calculations within your text or insert values into text, say from a dataframe, to make an informative sentence. Quickfire Questions You need Two One Three back tick(s) to insert code chunks Why is this inline code, `r 6 * 8` , not going to show the calculated answer when you knit the file? Try editing the code line in Rmarkdown and knitting it to get it to work. You need a space between each back tick and the code Inline code cannot complete calcuations Curly brackets are only needed for code chunks Explain This - Why are these answers correct? All code chunks start and end with three back-ticks. Inline coding does not use the curly brackets around the r. All you need is a back-tick, r, space, code, and a final back-tick. 1.2.6 Formatting the R Markdown File The last thing we want to show you in this preclass activity is how to format your text. When you're not writing in code chunks you can format your document in lots of different ways just like you would in a Word document. The R Markdown cheatsheet provides lots of information about how to do this but we will show you a couple of things that you might want to try out. We can make some text bold by including two ** (two asterisks) at the start and end of the text we want to present in bold font. For example: &quot;We ran **4 people**. Which when knitted becomes: &quot;We ran 4 people&quot;. Now write some text in your Rmd file and put it in bold. Knit the file to check it worked. You could also try use italics by putting a single * (asterisk) at the start and end of the word or sentence. Try this now Finally, you might want to add headings and sub-headings to your file. For example, maybe you are writing a Psychology journal article and want to put in a header for the Introduction, Methods, Results, or Discussion sections. We do this using the # (hashtag) symbol as shown in Figure 1.10. Figure 1.10: Inputting different Header levels using #s Now, type the four main sections found in a Psychology journal article in your RMarkdown file, typing each one in a separate line. These are mentioned above. Knit the file. What do these look like? Now add a different number of #'s before each heading, with a space between the heading and the hashtag (e.g. # Introduction) and knit the file again. What do you notice about the different number of hashtags? Quickfire Questions If * puts words into italics, and ** puts words into bold, type in the box what might you put before (and technically after) a word to put it into italics with bold? True or False: The more '#'s you include, the smaller the header is: TRUE FALSE From the options, the most common order of headings found in a Psychology Journal are: Discussion, Introduction, Methods, Results Discussion, Results, Methods, Introduction Introduction, Methods, Results, Discussion Introduction, Results, Methods, Discussion Explain This - I don't get these answers If * at the start and end of the word puts it in italics (e.g. italics) and ** puts it in bold (e.g. bold), then putting three *** at the start and end will put it in italics with bold (e.g. italics-bold). It is true that the more #'s you use, the smaller the heading is. Word and other document writers use different headings as well. Here, # gives the biggest heading, and it gets smaller and smaller with every extra #. Finally, in Psychology, the vast majority of journal articles are written in the format of: Introduction, Methods, Results, Discussion. In Semester 1 of Level 2 Psychology, you will write a report based on just the Introduction and the Methods. In Semester 2 you will write a report including all four sections. More on that to follow. Preclass Activity Complete! Well done on working your way through this activity. Be sure to make notes for yourself, and to post any questions on the forums that you may have. See you in the lab! 1.3 InClass Activiy In order to complete this activity you should have worked through the Lab PreClass Activity. You may also need The R Markdown Cheatsheet. Everything you need to complete this inclass activity can be found in those documents. 1.3.1 R Markdown and The Experimental Design Portfolio In the preclass activity we asked you to start playing about with R Markdown. Now in the lab we are going to continue learning about R Markdown as creating your own files from scratch is a great start to creating reproducible science! We will also start you off in creating your own Experimental Design Portfolio through R Markdown. The aim of this portfolio is to consolidate your learning in experimental design, allowing you to reflect back on how your learning has progressed. You should add to it whenever you think &quot;Oh that is a good tip!&quot; or &quot;That is something I want to remember!&quot;. Do this after a lab or a lecture, when you are collating all your notes together. Your portfolio is for you, unless you choose to share it of course, and will not be assessed or marked in anyway. It is your learning aid to help you develop your understanding of Research Methods in Psychology. By the end of these labs you should be able to use your portfolio for exam revision, particularly if you incorporate lecture material, so it's worth maintaining a clear structure to your portfolio! In this first lab, across 9 tasks, we will help you to understand how to structure and format R Markdown files; you can then apply what you learn here to your portfolio in your own time. We suggest having your portfolio open in every lab so you can add to it as you go along. Let's begin! Portfolio Point - Things you could include Throughout the semester you will see these Portfolio Points. They aren't always necessary to complete the lab but sometimes they will be and sometimes they are just things you might consider putting in your Portfolio to remember. What you keep in your portfolio is up to you but here are some examples of the kind of things we would recommend you include: Key points about classic experiments their main goal, outcome, authors, year a top tip is to write a short summary after every paper you read, including the authors names to help you consolidate that information Design aspects of your Registered Report what decisions you made and why; how they compare to other studies. Glossary points for R code functions For codes you find more challenging to understand the function of For codes you might use more frequently in future activities Reflection Points on what you have learned in your labs each week. 1.3.2 The Ponzo Illusion and Age The activities in this lab will make use of an open dataset. Explain This - What is an open dataset? An open dataset is made available for everyone to see and is stored on the internet for other researchers to use. In the PreClass activity you saw an example of this at the very start in the PLOS One article. Many journals now ask researchers to make their data available or to post it somewhere accessible like the Open Science Framework. Interestingly, the art of making your data available was standard in classic older articles. The data we are using today comes from 1967. Sometime between then and more recent times data started being made unavailable - closed. We believe all data should be made available and will encourage you to do that over the coming years. The data we will use today is from a paper looking at the Ponzo illusion and Age: Leibowitz, H. W. &amp; Judisch, J. M. (1967). The Relation between Age and the Magnitude of the Ponzo Illusion. The American Journal of Psychology, 80(1), 105-109. It can be accessed on campus (University of Glasgow) through this link. The basics of the Ponzo illusion (Wikipedia page) is that two lines of the same size are viewed as being of different length based on surrounding information - like sleepers on a traintrack. See Figure 1 of the Leibowitz and Judisch (1967) for an example (P106). The authors showed people two vertical lines surrounded by differing horizontal lines running at angles behind the main vertical lines. The authors varied the size of one of the vertical lines (left line) and asked the participants to judge which of the two vertical lines was bigger or longer; the left line (variable) or the right one (standard). The paper also tested how this illusion was influenced by age. For more info, see the paper. For the dependent variable, they measured what size the left line had to be to be considered the same size as the standard line on the right. The data we will be using can be seen on Pg107, and includes: Which Group people were assigned to according to age, with each group being made of 10 people of the same sex The Sex of the Group The Mean Age of the Group The Mean Length of the left vertical line 1.3.3 Task 1: Setting up Your R Markdown Portfolio As above our overall goal is to make a reproducible &quot;report&quot; summarising the data in the Leibowitz and Judisch (1967). As we go along, remember to refer back to the PreClass activity and cheatsheets to help you. Let's begin! Create a new R Markdown document. Give it a title, e.g. My Psychology Research Methods Portfolio Enter your GUID or name as the author Set the output as HTML. Helpful Hint Throughout the labs you will see these Helpful Hints. Usually the solutions are nearby or at the end of the chapter to prevent temptation. In setting up this Rmd file, if you have followed these steps correctly, you will probably see a new R Markdown file with a header containing the title, author, date and output information as shown in the PreClass activity. If you don't see the document header, then you've probably created an R Script instead. Refer back to the PreClass activity and try again. Look further down the list of File options on the top menu. If stuck, speak to a tutor. You can now remove the parts of the generic R Markdown code that we do not need; anything after the setup code chunk can be removed (see Figure 1.3). So anything after line 11 can be removed. Leave the first code chunk however - lines 8 to 10 - as these lines make R Markdown show code chunks unless otherwise specified. Portfolio Point - Code Chunk Reminders After the lab it might be handy to write a reminder somewhere in your portfolio about what a code chunk is. Writing it in your own notes somewhere accessible to you will mean you can find it more easily than searching through all the labs for the right information. This is something to keep in mind for any information you come across that you might need to recall later on! 1.3.4 Task 2: Give your Report a Heading We are going to start off your portfolio with creating a brief report on the Leibowitz and Judisch paper, so we should give it a heading. After the setup code chunk, give your report a heading, e.g. Lab 1 - The Magnitude of the Ponzo Illusion varies as a function of Age. Using hashtags, give this heading a Header 1 size. Helpful Hint Remember that the fewer the number of hashtags the larger the heading size. 1.3.5 Task 3: Creating a Code Chunk We are going to need the data soon so best to bring it in at the start. Set your working directory: Session &gt;&gt; Set Working Directory &gt;&gt; Choose Directory Download the data for this lab in a zip file by clicking this link. Unzip it and save it to the folder you are working in. Create a new code chunk in your R Markdown script, give this code chunk the name load_data. Copy and paste the code below into your code chunk. Spend a couple of minutes with a partner reminding yourself what the code does. The answer is in the hint below. Now, add or change the echo rule in your code chunk so that when you knit the file, the code will not be included in the final document. library(&quot;tidyverse&quot;) ponzo_data &lt;- read_csv(&quot;PonzoAgeData.csv&quot;) Knit the document now and see what the output looks like. It will ask you to save the file somewhere. Remember that on the Boyd Orr Lab PCs this is best done on your M: drive, given available space. Important: There is a good chance that on the webpage that you have knitted, that you will see either some warnings or messages. You can suppress these using the message and warning rules within the code chunks as well. Try this now - the PreClass Activities and the R-Markdown cheatsheet will help. Helpful Hint Hints: echo can equal TRUE or FALSE. Remember to separate rules in the code chunk with commas. E.g. {r, rule1 = FALSE, rule2 = TRUE} What does the code do? Loads the tidyverse packages and all associated packages e.g. dplyr, readr and ggplot2. Remember, you used these in Level 1. Loads in the data using the read_csv() function and stores it in ponzo_data. Important points to note: ponzo_data could have been called anything but best to call it something that makes it clear what it is. read_csv() is actually in the readr package and is available to you only after you have loaded in the tidyverse through library(tidyverse). We will always tell you to use read_csv() to read in data from a csv file. remember &lt;- essentially means assign this to that. Assigning the ponzo data to the table ponzo_data can actually can be written the other way around - read_csv(&quot;PonzoAgeData.csv&quot;) -&gt; ponzo_data - but convention usually puts it the way we have in the code. Portfolio Point - Set Working Directory One of the most common issues we see with people using RStudio is that they forget to set their working directory to the folder containing the data file they are working on. This means that when you try to knit or run a code line it won't work because RStudio doesn't know where the data is. Remember to set your working directory at the start of each session, using Session &gt;&gt; Set Working Directory &gt;&gt; Choose Directory. Avoid using code to set your working directory as this will only work on your machine and not others and is therefor not fully reproducible without editing the script. &quot;) 1.3.6 Task 4: Writing your Report Let's start giving this brief report some information and structure as we would a full report. Underneath the code chunk you entered, put a new heading called Introduction and give it a Header 2 size. Next, do a little research with your group on the Ponzo Illusion and write a sentence or two describing its function; include a citation to support your research. There is a link to the wikipedia page on the illusion at the top of this lab which might help. Finally, copy the text in the box below into your report and finish the text by putting the names of two hypotheses behind the illusion below the sentence in an ordered list style; i.e. 1... 2..., etc. The two hypotheses are The Framing hypothesis and The Perspective hypothesis. &quot;There are two underlying hypotheses that may explain the Ponzo Illusion. These are: ...&quot; Quickfire Question Here are a couple of questions to try out in your group to remind you about using citations: When writing a report, how would you cite: Papers with five authors for the first time? Author 1, Author 2, Author 3, Author 4, &amp; Author 5 Author 1, Author 2, Author 3, Author 4, &amp; Author 5, Year Author 1 et al., Year Papers with five authors for the second time? Author 1, Author 2, Author 3, Author 4, &amp; Author 5 Author 1, Author 2, Author 3, Author 4, &amp; Author 5, Year Author 1 et al., Year Papers with seven authors for the first time? Author 1, Author 2, Author 3, Author 4, Author 5, Author 6, &amp; Author 7 Author 1, Author 2, Author 3, Author 4, Author 5, Author 6, &amp; Author 7, Year Author 1 et al., Year Two papers in one paretheses? Order chronologically according to year, separated by a semi-colon Order alphabetically according to first author surname, separated by a semi-colon Two papers of the same author? Order chronologically according to year, separated by a semi-colon Order chronologically according to year, separated by a comma Order alphabetically by adding a letter to each year For more information on how to format citations, you can look at this helpful APA formatting and style guide. 1.3.7 Task 5: Making Text Bold or Italicized Sometimes we want to add some emphasis to text. In your report, format the line There are two underlying hypotheses... in bold. Answering the below question might help you remember how. Quickfire Question Bold text and italicized text are created similarly, how do you create italicized text? * (before text) ** (before and after text) * (before and after text) ** (before text) It's a good idea to knit the file at this point to make sure the bold font is working correctly. 1.3.8 Task 6: Adding Links to the Data in your Methods Good practice in a Report is to include information about where we got the data from. 1. Create a new heading below your list of the two hypotheses and call it Methods. Set it as Header 2 size. 2. Below Methods write a new heading called Data and set it as Header 3 size. 3. Underneath the Methods heading, copy and paste the sentence in below and turn the citation into an internet link to the paper. &quot;The data in this report was obtained from within the original paper, (Lebowitz and Judisch, 2016). &quot; Now knit your document again to make sure your formatting is working. Titles should be bigger than normal text and the list should be indented and have numbers at the start of each line. Helpful Hint You can get the web address by following the link to the paper shown towards the beginning of this lab activity. Include the https part. Use the R Markdown cheatsheet to see how to insert links. It has something to do with square brackets and circular brackets next to each other - e.g. . &quot;) 1.3.9 Task 7: Adding an Image to your Methods For certain studies you may want to add an image to the Methods section, either of the stimuli, of the materials, or of the procedure. If you look at the R Markdown cheatsheet you'll see that adding an image is very similar to adding a link, the only difference is the ! beforehand. For now we will just add an image of the illusion taken from the internet to illustrate how to do this. Below the sentence you added for Task 6, add a new heading called Stimuli and set it as Header 3 size. Below the Stimuli heading, insert the image at the following web address: https://upload.wikimedia.org/wikipedia/en/8/89/Ponzo_Illusion.jpg Portfolio Point - A good methods section Remember that a good methods section will contain all the necessary information that would be required for another researcher to replicate your experiment exactly! It would normally be split into three or four sections including Ethics, Participants, Stimuli, and Procedure. 1.3.10 Task 8: Adding a Table to your Results Another benefit of R Markdown is that you can insert tables of results directly into your report without having to format them - though for aesthetics you will want to learn how to format tables eventually. But for now... Create a new heading below your methods sentence, called Results and format it as Header 2 size. Add a new code chunk and give it the name table, and include the code shown below. Add an echo rule so that the code IS NOT included in the final document but the ouput table is included. group_by(ponzo_data, Sex) %&gt;% summarise(NofGroups=n(), mean_length = mean(ComparisonLength)) Now, knit your document to see what you have produced. You should not see the above code, just the output table. 1.3.11 Task 9: Adding a Figure to your Results Nearly all research reports have a figure so we will want to add one as well. Underneath your table code chunk, add a new code chunk and give it the name plot. Add the below code to the chunk and set the include rule so that both the code and the plot are included in the final report. ggplot(ponzo_data, aes(x = Mean_Age, y = ComparisonLength, color = Sex)) + geom_point() Portfolio Point - autocompletes You can use RStudio's autocomplete (the tab button) to see the different options for the different rules. For example, type include = and then hit the tab button on your keyboard. You should see the options of TRUE or FALSE. This works for a lot of functions you can't quite remember how to spell as well. Again, knit your document to make sure it is working correctly. Below your table you should now have the ggplot code followed by the nice scatterplot. We will learn more about how to improve the visualisations as we progress but for now you have completed the bones of your first report!. You can compare it to the one we have created to see if they match by clicking here for the html output or here for the R Markdown code. Fix anything that is not formatted as in our template.** Portfolio Point - The Power of R Markdown and the ggplot Package Here is a real-world scenario of why plotting in R Markdown can save a lot of effort. Say you carried out an experiment, made a figure of the results using an R Script, and wrote up the report using Microsoft Word. Then you realised you forgot to include 2 participants. To fix this, you would have to re-run the R script, make a new plot, save the plot, and then transfer that to your Word document. However, had you used R Markdown to begin with and both analysis and report were in the same place, then you can simply update the code within the document and a new figure will be created in the exact same place as the old one. Magic! The code above uses the ggplot2 package you used in Level 1. This is the main package we use for plots, figures, visualisations, or however you like to call them. It can be called into the library by itself, or is automatically called in when you call in the tidyverse package. Later this semester we will revist ggplot2 in more detail. For now, we are using it to make a scatterplot (geom_point) of Age (Mean_Age) and Comparison Length (ComparisonLength), and splitting the data for males and females. Group Discussion Point In your group, have a brief discussion about the figure to answer the following question. &quot;Based on the distibrution of the data, shown in the above Figure, ...&quot; as age increases, people perceive a shorter vertical line to be of same length as the standard vertical line as age increases, people perceive a longer vertical line to be of same length as the standard vertical linge There is no relationship between Age and the Ponzo illusion This figure tells me nothing about the relationship between Age and the Ponzo illusion Helpful Hint What does each dot represent in the Figure, and what is the pattern of the dots? 1.3.12 Job Done! Great work! We have now created a rough layout of a report. The only section we are missing is the Discussion where you relate the information from previous research to what your study showed. Feel free to add one in your own time; read the short summary at the end of the actual paper to help get your thoughts together. We will talk more about the structuring of reports all throughout the year so you will have a great idea of how to write one by the end of Level 2. Well done on succesfully creating your own R Markdown file! You should now be ready to complete the Homework Assignment for this lab. The assignment for this Lab is SUMMATIVE and should be submitted through the Moodle Level 2 Assignment Submission Page no later than 1 minute before your next lab. If you have any questions, please post them on the slack forum under the channel #level2_labs. Finally, don't forget to add any useful information to your Portfolio before you leave it too long and forget.** 1.4 Assignment This is a summative assignment. Instructions as to how to access and submit your assignment will be made available during the course. 1.5 Solutions to Questions Below you will find the solutions to the questions for the Activities for this chapter. Only look at them after giving the questions a good try and speaking to the tutor about any issues. 1.5.1 InClass Activities 1.5.1.1 Task 2: Give your Report a Heading You should have used only one hashtag to give the biggest heading size. # Lab 1 - The magnitude of the Ponzo Illusion varies as a function of Age 1.5.1.2 Task 3: Creating a Code Chunk The echo rule, warning rule and message rule should all be set to FALSE. As such, the start of the code chunk should look like: ```{r load_data, echo = FALSE, warning = FALSE, message = FALSE}``` 1.5.1.3 Task 4: Writing your Report Task 4 is about setting a title to Header 2 style. This is done via two ## at the start of the line - before the word Introduction in this case but dont forget the space. ## Introduction Worth noting that, in basic R Scripts, # at the start of the line would result in turning the line into a comment. Here, in R Markdown, # sets the header size much like a Word document header For the second part, create an ordered list by putting 1 followed by a . then a space before the first piece of information. A 2 then a . before the second, and so on. Note that lists will only work if there is a empty line above the list as well: 1. The Perspective Hypothesis 2. The Framing Hypothesis 1.5.1.4 Task 5: Making Text Bold or Italicized To turn text to bold you need to put two ** at the start and end of the word or sentence you want as bold, e.g. **make me bold** 1.5.1.5 Task 6: Adding Links to the Data in your Methods To set a header as Header 2 style use ## at the start of the line. To set a header as Header 3 style use ### at the start of the line. A link is created by putting the words you want to act as the link between [] and then the link immediately after in (). For example: [Lebowitz and Judisch (2016)](https://www.jstor.org/stable/1420548?seq=1#page_scan_tab_contents) 1.5.1.6 Task 7: Adding an Image to your Methods To set a header as Header 3 style use ### at the start of the line. An image is created by putting the words you want to act as the name of the image [] and then the link to the image immediately after in (). The key thing is to start with an exclamation mark !. For example: ![name](link) and therefore ![The Ponzo Illusion](https://upload.wikimedia.org/wikipedia/en/8/89/Ponzo_Illusion.jpg) 1.5.1.7 Task 8: Adding a Table to your Results To set a header as Header 2 style use ## at the start of the line. The code chunk heading should read as follows: ```{r table, echo = FALSE}``` 1.5.1.8 Task 9: Adding a Figure to your Results The code chunk heading should read as follows: ```{r plot, include = TRUE}``` "],
["data-wrangling-a-key-skill.html", "Lab 2 Data-Wrangling: A Key Skill 2.1 Overview 2.2 PreClass Activity 2.3 InClass Activity 2.4 Assignment 2.5 Solutions to Questions", " Lab 2 Data-Wrangling: A Key Skill 2.1 Overview One of the key skills in an researcher's toolbox is the ability to work with data. When you run an experiment you get lots of data in various files. For instance, it is not uncommon for an experimental software to create a new file for every participant you run and for each participant's file to contain numerous columns and rows of data, only some of which are important. Being able to wrangle that data, manipulate it into different layouts, extract the parts you need, and summarise it, is one of the most important skills we will help you learn in the coming weeks. The next few labs are aimed at refreshing and consolidating your skills in working with data. This lab focuses on organizing data using the tidyverse package. Over the course of the activities, you will recap the main functions and how to use them, and we will use a number of different datasets to give you a wide range of exposure to what Psychology is about, and to reiterate that the same skills apply across different datasets. The skills don't change, just the data! There are some questions to answer as you go along to test your skills: use the example code as a guide and the solutions are at the bottom. Finally, remember to be pro-active in your learning, work together as a community, and if you get stuck use the cheatsheets. The key cheatsheet for this activity is the Data Transformation with dplyr. In this lab you will recap from Level 1 on: Data-Wrangling with the Wickham six verbs. Additional useful tools such as count, gather and joins Piping and making efficient codes. Note: this preclass is a bit of a read but it is important that you have all this information in the one place so you can quickly refer back to it. Also, you did a very similar task in Level 1 so it is about recapping more than learning afresh. But take your time to try to understand the information and be sure to bring any questions to class with you. Portfolio Point - Getting Help cat(&quot;Remember to open up your Portfolio that you created in Lab 1 so you can add useful information to it as you work though the tasks! Also summarising the information we give in this preclass, in your own words, is a great way to learn! You don't have to read all of these but they might help from time to time explain parts further. For instance, do you remember how to get help in R Studio? You can call the help function (e.g. ?gather) to view the reference page for each function. This example shows how to get help on the gather function, which we will use in later labs.&quot;) 2.2 PreClass Activity Revisiting Tabular Data Remember from your previous experience that nearly all data in research methods is stored in two-dimensional tables, either called data-frames, tables or tibbles. There are other ways of storing data that you will discover in time but mainly we will be using tibbles (if you like more info, type vignette(&quot;tibble&quot;) in the console). A tibble is really just a table of data with columns and rows of information. But within that table you can get different types of data, i.e. numeric, integer, and character. Type of Data Description Numeric Numbers including decimals Integer Numbers without decimals Character Tends to contain letters or be words 2.2.0.1 Quickfire Questions What type of data would these most likely be: Male = Character Numeric Integer 7.15 = Character Numeric Integer 137 = Character Numeric Integer Portfolio Point - Data types and levels of measurement There is a lot of different types of data and as well as different types of levels of measurements and it can get very confusing. It's important to try to remember which is which because you can only do certain types of analyses on certain types of data and certain types of measurements. For instance, you can't take the average of Characters just like you can't take the average of Categorical data. Likewise, you can do any maths on Numeric data, just like you can on Interval and Ratio data. Integer data is funny in that sometimes it is Ordinal and sometimes it is Interval, sometimes you should take the median, sometimes you should take the mean. The main point is to always know what type of data you are using and to think about what you can and cannot do with them. 2.2.1 Revisiting the Wickham Six The main way we teach data-wrangling skills is by using the Wickham Six verbs. These are part of the tidyverse package which we introduced to you in Level 1. We will look at some of the basics again here but keep in mind that to look back at last years' exercises to see how used these verbs. The Wickham Six are: Function Description select() Include or exclude certain variables (columns) filter() Include or exclude certain observations (rows) mutate() Creates new variables (columns) arrange() Changes the order of observations (rows) group_by() Organises the observations (rows) into groups summarise() Derives summary variables for groups of observations (rows) 2.2.2 Learning to Wrangle: Is there a Chastity Belt on Perception Today we are going to be using data from this recent paper: Is there a Chastity Belt on Perception. You can read the full paper if you like, particularly if you are thinking about doing the action-perception registered report option, but we will summarise the paper for you. The paper asks, does your ability to perform an action influence your perception? For instance, does your ability to hit a tennis ball influence how fast you perceive the ball to be moving? Or to phrase another way, do expert tennis players perceive the ball moving slower than novice tennis players? This experiment does not use tennis players however, they used the Pong task: &quot;a computerised game in which participants aim to block moving balls with various sizes of paddles&quot;. A bit like a very classic retro arcade game. Participants tend to estimate the balls as moving faster when they have to block it with a smaller paddle as opposed to when they have a bigger paddle. You can read the paper to get more details if you wish but hopefully that gives enough of an idea to help you understand the wrangling we will do on the data. We have cleaned up the data a little to start with. Let's begin! Download the data as a zip file from this link and save it to somewhere you have access. In the lab, use your M: drive. Set your working directory to the same folder as the data. Session &gt;&gt; Set Working Directory &gt;&gt; Choose Directory Open a new script and copy and paste the two lines below. Here we are a) bringing the tidyverse into our library and then b) loading in the data through the read_csv() function and storing it in the tibble called pong_data. library(&quot;tidyverse&quot;) pong_data &lt;- read_csv(&quot;PongBlueRedBack 1-16 Codebook.csv&quot;) Help my data is not loading? These are the three most common mistakes. Make sure you have spelt the data file name exactly as it is shown. Spaces and everything. Do not change the name of the csv file, fix your code instead. The reason being is that if you have a different name for your file than someone else then your code is not reproducible. Secondly, remember when uploading data we use read_csv which has an underscore, whereas the data file itself will have a dot in its name, filename.csv. Finally, check that the datafile is actually in the folder you have set as your working directory. Note: If you are using a computer at home and you haven't previously installed the tidyverse package before you will have to install it first, e.g. install.packages(&quot;tidyverse&quot;). DO NOT install packages in the Boyd Orr labs; they are already there and just need called in through library(). Let's have a look at the pong_data and see how it is organized. Type View(pong_data) or glimpse(pong_data) in your Console window. In the dataset you will see that each row (observation) represents one trial per participant and that there were 288 trials for each of the 16 participants. The columns (variables) we have in the dataset are as follows: Variable Type Description Participant integer participant number JudgedSpeed integer speed judgement (1=fast, 0=slow) PaddleLength integer paddle length (pixels) BallSpeed integer ball speed (2 pixels/4ms) TrialNumber integer trial number BackgroundColor character background display colour HitOrMiss integer hit ball=1, missed ball=0 BlockNumber integer block number (out of 12 blocks) We will use this data to master our skills of the Wickham Six verbs, taking each verb in turn and looking at it briefly. You should develop your skills by setting yourself new challenges based on the ones we set. There are 6 verbs to work through and then after that we will briefly recap on two other functions before finishing with a quick look at pipes. Try everything out and let us know anything you can't quite get. Portfolio Point - The Wickham Six You will use the Wickham Six very frequently for wrangling your data so this would definitely be something you should be making notes about - not just the names, but how they work and any particular nuances that you spot. 2.2.3 The select() Function - to keep only specific columns The select() function lets us pick out the variables within a dataset that we want to work with. For example, say in pong_data we wanted to only keep the columns Participant, JudgedSpeed, PaddleLength, BallSpeed, TrialNumber, and HitOrMiss but we don't need BackgroundColor or BlockNumber: # We can tell the function what variables we want to include select(pong_data, Participant, JudgedSpeed, PaddleLength, BallSpeed, TrialNumber, HitOrMiss) # or do it the opposite way by exclude through `-ColumnName` select(pong_data, -BackgroundColor, -BlockNumber) # `-BackgroundColor` means &#39;not BackgroundColor&#39;, so here you are saying all columns except `BackgroundColor` and `BlockNumber`. The minus sign is the key part! 2.2.3.1 Task 1: Using the Select Function Either by inclusion or exclusion, select only the Participant, PaddleLength, TrialNumber, BackgroundColor and HitOrMiss columns from pong_data. select() can be used to reorder. Use select() to keep only the columns Participant, JudgedSpeed, BallSpeed, TrialNumber, and HitOrMiss but have them display in alphabetical order, left to right. Helpful Hint Have you remembered to include the dataset pong_data? Pay attention to upper/lower case letters and spelling! Think about how you first entered the column names as they appeared. But what happens if you change the order that you enter the column names? 2.2.4 The arrange() Function - to sort and arrange columns The arrange() function sorts the rows in the tibble according to what column you tell it to sort by. # You can arrange by one column e.g. by `BallSpeed` arrange(pong_data, BallSpeed) # Or by multiple columns e.g. by `BallSpeed` (fastest first) and `BackgroundColor` arrange(pong_data, desc(BallSpeed), BackgroundColor) Explain this - where did desc come from? What does desc() do? Compare the output of the two lines above on the BallSpeed column. Does desc() also work for BackgroundColor? desc is how to sort by largest to smallest - i.e. descending order. 2.2.4.1 Task 2: Arranging Data Arrange the data by two variables: HitOrMiss (putting hits - 1 - first), and JudgedSpeed (fast judgement - 1 - first). 2.2.5 The filter() Function - to keep only parts of the data The filter() function lets us parse out a subset of the data, meaning we keep only parts of the data. # we might want only the red `BackgroundColor` filter(pong_data, BackgroundColor == &quot;red&quot;) # or higher speeds above 4 pixels filter(pong_data, BallSpeed &gt; 4) # or both! filter(pong_data, BackgroundColor == &quot;red&quot;, BallSpeed &gt; 4) # this can also be written as: filter(pong_data, BackgroundColor == &quot;red&quot; &amp; BallSpeed &gt; 4) # or we might want to see specific Participants: filter(pong_data, Participant %in% c(&quot;1&quot;, &quot;3&quot;, &quot;10&quot;, &quot;14&quot;, &quot;16&quot;)) # The c() creates a little container of items called a vector. # the `%in%` is called `group membership` and means keep each of these Participants # or excluding a specific Participant filter(pong_data, Participant != &quot;7&quot;) # you can read != as &#39;does not equal&#39;. So keep all Participants except 7. 2.2.5.1 Task 3: Using the Filter Function Use filter() to extract all Participants that had a fast speed judgement, for speeds 2, 4, 5, and 7, but missed the ball. Store this remaining data in a variable called pong_fast_miss Helpful Hint There are three parts to this filter so it is best to think about them individually and then combine them. Filter all fast speed judgements (JudgedSpeed) Filter for the speeds 2, 4, 5 and 7 (BallSpeed) Filter for all Misses (HitOrMiss) You could do this in three filters where each one uses the output of the preceeding one, or remember that filter functions can take more than one arguement - see the example above. Also, because the JudgedSpeed and HitOrMiss are Integer you will need == instead of just =. Portfolio Point - And not Or The filter function is very useful but if used wrongly can give you very misleading findings. This is why it is very important to always check your data after you perform an action. Let's say you are working in comparative psychology and have run a study looking at how cats, dogs and horses perceive emotion. Let's say the data is all stored in the tibble animal_data and there is a column called animals that tells you what type of animal your participant was. Ok, so imagine you wanted all the data from just cats filter(animal_data, animals == &quot;cat&quot;) Exactly! But what if you wanted cats and dogs? filter(animal_data, animals == &quot;cat&quot;, animals == &quot;dog&quot;) Right? Wrong! This actually says &quot;give me everything that is a cat and a dog&quot;. But nothing is a cat and a dog, that would be weird - like a dat or a cog. In fact you want everything that is either a cat or a dog, which is filter(animal_data, animals == &quot;cat&quot; | animals == &quot;dog&quot;) The vertical line is the symbol for Or. So always pay attention to what you want and most importantly to what your code produces. 2.2.6 The mutate() Function - for adding new columns The mutate() function lets us create a new variable in our dataset. For example, let's add a new column to pong_data in which the background color is converted into numeric form where red will become 1, and blue will become 2. pong_data &lt;- mutate(pong_data, BackgroundColorNumeric = recode(BackgroundColor, &quot;red&quot; = 1, &quot;blue&quot; = 2)) The code here is is a bit complicated but: BackgroundColorNumeric is the name of your new column, BackgroundColor is the name of the old column and the one to take information from, and 1 and 2 are the new codings of red and blue respectively. The mutate() function is also handy for making some calculations on or across columns in your data. For example, say you realise you made a mistake in your experiment where your participant numbers should be 1 higher for every participant, i.e. Participant 1 should actually be numbered as Participant 2, etc. You would do something like: pong_data &lt;- mutate(pong_data, Participant = Participant + 1) Note here that you are giving the new column the same name as the old column Participant. What happens here is that you are overwriting the old data with the new data! So watch out, mutate can create a new column or overwrite an existing column, depending on what you tell it to do! 2.2.6.1 Task 4: Mutating Variables You realise another mistake in that all your trial numbers are wrong. The first trial (trial number 1) was a practice so should be excluded. And your experiment actually started on trial 2. Tidy this up by: Creating a new variable and filtering out all trials with the number 1 (TrialNumber column) from pong_data, and then use the mutate() function to recount all the remaining trial numbers, starting them at one again instead of two. Save it in pong_data. Helpful Hint Step 1. filter(TrialNumber does not equal 1) - remember to store this output in a variable? Step 2. mutate(TrialNumber = TrialNumber minus 1) 2.2.7 The group_by() Function - to group parts of data altogether The group_by() function groups the rows in a dataset according to a category you specify, e.g. grouping all Male data together and all Female data together. # In this data we could group trials by `BackgroundColor` group_by(pong_data, BackgroundColor) # or by multiple criteria e.g. `HitOrMiss` and `BackgroundColor` group_by(pong_data, HitOrMiss, BackgroundColor) Note that nothing actually appears to change in the data, unlike with the other functions, but a big operation has taken place. Look at the output in your console when you run group_by(pong_data, BackgroundColor). At the top of the output notice that the 2nd line of the output tells us the grouping criteria and how many groups now exist: see the line Groups: BackgroundColor [2]: we grouped by BackgroundColor and there are [2] groups - one for red and one for blue. 2.2.7.1 Task 5: Grouping Data Group the data by BlockNumber and by BackgroundColor, in that order, and then enter the number of groups (i.e. a number) you get as a result: Helpful Hint It is the same procedure as this but with different column names: group_by(pong_data, HitOrMiss, BackgroundColor) The number of groups should be between the sum of the number of background colors (red and blue) and the number of blocks (12). group_by() is incredibly useful as once the data is organised into groups you can then apply other functions (filter, arrange, mutate...etc.) to the groups within your data that you are interested in, instead of to the entire dataset. For instance, a common second step after group_by might be to summarise the data... 2.2.8 The summarise() Function - to do some calculations on the data The summarise() function lets you calculate some descriptive statistics on your data. For example, say you want to know the number of hits there were for different paddle lengths, or number of hits there were when the background color was red or blue. # First we group the data accordingly, storing it in `pong_data_group` pong_data_group &lt;- group_by(pong_data, BackgroundColor, PaddleLength) # And then we summarise it, storing the answer in `total_hits` pong_data_hits &lt;- summarise(pong_data_group, total_hits = sum(HitOrMiss)) # And then for fun we can filter just the red, small paddle hits pong_data_hits_red_small &lt;- filter(pong_data_hits, BackgroundColor == &quot;red&quot;, PaddleLength == 50) summarise() has a range of internal functions that make life really easy, e.g. mean, sum, max, min, etc. See the dplyr cheatsheets for more examples. 2.2.8.1 Task 6: Summarising Data Run the first two lines of code in the box above to create pong_data_hits and then enter the number of hits made with the small paddle (50) and the red color background in this box: Note: The name of the column within pong_data_hits is total_hits; this is what you called it in the above code. You could have called it anything you wanted but always try to use something sensible. Make sure to call your variables something you (and anyone looking at your code) will understand and recognize later (i.e. not variable1, variable2, variable3. etc.), and avoid spaces (use_underscores_never_spaces). Portfolio Point - the ungroup function After grouping data together using the group_by() function and then peforming a task on it, e.g. filter(), it can be very good practice to ungroup the data before performing another function. Forgetting to ungroup the dataset won't always affect further processing, but can really mess up other things. Again just a good reminder to always check the data you are getting out of a function a) makes sense and b) is what you expect. 2.2.8.2 Quickfire Questions Which of the Wickham Six would I use to sort columns from smallest to largest: select filter mutate arrange group_by summarise Which of the Wickham Six would I use to calculate the mean of a column: select filter mutate arrange group_by summarise Which of the Wickham Six would I use to remove certain observations - e.g. remove all males: select filter mutate arrange group_by summarise 2.2.9 Two Other Useful Functions The Wickham Six verbs let you to do a lot of things with data, however there are thousands of other functions at your disposal. If you want to do something with your data that you are not sure how to do using these functions, do a Google search for an alternative function - chances are someone else has had the same problem and has a help guide. For example, two other functions to note are the bind_rows() function and the count() functions. The bind_rows() function is useful if you want to combine two tibbles together into one larger tibble that have the same column structure. For example: # a tibble of ball speeds 1 and 2 slow_ball&lt;- filter(pong_data, BallSpeed &lt; 3) # a tibble of ball speeds 6 and 7 fast_ball &lt;- filter(pong_data, BallSpeed &gt;= 6) # a combined tibble of extreme ball speeds extreme_balls &lt;- bind_rows(slow_ball, fast_ball) Finally, the count() function is a shortcut that can sometimes be used to count up the number of rows you have for groups in your data, without having to use the group_by() and summarise() functions. For example, in Task 6 we combined group_by() and summarise() to calculate how many hits there were based on background color and paddle length. Alternatively we could have done: count(pong_data, BackgroundColor, PaddleLength, HitOrMiss) The results are the same, just that in the count() version we get all the information, including misses, because we are just counting rows. In the summarise() method we only got hits because that was the effect of what we summed. So two different methods give similar answers - coding can be individualised and get the same result! 2.2.10 Last but not least - Pipes (%&gt;%) to make your code efficient By now you'll have noticed thattidyverse functions generally take the following grammatical structure (called syntax): function_name(dataset, arg1, arg2,..., argN) where the dataset is the entire tibble of data you ar eusing, and each argument (arg) is some operation on a particular column or variable, or the column name you want to work with. For example: # function_name(dataset, arg1, arg2, ....) filter(pong_data, PaddleLength == &quot;50&quot;, BallSpeed &gt; 4) group_by(pong_data, BallSpeed, Participant) In the first example, we are filtering the whole pong_data dataset by a particular paddle length, then by particular speeds. In the second, we are grouping by BallSpeed and then by Participant. Note that the order of arguments is specific as it performs argument1 then argument2, etc. Changing the order of arguments may give a different output. So the order you work in is important, and this is called your pipeline. For example, here is one we used above to find how many hits there with the small paddle length and the red background. # First we group the data accordingly, storing it in `pong_data_group` pong_data_group &lt;- group_by(pong_data, BackgroundColor, PaddleLength) # And then we summarise it, storing the answer in `total_hits` pong_data_hits &lt;- summarise(pong_data_group, total_hits = sum(HitOrMiss)) # And filter just the red, small paddle hits pong_data_hits_red_small &lt;- filter(pong_data_hits, BackgroundColor == &quot;red&quot;, PaddleLength == 50) Pipelines allow us to quickly, accurately, and reproducibly, perform an action that would take much longer manually. However we can make our code even more efficient, using less code, by stringing our sequence of functions together using 'pipes', written as %&gt;%. This would look like: # Same pipeline using pipes pong_data_hits_red_small &lt;- pong_data %&gt;% group_by(BackgroundColor, PaddleLength) %&gt;% summarise(total_hits = sum(HitOrMiss)) %&gt;% filter(BackgroundColor == &quot;red&quot;, PaddleLength == 50) Both these chunks show exactly the same procedure, but adding pipes can make code easier to read and follow once you understand piping. Code without a pipe would look like function_name(dataset, arg1, arg2,...,argN) but a pipe version would look like dataset %&gt;% function_name(arg1, arg2,...,argN) The premise is that you can pipe (%&gt;%) between functions when the input of a function is the output of the previous function. Alternatively, you can use a pipe to put the data into the first function, as shown directly above. You can think of the pipe (%&gt;%) as saying 'and then' or 'goes in to'. E.g. the data goes into this function and then this function and then this function. We will expand on this in the lab where you can ask more questions, but try comparing the two chunks of code above and see if you can match them up. One last point on pipes is that they can be written in a single line of code but it's much easier to see what the pipe is doing if each function takes its own line. Every time you add a function to the pipeline, remember to add a %&gt;% first and Note that when using separate lines for each function, the %&gt;% must appear at the end of the line and not the start of the next line. Compare the two examples below. The first won't work but the second will because the second puts the pipes at the end of the line where they need to be! # Piped version that wont work data_arrange &lt;- pong_data %&gt;% filter(PaddleLength == &quot;50&quot;) %&gt;% arrange(BallSpeed) # Piped version that will work data_arrange &lt;- pong_data %&gt;% filter(PaddleLength == &quot;50&quot;) %&gt;% arrange(BallSpeed) Portfolio Point - Pipes are good for the Environment Where piping becomes most useful is when we string a series of functions together, rather than using them as separate steps and having to save the data each time under a new variable name and getting ourselves all confused. In the non-piped version we have to create a new variable each time, for example, data, data_filtered, data_arranged, data_grouped, data_summarised just to get to the final one we actually want, which was data_summarised. This creates a lot of variables and tibbles in our environment and can make everything unclear and eventually slow down our computer. The piped version however uses one variable name, saving space in the environment, and is clear and easy to read. With pipes we skip unnecessary steps and avoid cluttering our environment. 2.2.10.1 Quickfire Question What does this line of code say? data %&gt;% filter() %&gt;% group_by() %&gt;% summarise(): take the data and group it and then filter it and then summarise it take the data and filter it and then group it and then summarise it take the data and summarise it and then filter it and then group it take the data and group it and then summarise it and then filter it 2.2.11 PreClass Activity Completed! We have now recapped a number of functions and verbs that you will need as the semester goes on. You will use them in the lab next week so be sure to go over these and try them out to make yourself more comfortable with them. Remember to also start looking back at your first year labs and remembering some of the work you did there. If you have any questions please post them on the Moodle forum of the slack forum rguppies.slack.com under the channel #level2_labs. Finally, don't forget to add any useful information to your Portfolio before you leave it too long and forget. Happy Wrangling! 2.3 InClass Activity Data Wrangling In the PreClass activity we looked at using one-table Wickham verbs to filter, arrange, group_by, select, mutate and summarise. Now we will focus on working with data across two or more tables using verbs you will have come across in Level 1. The two main verbs we will add to the Wickham six today are gather() and inner_join(). gather() allows us to transform a table from wide format to long format (more on this below). inner_join() allows us to combine two tables together based on common columns. Portfolio Point - Still not sure what a function is and how to remember them? A function is a tool that takes an input, performs some action, and gives an output. They are nothing more than that. If you think about it your toaster is a function: it takes bread as an input; it perfoms the action of heating it up (nicely sometimes; on both sides would be a luxury); and it gives an output, the toast. A good thing about the Wickham six functions is that they are nicely named as verbs to describe what they do - mutate() mutates (adds on a column); arrange() arranges columns, summarise() summarises, etc. In terms of remembering all the functions, the truth is you don't have to know them all. However, through practice and repetition, you will quickly learn to remember which ones are which and what package they come from. Sort of like where to find your spoons in your kitchen - you don't look in the fridge, and then the washing machine, and then the drawer. Nope, you learnt, by repetition, to look in the drawer first time. It's the same with functions. Keep in mind that research methods is like a language in that the more you use it and work with it the more it makes sense. 2.3.1 A Note on Tidy Data In the style of programming we teach the most efficient format/layout of data is what is known as Tidy Data, and any data in this format is easily processed through the tidyverse package. However, the data you work with will not always be formatted in the most efficient way possible. If that happens then our first step is to put it into Tidy Data format. There are three fundamental rules defining Tidy Data: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell (i.e. no grouping two variables together, e.g. time/date in one cell). A cell is where any specific row and column meet; a single data point in a tibble is a cell. Explain this - If it isnt Tidy then what is it? We use Tidy Data because it is really efficient and works well with the tidyverse. However, people used to use data structured in long format or wide format. Long format is where each row is a participant from any given condition of your experiment and you would have a column saying what condition that participant was in and another column showing that participants score. Wide format on the other hand, again each row was a participant, but each column was a condition, so each participant would be spread across multiple columns. Tidy is a mix of both of these approachs and works best for data-wrangling. 2.3.2 Today's Lab - Analysing the Autism Specturm Quotient (AQ) To continue building your data wrangling skills we will recap the L11 lab on tidying up data from the Autism Spectrum Quotient (AQ) questionnaire. In that lab we used the AQ10; a non-diagnostic short form of the AQ with only 10 questions per participant. It is a discrete scale and the higher a participant scores on the AQ10 the more autistic-like traits they are said to display. Anyone scoring 7 or above is recommended for further diagnosis. You can see an example of the AQ10 through this link: AQ10 Example. Remember you can revisit your Level 1 notes at any time but we will recap here for you. We have 66 participants and your goal in this lab is to find an AQ score for each of them through your data-wrangling skills. We have four data files to work with: responses.csv containing the AQ survey responses to each of the 10 questions for our 66 participants qformats.csv containing information on how a question should be coded - i.e. forward or reverse coding scoring.csv containing information on how many points a specific response should get; depending on whether it is forward or reverse coded pinfo.csv containing participant information such as Age, Sex and importantly ID number. Click here to download the files as a zip file. Alternatively download them from the Lab2 Moodle tab. Now unzip the files into a folder on your M: drive. We will use zip folders a lot so if this is something you struggle with please ask. Portfolio Point - Open Data is best in csv format csv stands for 'comma separated variable', and is a very basic way of transferring data. It really just stores numbers and text and nothing else. The great thing about being this basic is that it can be read by many different machines and does not need expensive licenses to open it. Now set your working directory to the folder where you saved the .csv files. Do this through the dropdown menus at the top toolbar: Session &gt;&gt; Set Working Directory &gt;&gt; Choose Directory and then find your folder with your .csv files. This should be somewhere on your M: drive, but never in a folder titled R. Today we will work in a Rscript instead of .Rmd but if you want to turn the lab into a RMarkdown report or to add elements to your Portfolio then please feel free. 2.3.2.1 Group Discussion Point Now would be a good time to make sure that you are all using RStudio effectively and know what each window does. TRUE or FALSE, the Console is best for practice and the Script Window is for saving: TRUE FALSE TRUE or FALSE, the Environment holds all the data you have loaded in and created: TRUE FALSE TRUE or FALSE, clicking the name of a variable or data in the Environment window will open it in the Script window: TRUE FALSE Explain this - I don't get these answers The answer to all of these are True. The Script window is where you should write code and comments that you are going to save and send to people. The Console is where you should practice stuff - nothing is saved here; it is like a sandbox that just gets wiped away. Any data you load in or create is held in the Environment (or Global Environment) window with the variable name that you gave it. By clicking the name of the variable in the Environment window it will open up in the Script window and you can look at it to make sure it is what you expect. 2.3.3 Task 1: Open a Script Start a new Rscript and save it to your M: drive in the same folder as your .csv files, calling the Rscript something informative like Lab2_AQ_DataWrangling.R. Make sure your environment is completely empty so we dont mix up one analysis with the other. You can run the following code line in the console to clear the environment or by clicking the little brush on your environment window. rm(list = ls()) Portfolio point - comments on scripts and running lines Remember that when using a script you can write notes to yourself to remind you what a line of code does. Just put a hashtag at the start of the line and R will ignore this line. This is where you have to be clear on using a Script versus a RMarkdown file. In a Script, # means the line is ignored, in Markdown # sets the line as a header!. To run any line on a script, the simplest way is to click anywhere on that line and either press Run on the top of the script window or press CTRL+Enter on the keyboard (or mac equivalent). 2.3.4 Task 2: Bring in Your Library Add a line to your code that brings the tidyverse package into your working environment and run it. Helpful Hint - on Library vs Install combine the function library() and the package tidyverse and remember that the solutions are at the bottom of the page. On our lab machines in Psychology all the necessary packages will already be on the machines, they just need called into the library. If however you are using your own machine you will have to install the packages first. Do not install packages on the Psychology machines! Why? They are already installed and can cause the package to stop working if a student tries to install the same package on our machines. They are already installed and it is a bit like using apps on your phone. Install is putting the app onto your phone, library is just opening the app. If you've already downloaded the app (package) then you just need to open it (library()) to use it! 2.3.5 Task 3: Load in the Data Now we have to load in the .csv datafiles using the read_csv() function and save them as variables in our environment. For example, to load in the responses we would type: responses &lt;- read_csv(&quot;responses.csv&quot;) Add the following lines of code to your script and complete them to load in all four .csv datafiles. Use the above code as an example and name each variable the same as its original filename (minus the .csv part), again as above, e.g. responses.csv gets saved as responses. Remember to run the lines so that the data loaded in and is stored in your environment. responses &lt;- read_csv() # survey responses qformats &lt;- # question formats scoring &lt;- # scoring info pinfo &lt;- # participant information Portfolio Point - Haven't I read_csv before As you work with data and functions you will find there are functions with similar names but that give different results. One of these is the read function for csv. Make sure to always use read_csv() as your function to load in csv files. Nothing else. It is part of the readr package automatically brought in with tidyverse. 2.3.6 Task 4: Review Your Data. 2.3.6.1 Group Discussion Point Now that we have the data loaded in it is always best to have a look at the data to get an idea of its layout. We showed you one way before, by clicking on the name in the environment, but you can also use the glimpse() or View() functions in your Console window and putting the name of the data between the brackets to see how it is arranged. Don't add these to your script though they are just one-offs for testing. As a small group, have a look at the data in responses to see if you think it is Tidy or not and answer the following question: The data in responses is in Tidy Long Wide format Explain This - I don't get why? The reponses tibble is far from being tidy; each row represents multiple observations from the same participant, i.e. each row shows responses to multiple questions - wide format. Remember we want the data in tidy format as described above. Eh, what's a tibble? A tibble is simply a dataframe - or a table of data with columns and rows - that is really handy for working with when using the tidyverse packages. When we say tibble, you can think of a dataframe with rows and columns of information and numbers stored in them - like responses, it is a tibble. For more info, see here: Tibbles 2.3.7 Task 5: Gathering Data. We now have all the data we need loaded in, but in order to make it easier for us to get the AQ score for each participant, we need to change the layout of the responses tibble using the gather() function. Copy the below code line to your script and run it. rlong &lt;- gather(responses, Question, Response, Q1:Q10) The first argument given to the gather() function is the dataset which holds the data we want to wrangle, responses. The second and third arguments are the names we want to give the columns we are creating; the first will store the question numbers, Question the second will store the responses, Response. Note that these names could have been anything but by using these names the code make more sense. Finally, the fourth argument is the names of specific columns in the original tibble that we want to gather together. In case you are wondering if we wanted to go back the way, and ungather the data we just gathered, we would use the spread() function: e.g. rwide &lt;- spread(rlong, Questions, Response). But we do not want to do that here so let's not add this to the code. 2.3.7.1 Quickfire Question Let's see if you understand gather(). Say I wanted to gather the first three columns of responses (Q1, Q2, Q3), put the question numbers in a column called Jam, the responses in a column called Strawberry, and store everything in a tibble called sandwich. Fill in the box with what you would write: Explain this - I dont get the right answer! sandwich &lt;- gather(responses, Jam, Strawberry, Q1:Q3) gather wants the data first, then the name of the new column to store the gathered column names, then the name of the new column to store the data, and then finally which columns to gather. 2.3.8 Task 6: Combining Data. So now our responses data is in tidy format, we are closer to getting an AQ score for each person. However, we still need to add some information together to: Show if the question is reverse or forward scored - found in qformats Show the number of points to give a specific response - found in scoring. This is a typical analysis situation where different information is in different tables and you need to join them altogether. Both these pieces of information are contained in qformats and scoring respectively, but we want to join it to responses to create one informative tidy table with all the info. We can do this through the function inner_join(); a function to combine information in two tibbles using a column common to both tibbles. Copy the below line into your code and run it. # combine rows in the tibble rlong with rows in the tibble `qformats`, based on the common column &quot;Question&quot; rlong2 &lt;- inner_join(rlong, qformats, &quot;Question&quot;) Now have a look in rlong2. We have matched each question with its scoring format, forward or reverse. Portfolio Point - Reverse and Forward A lot of questionnaires have some questions that are Forward scored and some questions that are Reverse scored. What does this mean? Imagine a situation where your options in replying to a question are: 1 - extremely agree, 2 - agree, 3 - neutral, 4 - disagree, 5 - extremely disagree. In a forward-scoring question you would get 1 point for extremely agree, 2 for agree, 3 for neutral, etc. In a reverse scoring question you would get 5 for extremely agree, 4 for agree, 3 for neutral, etc. The reasoning behind this shift is that sometimes agreeing or disagreeing might be more favourable depending on how the question is worded. Secondly, sometimes these questions are used just to catch people out - imagine if you had two similar questions where one has the reverse meaning of the other. In this scenario, people should respond opposites. If they respond the same then they might not be paying attention. Now we need to combine the information in our table, rlong2, with the scoring table so we know how many points to attribute each question based on the answer the participant gave, and whether the question was forward or reverse coded. Again, we use the inner_join() function, but this time the common columns found in rlong2 and scoring are QFormat and Response. To combine by two columns you just write them in sequence as shown below. Note: when there is more than one common column between two tibbles you are joining, it is best to combine by all the columns to avoid repeat columns names in the new tibble. Copy the below line into your code and run it. # combine rows in rlong2 and scoring based on QFormat and Response rscores &lt;- inner_join(rlong2, scoring, c(&quot;QFormat&quot;, &quot;Response&quot;)) 2.3.9 Task 7: Calculating the AQ Scores. We have now created rscores which has information on how each participant responded to each question and how each question should be coded and scored, all within the one tibble. All we need now is to sum the scores for each participant to get their AQ score. Based on your Preclass knowledge, copy the below line into your code and complete it to obtain individual aq_scores for each participant. Save your script and run it all again from the start to make sure it works! aq_scores &lt;- rscores %&gt;% group_by() %&gt;% # how will you group individual participants? summarise(AQ = sum()) # which column will you sum to obtain AQ scores? Helpful Hint Each participant could be grouped by their Id. If we summed up the value for each Score we might get a full AQ Score for each particpipant. Portfolio Points - Hang on isn't that a Pipe? We saw Pipes in Level 1 and then again in the Preclass for this lab. Pipes are your friend. Think of them as saying 'and then' or 'goes into'. So in the example above we take rscores and then group it by something and then summarise it into AQ scores based on... In most cases, the pipe serves the purpose of putting the input into the function or taking the output of one function and treating it as the input of another function. In the example above the first pipe takes rscores as the input for group_by, and the second pipe takes the output of group_by and puts it as the input to summarise. See how you can almost read it as a chain of actions or steps. 2.3.9.1 Group Discussion Point The whole purpose of this lab was to calculate AQ scores for individual participants. As a small group, try to answer the following questions. Try to do it using code where possible to help you based on your knowledge from the preclass and inclass activity. Remember the cheatsheets as well. Look for the dplyr one! From the options, choose the correct citation for the AQ 10 question questionnaire: Allison, Auyeung, and Baron-Cohen, (2011) Allison, Auyeung, and Baron-Cohen, (2012) Allison and Baron-Cohen, (2012) Auyeung, Allison, and Baron-Cohen, (2012) Complete the sentence, the higher the AQ score... the less autistic-like traits displayed has no relation to autistic-like traits the more autistic-like traits displayed Type in the AQ score (just the number) of Participant ID No. 87: Type how many participants had an AQ score of 3 (again just the number): The cut-off for the AQ10 is usually said to be around 6 meaning that anyone with a score of more than 6 should be referred to for diagnostic assessment. Type in how many participants should we refer from our sample: Explain This - I dont get these answers From the link above you can see that an appropriate citation for the AQ10 would be (Allison, Auyeung, and Baron-Cohen, (2012)) As mentioned, the higher the score on the AQ10 the more autistic-like traits a participant is said to show. You could do this by code with filter(aq_scores, Id == 87), which would give you a tibble of 1x2 showing the ID number and score. If you just wanted the score you could use pull() which we havent shown you that yet: filter(aq_scores, Id == 87) %&gt;% pull(AQ). The answer is an AQ score of 2. Same as above but changing the arguement of the filter. filter(aq_scores, AQ == 3) %&gt;% count(). The answer is 13. Remember you can do this by counting but the code makes it reproducible and accurate every time. You might make mistakes. filter(aq_scores, AQ &gt; 6) %&gt;% count() or filter(aq_scores, AQ &gt;= 7) %&gt;% count(). The answer is 6. 2.3.10 Task 8: One Last Thing on Pipes You now have a complete code to load in your data, convert it to Tidy, combine the tables and calculate an AQ score for each participant. But, if you look at it, some of your code could be more efficient by using pipes. Go back through your code and rewrite it using pipes %&gt;% so that it is as efficient as possible. Helpful Hint At any point where the first argument of your function is the name of a variable created before that line, there is a good chance you could have used a pipe! Here are all the bits of this code that could be piped together into one chain: rlong &lt;- gather(responses, Question, Response, Q1:Q10) rlong2 &lt;- inner_join(rlong, qformats, &quot;Question&quot;) rscores &lt;- inner_join(rlong2, scoring, c(&quot;QFormat&quot;, &quot;Response&quot;)) aq_scores &lt;- rscores %&gt;% group_by(Id) %&gt;% summarise(AQ = sum(Score)) 2.3.11 Job Done! You have now recapped one-table and two-table verbs. These are great to know as for example, in the above, it actually only took a handful of reproducible steps to get from messy data to tidy data; could you imagine doing this by hand in Excel through cutting and pasting? Not to mention the mistakes you could make! You should now be ready to complete the Homework Assignment for this lab. The assignment for this Lab is FORMATIVE and is NOT to be submitted and will NOT count towards the overall grade for this module. However you are strongly encouraged to do the assignment as it will continue to boost your data-wrangling skills which you will need in future assignments. If you have any questions, please post them on the slack forum under the channel #level2_2018. Finally, don't forget to add any useful information to your Portfolio before you leave it too long and forget.** Happy wrangling! Excellent work! You are a DataWrangling expert! Now go try the assignment! 2.4 Assignment This is a formative assignment but we strongly encourage you to do the assignment as the knowledge gained from the practical activities in this lab will be super important to your future-self! Lab 2: Formative Data Wrangling Assignment In order to complete this assignment you will need to download the data .csv files, as well as the assignment .Rmd file, which you need to edit, titled GUID_Level2_Lab2.Rmd. These can be downloaded within a zip file from the below link. Once downloaded and unzipped, you should create a new folder that you will use as your working directory; put the data files and the .Rmd file in that folder and set your working directory to that folder through the drop-down menus at the top. Download the Assignment .zip file from here or on Moodle. Now open the assignment .Rmd file within RStudio. You will see there is a code chunk for each task. Much as you did in the first assignment, follow the instructions on what to edit in each code chunk. This will often be entering code based on the lab you have just done as opposed to always just entering a value. In the lab we recapped on data-wrangling using the Wickham 6 verbs, looked at additional functions such as gather() and inner_join(), and at piping chains of code for efficiency using %&gt;%. You will need these skills to complete the following assignment so please make sure you have carried out the PreClass and InClass activities before attempting this formative assignment. Remember to follow the instructions and if you get stuck at any point to post questions on rguppies.slack.com channel #level2_2018. Also, two useful online resources are: Hadley Wickham's R for Data Science book @ http://r4ds.had.co.nz RStudio's dplyr cheatsheet @ Rstudio.com 2.4.1 Today's Topic - The Ageing Brain A key topic in current psychologial research, and one which forms a main focus for some of the research in our School, is that of human ageing. In this research we use brain imaging techniques to understand how changes in brain function and structure relate to changes in perception and behaviour. A typical 'ageing' experiment will compare a measure (or a number of measures) such as performance on a cognitive or perceptual task between younger and older adults (i.e. a between-subjects design experiment). However in order to make sure we are studying 'healhty' ageing, we first have to 'screen' our older participants for symptoms of age-related dementia (Alzheimer's Disease), where cognitive function can be significantly impaired. We do this using a range of cognitive tests. Some studies will also test participants' sensory acuity (ability to perceive something), as a function of age (particularly eyesight and hearing). The data you have downloaded for this lab is example screening data taken from research investigating how the ageig brain processes different types of sounds. The tests used in this study are detailed below. Please note that the links are there to provide you with further information and examples of the tests once you have completed the assignment if you so wish; you do not have to read them to complete the assignment. Montreal Cognitive Assessment (MoCA) : a test specifically devised as a stand-alone screening tool for mild cognitive impairment. Assesses visuospatial skills, memory, language, attention, orientation, and abstraction skills. Example here Working Memory Digit Span Test (D-SPAN): measures the capacity of participants' short-term (working) memory. Example here D2 Test of Attention: measures participant's selective and sustained concentration and visual scanning speed. Example here Better Hearing Institute Quick Hearing Check: a self-report questionnaire which measures participant's subjective experience of their own hearing abilities. Paper version and scoring on pages 8-9. Example here and Main Test page here 2.4.2 The data files You have just downloaded the three .csv files containing all the data you need. Below is a list of the .csv files names and a description of the variables each contains: p_screen.csv contains particpants demographic information including: ID Participant Id number - for confidentiality (no names or other identifying info) AGE in years SEX M for male, F for female HANDEDNESS L for left-handed, R for right-handed EDUCATION in years MUSICAL whether they have any musical abilties/experience (YES or NO) FLANG speak any foreign languages (YES or NO) MOCA Montreal Cognitive Assessment score D-SPAN Working Memory Digit Span test score D2 D2 Test of Attention score QHC_responses.csv contains participants' responses to each question on the Better Hearing Institute Quick Hearing Check (QHC) questionnaire. Column 1 represents participants' ID (matching up to that in p_screen.csv). Each column thereafter represents the 15 questions from the questionnaire. Each row represents a participant and their response to each question. QHC_scoring.csv contains the scoring key for each question of the QHC, with the columns: RESPONSE the types of responses participants could give (STRONGLY DISAGREE, SLIGHTLY DISAGREE, NEUTRAL, SLIGHTLY AGREE, STRONGLY AGREE) SCORE the points awarded for each response type (from 0 to 4). A score for each participant can be calculated by converting their categorical responses to values and summing the values. 2.4.3 Before starting lets check: The .csv files are saved into a folder on your computer and you have manually set this folder as your working directory. The .Rmd file is saved in the same folder as the .csv files. For assessments we ask that you save it with the format GUID_Level2_Lab2.Rmd where GUID is replaced with your GUID. Though this is a formative assessment, it may be good practice to do the same here. 2.4.4 Load in the data You will see a code chunk called libraries, similar to the one below, at the top of your .Rmd assignment file. It is all set-up to load in the data for you and to call tidyverse to the library(). Run this code chunk now to bring in the data and tidyverse. You can do this in the console, in a script, or even through the code chunk by clicking the small green play symbol in the top right of the code chunk. library(&quot;tidyverse&quot;) screening &lt;- read_csv(&quot;p_screen.csv&quot;) responses &lt;- read_csv(&quot;QHC_responses.csv&quot;) scoring &lt;- read_csv(&quot;QHC_scoring.csv&quot;) 2.4.5 View the data It is always a good idea to familiarise yourself with the layout of the data that you have just loaded in. You can do this through using glimpse() or View() in the Console windown, but you must never put these functions in your assignment file. 2.4.6 The Tasks Now that we have the data loaded, tidyverse attached, and have viewed our data, you should now try to complete the following 9 tasks. You may want to practice them first to get the correct code and format, and to make sure they work. You can do this in the console or a script, but remember, once you have the correct code, edit the necessary parts of the assignment .Rmd file to produce a reproducible Rmd file. This is what you will do from now on for all other assessment files so practicing this now will really help. In short, go through the tasks and change only the NULL with what the question asks for and then make sure that the file knits at the end so that you have a fully reproducible code. 2.4.7 Task 1 - Oldest Participant Replace the NULL in the T1 code chunk with the Participant ID of the oldest participant. Store this single value in oldest_participant (e.g. oldest_participant &lt;- 999. # hint: look at your data, who is oldest? oldest_participant &lt;- NULL 2.4.8 Task 2 - Arranging D-SPAN Replace the NULL in the T2 code chunk with code that arranges participants' D-SPAN performance from highest to lowest using the appropriate one-table dplyr (i.e., Wickham) verb. Store the output in cogtest_sort. (e.g. cogtest_sort &lt;- verb(data, argument)) # hint: arrange your screening data cogtest_sort &lt;- NULL 2.4.9 Task 3 - Foreign Language Speakers Replace the NULL in each of the two lines of code chunk T3, so that descriptives has a column called n that shows the number of participants that speak a foreign language and number of participants that do not speak a foreign language, and another column called median_age that shows the median age for those two groups. If you have done this correctly, descriptives should have 3 columns and 2 rows of data, not including the header row. # hint: First need to group_by() foreign language screen_groups &lt;- NULL # hint: second need to summarise(). Pay attention to specific column names given. descriptives &lt;- NULL 2.4.10 Task 4 - Creating Percentage MOCA scores Replace the NULL in the T4 code chunk with code using one of the dplyr verbs to add a new column called MOCA_Perc to the dataframe screening In this new column should be the MOCA scores coverted to percentages. The maximum achievable score on MOCA is 30 and percentages are calculated as (participant score / max score) * 100. Store this output in screening. # hint: mutate() something using MOCA and the percentage formula screening &lt;- NULL 2.4.11 Task 5 - Remove the MOCA column Now that we have our MoCA score expressed as a percentage MOCA_Perc we no longer need the raw scores held in MOCA. Replace the NULL in the T5 code chunk using a one-table dplyr verb to keep all the columns of screening, with the same order, but without the MOCA column. Store this output in screening. # hint: select your columns screening &lt;- NULL Halfway There! The remaining tasks focus on merging two tables. You suspect that the older adults with musical experience might report more finely-tuned hearing abilities than those without musical experience. You therefore decide to check whether this trend exists in your data. You measured participant's self reported hearing abilties using the Better Hearing Institute Quick Hearing Check Questionnaire. In this questionnaire participants rated the extent to which they agree or disagree with a list of statements (e.g. 'I have a problem hearing over the telephone') using a 5 point Likert scale (Strongly Disagree, Slightly Disagree, Neutral, Slightly Agree, Strongly Agree). Each participant's response to each question is contained in the responses dataframe in your environment. Each response type is worth a certain number of points (e.g. Strongly Disagree = 0, Strongly Agree = 5), and the scoring key is contained in the scoring dataframe. A score for each participant is calculated by totalling up the number of points across all the questions to derive at an overall score. The lower the overall score, the better the participant's self-reported hearing ability. In order to score the questionnaire we first need to perform a couple of steps. 2.4.12 Task 6 - Gather the Responses together Replace the NULL in the T6 code chunk using code to gather the responses to all the questions of the QHC from wide format to tidy/long format. Name the first column Question and the second column RESPONSE. Store this output in responses_long. # hint: gather the question columns (Q1:Q15) in responses responses_long &lt;- NULL 2.4.13 Task 7 - Joining the data Now we need to join the number of points for each response in scoring to the participants' responses in responses_long. Replace the NULL in the T7 code chunk using inner_join() to combine responses_long and scoring into a new variable called responses_points. # hint: join them by the column common to both scoring and responses_long responses_points &lt;- NULL 2.4.14 Task 8 - Working the Pipes Below we have given you a code chunk with 5 lines of code. The code takes the data in its current long format and then creates a QHC score for each participant, before calculating a mean QHC score for the two groups of participants - those that play musical intruments and those that don't - and stores it in a variable called musical_means. participant_groups &lt;- group_by(responses_points, ID) participant_scores &lt;- summarise(participant_groups, Total_QHC = sum(SCORE)) participant_screening &lt;- inner_join(participant_scores, screening, &quot;ID&quot;) screening_groups_new &lt;- group_by(participant_screening, MUSICAL) musical_means &lt;- summarise(screening_groups_new, mean_score = mean(Total_QHC)) Replace the NULL in the T8 code chunk with the following code converted into a fuctioning pipeline using pipes. Put each function on a new line one under the other. This pipeline should result in the mean QHC values of musical and non-musical people stored in musical_means which should be made of two rows by two columns. # hint: in pipes, the output of the previous function is the input of the subsequent function. # hint: function1 %&gt;% function2 musical_means &lt;- NULL 2.4.15 Task 9 - Difference in Musical Means Finally, replace the NULL in the T9 code chunk with a single value, to two decimal places, that is the value of how much higher the QHC score of people who play music is compared to people who don't play music (e.g. 2.93) # hint: look in musical means and enter the difference between the two means. QHC_diff &lt;- NULL Finished Well done, you are finshed! Now you should go check your answers against the solution file which can be found on Moodle. You are looking to check that the answers you have submitted are exactly the same as the ones in the solution - for example, remember that Mycolumn is different to mycolumn and only one is correct. If you have any questions, please post them on the moodle forum or on the rguppies.slack.com forum #level2_2018. 2.5 Solutions to Questions Below you will find the solutions to the questions for the Activities for this chapter. Only look at them after giving the questions a good try and speaking to the tutor about any issues. 2.5.1 PreClass Activities 2.5.1.1 Task 1 Solution Task 1 # To include variables: select(pong_data, Participant, PaddleLength, TrialNumber, BackgroundColor, HitOrMiss) # To exclude variables: select(pong_data, -JudgedSpeed, -BallSpeed, -BlockNumber) # To select in order: select(pong_data, BallSpeed, HitOrMiss, JudgedSpeed, Participant, TrialNumber) click the tab to see the solution 2.5.1.2 Task 2 Solution Task 2 arrange(pong_data, desc(HitOrMiss), desc(JudgedSpeed)) click the tab to see the solution 2.5.1.3 Task 3 Solution Task 3 filter(pong_data, JudgedSpeed == 1, BallSpeed %in% c(&quot;2&quot;, &quot;4&quot;, &quot;5&quot;, &quot;7&quot;), HitOrMiss == 0) click the tab to see the solution 2.5.1.4 Task 4 Solution Task 4 pong_data_filt &lt;- filter(pong_data, TrialNumber &gt;= 2) # remember you can call this variable anything, as long as it makes sense to yourself and others pong_data &lt;- mutate(pong_data_filt, TrialNumber = TrialNumber - 1) click the tab to see the solution 2.5.1.5 Task 5 Solution Task 5 group_by(pong_data, BlockNumber, BackgroundColor) click the tab to see the solution 2.5.1.6 Task 6 Solution Task 6 pong_data &lt;- read_csv(&quot;PongBlueRedBack 1-16 Codebook.csv&quot;) pong_data_group &lt;- group_by(pong_data, BackgroundColor, PaddleLength) pong_data_hits &lt;- summarise(pong_data_group, total_hits = sum(HitOrMiss)) # the answer should give 517 2.5.2 InClass Actitivies 2.5.2.1 Solution to Task 2 Solution Task 2 library(tidyverse) # or library(&quot;tidyverse&quot;) # both do the same thing. Click the tab to see the solution 2.5.2.2 Solution to Task 3 Solution Task 3 responses &lt;- read_csv(&quot;responses.csv&quot;) qformats &lt;- read_csv(&quot;qformats.csv&quot;) scoring &lt;- read_csv(&quot;scoring.csv&quot;) pinfo &lt;- read_csv(&quot;pinfo.csv&quot;) Click the tab to see the solution 2.5.2.3 Solution to Task 7 Solution Task 7 aq_scores &lt;- rscores %&gt;% group_by(Id) %&gt;% # group by the ID number in column Id summarise(AQ = sum(Score)) # sum column Score to obtain AQ scores. Click the tab to see the solution 2.5.2.4 Solution to Task 8 Solution Task 8 aq_scores2 &lt;- responses %&gt;% gather(Question, Response, Q1:Q10) %&gt;% inner_join(qformats, &quot;Question&quot;) %&gt;% inner_join(scoring, c(&quot;QFormat&quot;, &quot;Response&quot;)) %&gt;% group_by(Id) %&gt;% summarise(AQ = sum(Score)) Click the tab to see the solution 2.5.3 Assignment Solution 2.5.3.1 Task 1 - Oldest Participant Replace the NULL in the T1 code chunk with the Participant ID of the oldest participant. Store this single value in oldest_participant (e.g. oldest_participant &lt;- 999. oldest_participant &lt;- 3 # This could also be answered with code. We haven&#39;t quite shown you how yet but it would look like this oldest_participant_code &lt;- arrange(screening, desc(AGE)) %&gt;% slice(1) %&gt;% pull(ID) # Either way the Participant with ID = 3 is the oldest. 2.5.3.2 Task 2 - Arranging D-SPAN Replace the NULL in the T2 code chunk with code that arranges participants' D-SPAN performance from highest to lowest using the appropriate one-table dplyr (i.e., Wickham) verb. Store the output in cogtest_sort. (e.g. cogtest_sort &lt;- verb(data, argument)) # arrange was the function and you also required desc() to sort high to low cogtest_sort &lt;- arrange(screening, desc(DSPAN)) 2.5.3.3 Task 3 - Foreign Language Speakers Replace the NULL in each of the two lines of code chunk T3, so that descriptives has a column called n that shows the number of participants that speak a foreign language and number of participants that do not speak a foreign language, and another column called median_age that shows the median age for those two groups. If you have done this correctly, descriptives should have 3 columns and 2 rows of data, not including the header row. # first run the group_by based on FLANG screen_groups &lt;- group_by(screening, FLANG) # then summarise, paying attention to use the variable names as instructed descriptives &lt;- summarise(screen_groups, n = n(), median_age = median(AGE)) 2.5.3.4 Task 4 - Creating Percentage MOCA scores Replace the NULL in the T4 code chunk with code using one of the dplyr verbs to add a new column called MOCA_Perc to the dataframe screening In this new column should be the MOCA scores coverted to percentages. The maximum achievable score on MOCA is 30 and percentages are calculated as (participant score / max score) * 100. Store this output in screening. # mutate is the necessary function to add a new column in a dataframe screening &lt;- mutate(screening, MOCA_Perc = (MOCA / 30) * 100) 2.5.3.5 Task 5 - Remove the MOCA column Now that we have our MoCA score expressed as a percentage MOCA_Perc we no longer need the raw scores held in MOCA. Replace the NULL in the T5 code chunk using a one-table dplyr verb to keep all the columns of screening, with the same order, but without the MOCA column. Store this output in screening. # Two options here; both would give the same dataframe # All but MOCA screening &lt;- select(screening, -MOCA) # All of these screening &lt;- select(screening, ID, AGE, SEX, HANDEDNESS, EDUCATION, MUSICAL, FLANG, DSPAN, D2, MOCA_Perc) Halfway There! The remaining tasks focus on merging two tables. You suspect that the older adults with musical experience might report more finely-tuned hearing abilities than those without musical experience. You therefore decide to check whether this trend exists in your data. You measured participant's self reported hearing abilties using the Better Hearing Institute Quick Hearing Check Questionnaire. In this questionnaire participants rated the extent to which they agree or disagree with a list of statements (e.g. 'I have a problem hearing over the telephone') using a 5 point Likert scale (Strongly Disagree, Slightly Disagree, Neutral, Slightly Agree, Strongly Agree). Each participant's response to each question is contained in the responses dataframe in your environment. Each response type is worth a certain number of points (e.g. Strongly Disagree = 0, Strongly Agree = 5), and the scoring key is contained in the scoring dataframe. A score for each participant is calculated by totalling up the number of points across all the questions to derive at an overall score. The lower the overall score, the better the participant's self-reported hearing ability. In order to score the questionnaire we first need to perform a couple of steps. 2.5.3.6 Task 6 - Gather the Responses together Replace the NULL in the T6 code chunk using code to gather the responses to all the questions of the QHC from wide format to tidy/long format. Name the first column Question and the second column RESPONSE. Store this output in responses_long. # gather function as clued in the question; main thing is to state the columns # and to name the columns accordingly responses_long &lt;- gather(responses, Question, RESPONSE, Q1:Q15) 2.5.3.7 Task 7 - Joining the data Now we need to join the number of points for each response in scoring to the participants' responses in responses_long. Replace the NULL in the T7 code chunk using inner_join() to combine responses_long and scoring into a new variable called responses_points. # inner_join uses a common column/variable; in this case RESPONSE responses_points &lt;- inner_join(responses_long, scoring, &quot;RESPONSE&quot;) 2.5.3.8 Task 8 - Working the Pipes Below we have given you a code chunk with 5 lines of code. The code takes the data in its current long format and then creates a QHC score for each participant, before calculating a mean QHC score for the two groups of participants - those that play musical intruments and those that don't - and stores it in a variable called musical_means. participant_groups &lt;- group_by(responses_points, ID) participant_scores &lt;- summarise(participant_groups, Total_QHC = sum(SCORE)) participant_screening &lt;- inner_join(participant_scores, screening, &quot;ID&quot;) screening_groups_new &lt;- group_by(participant_screening, MUSICAL) musical_means &lt;- summarise(screening_groups_new, mean_score = mean(Total_QHC)) Replace the NULL in the T8 code chunk with the following code converted into a fuctioning pipeline using pipes. Put each function on a new line one under the other. This pipeline should result in the mean QHC values of musical and non-musical people stored in musical_means which should be made of two rows by two columns. # in pipes, the output of the previous function is the input of the subsequent function musical_means &lt;- group_by(responses_points, ID) %&gt;% summarise(Total_QHC = sum(SCORE)) %&gt;% inner_join(screening, &quot;ID&quot;) %&gt;% group_by(MUSICAL) %&gt;% summarise(mean_score = mean(Total_QHC)) 2.5.3.9 Task 9 - Difference in Musical Means Finally, replace the NULL in the T9 code chunk with a single value, to two decimal places, that is the value of how much higher the QHC score of people who play music is compared to people who don't play music (e.g. 2.93) QHC_diff &lt;- 1.53 # Again this could more reproducibly be done by code but with functions you will soon learn. Here is how you would do it. QHC_diff_code &lt;- spread(musical_means, MUSICAL, mean_score) %&gt;% mutate(diff = YES - NO) %&gt;% pull(diff) %&gt;% round(2) End of Lab 2 "],
["visualisation-through-ggplot2.html", "Lab 3 Visualisation Through ggplot2 3.1 Overview 3.2 PreClass Activity 3.3 InClass Activity 3.4 Assignment 3.5 Solutions to Questions", " Lab 3 Visualisation Through ggplot2 3.1 Overview In the last lab we encouraged you to always be looking at your data, making sure you are understanding your data and paying attention to how it is made up in regards to data types. A second way of looking at your data is through visualisation - figures and plots - to help understand patterns and effects in your data. For example, when we asked you about the relationship between age and the Ponzo illusion in Lab 1. Visualisation is very important for understanding your data, for example in regards to seeing differences between groups, but also for seeing where things don't quite match up with what you think is happening. A great example of this is Anscombe's Quartet, which you can read up about at a later date if you like - see here. The key point is that it is always good to visualise your data and visualisation should be a common step in your practical skill set. Last year you learnt a bit about data visualisation using ggplot2, the main visualisation package of tidyverse, and you can find more info here if you like: ggplot2. Today we will revisit plotting data and expand your skills in order to make effective and informative figures. This will become really beneficial to you as your progress through University and is a skill that applies to multiple careers, not just Psychology. In this lab you will: Recap on visualisation Expand your skills to produce new figures Learn about Mental Rotation 3.2 PreClass Activity Testing Mental Rotation Ability The data we will use today comes from a recent replication of a classic experiment merging the fields of Perception and Cognition. Shepard and Metzler (1971) demonstrated that when participants are shown two similar three-dimensional shapes, one just a rotated version of the other (see the figure below - top panel), and asked participants whether they were the same shape or not, the reaction time and error rates of responses were a function of rotation; i.e. the larger the difference in rotation between the two shapes, the longer it took participants to say &quot;same&quot; or &quot;different&quot;, and the more errors they made. Figure 3.1: The Mental Rotation Task as shown in Ganis and Kievit (2016) Figure 1 The image shown in Figure 3.1 actually comes from the recent replication, Ganis and Kievit (2016). In the top panel the two shapes are the same but the shape on the right is rotated vertically at 150 degrees from the original (the left shape) and so participants should respond &quot;same&quot;. In the bottom panel however the two shapes are different; the one on the right is again rotated at 150 degrees but in this trial it takes longer for participants to realise that they are different shapes. You can read more about Ganis and Kievit (2016) in your own time but the basic methods are that they ran 54 participants on a series of these images using 4 angles of rotation (0, 50, 100, 150 degrees) and asked people to respond same or different on each trial. The data can be downloaded from here. You should use this data to follow along below and try to answer the questions. Visualising Data Download the data folder, unzip it, and save it to a folder you have access to (e.g. your M: drive if using the lab machines). Set your working directory to that folder Session &gt;&gt; Set Working Directory &gt;&gt; Choose Directory Open a new Rscript and save it within the folder that contains the data, giving the script a sensible name, e.g. Lab3_preclass_visualisations.R Copy the three code lines below into your script and run them to bring tidyverse into the library and to read in the two datafiles. library(&quot;tidyverse&quot;) menrot &lt;-read_csv(&quot;MentalRotationBehavioralData.csv&quot;) demog &lt;- read_csv(&quot;demographics.csv&quot;) Portfolio Point - Why load tidyverse and not just ggplot2? This is a really great question as we always seem to be saying to use dplyr or readr or ggplot, but we never actually call them in. Remember however that tidyverse is actually a collection of packages, the most common packages in fact, and we use it to bring in these common packages (including ggplot2) because you will probably need the other packages along with it for the codes to run smoothly. We will try to tell you when you need to call other packages alongside tidyverse but do keep in mind that most of your codes will at least start with the tidyverse package. Small point, if looking for help on ggplot, the package is actually called ggplot2. This is the newer version of the package, so search ggplot2 if you need help. Let's start by having a look at the data we have brought in. You can do this whichever way you choose; we mentioned three ways in the previous labs - check your notes. First, demog - short for demographics. It has three columns: Participant - the ID of the participant Age - the age of the participant Sex - the sex of the participant Secondly, menrot - short for mental rotation. It has 8 columns: Participant - the ID of the participant; matches to demog Trial - the trial number in the experiment for each participant Condition - the name of the image shown; R indicates the rotated image was different Time - the reaction time to respond on each trial in milliseconds DesiredResponse - what participants should have responded on each trial; Different or Same ActualResponse - what participants did respond on each trial; Different or Same. Angle - the angle that the shape on the right was rotated compared to the shape on the left (0, 50, 100, 150) CorrectResponse - whether the participant was correct or incorrect on a given trial Portfolio Point - A nice procedure Ganis and Kievit (2016) is a very short paper that is really to introduce the stimuli set rather than give an extensive background on the topic of mental rotation - we call this a 'methods paper'. That said the writing of the paper is very clear and the procedure is well detailed as to how they ran the actual experiment. When writing a procedure rememeber to give as much information needed to allow someone to exactly replicate your study. Have a read at this procedure when you have time and think about what information is there, but also what information is not there, to help you develop your writing and your reports. For example, which fingers did the participants use to respond and why would that be important? Creating Some Plots We now have our data and we want to create some plots to visualise it. We will show you the code to create four types of plots and then get you to practice more yourself in the labs, but you will remember some of this from Level 1 and you should edit/change the codes we give you and see what changes you can create - this is a great way of working. Be sure to bring any questions with you to the lab. Portfolio Point - A note on how ggplot works The two main things to know about working with ggplot is that: the usual format is: ggplot(data, aes(x = x_axis, y = y_axis)) + geom_type_of_plot() it works on a concept of layers Looking at point a: The first thing you enter is your dataframe, your data. Then within the aes() you say what is my x_axis and y_axis, using the column names from within your dataframe. aes stands for aesthetics - how do you want it to look. Finally you tell the code what type of plot you want. On point b: Layers are a common way for graphics to work. Think about it as creating your first layer and then adding more layers on top to create the figure you want. The first layer is always your data and the x and y axes. The second layer, added by using the plus symbol '+', is the type of plot. We will look at adding more layers as we progress. ggplot() is an incredibly powerful package that is used by a whole range of industries, including newspapers and mainstream media outlets, as it can do quite sophisticated images. 3.2.1 Scatterplots - geom_point() Scatterplots are a great way of visualising continuous data - data that can take any value on the scale it is measured. For example, you can use scatterplots to explore the relationship between two continuous scales such as Age and Reaction Time: Do both variables increase at the same rate? Does one variable increase and the other decrease? Or maybe there is no overall relationship? In our data, say we want to test if the overall average time to respond in the mental rotation task is affected by age of participant. We could show this in a scatterplot. The below code: Wrangles the data to create an average response time for each participant, Mean_Time, and then joins this information to the demographic data, by Participant. All this is stored in menrot_time_age. It then plots a scatterplot (geom_point()) where age is plotted on the x axis, and Mean_Time is on the y axis Finally, it uses an additional aes call to color by Sex which will color each point based on whether it was a male or female participant responding: menrot_time_age &lt;- group_by(menrot, Participant) %&gt;% summarise(Mean_Time = mean(Time, na.rm = TRUE)) %&gt;% inner_join(demog, &quot;Participant&quot;) ggplot(data = menrot_time_age, aes(x = Age, y = Mean_Time, color = Sex)) + geom_point() Figure 3.2: A scatterplot of Mean Time as a function of Age 3.2.1.1 Quickfire Questions Looking at the scatterplot in Figure 3.2, what can you say about the relationship between age and overall response time? as age increases, overall response time increases as age increases, overall response time decreases there is no overall relationship Looking at the scatterplot, what can you say about difference between male and female participants? males show more of an increase in overall response time with age than females females show more of an increase in overall response time with age than males there is no real difference between males and females in terms of overall response time and age Explain This - I don't get these answers You will have covered correlations briefly before but that is essentially what the first question is asking. If you look at the figure, does it appear that as age increases (x axis) so does overall resposne time (y axis)? Or as age decreases so does overall response time? Or maybe even as age increases, overall response time decreases? Etc etc. Well, actually, looking at the figure there appears to be no relationship between the two variables at all and it is not the case that as one either increases or decreases so does the other. The relationship appears flat. When comparing sex, based on the color of the dots, again there appears to be no major differences here as the relationship looks flat for both sex. Note: It will often be the case that to visualise data you first half to wrangle it into a format. When we do this we will be using the functions we saw in Lab 2, so make sure you have done that lab and have understood what the wrangle verbs are doing and how pipes work. Keep in mind that most functions use the format, function(data, arguement) 3.2.2 Histograms - geom_histogram() Histograms are a great way of showing the overall distribution of your data. Is it skewed? Does it look normal? Is it peaky? Is it flat? These are terms that will be familiar to you through the statistics lectures so try to think about them when visualising your data. Looking at our data, say we wanted to test if the overall distribution of mean response times for correct trials was normally distributed. The code below: Wrangles the data to create an average response time for each participant, Mean_Time, and then filters this information for correct trials only. This is then stored in menrot_hist_correct Plots a histogram (geom_histogram()) where Mean_Time is plotted on the x axis, and the count of each value in Mean_Time is plotted on the y axis. The code creates the y axis automatically and we don't have to state it: menrot_hist_correct &lt;- group_by(menrot, Participant, CorrectResponse) %&gt;% summarise(Mean_Time = mean(Time, na.rm = TRUE)) %&gt;% filter(CorrectResponse == &quot;Correct&quot;) ggplot(data = menrot_hist_correct, aes(x = Mean_Time)) + geom_histogram() Figure 3.3: CAPTION THIS FIGURE!! 3.2.2.1 Quickfire Questions Looking at the histogram, what can you say about the overall shape of the distribution? the data looks reasonably normally distirbuted the data looks positively skewed the data looks negatively skewed Looking at the histogram, what is the most common average overall response time for correct trials? approximately 2000 milliseconds approximately 2500 milliseconds approximately 3000 milliseconds Explain This - I don't get these answers Keep in mind that real data will never give that beautiful textbook shape that you see in classic diagrams when looking for normal or skewed data. Your decisions regarding your distributions will often requre a degree of judgement. From your lectures you will remember that positive skewed data means that most of the data is shifted to the left (low numbers) with a tail stretching to the right (high numbers). Negative skew is where most of the data is shifted to the right (high numbers) with a tail stretching to the left (low numbers). Normal data has most of the data in the middle with even tails on either side. Although not perfect, the data shown in our histogram is a good representation of normal data in the real world. As the y axis is the count of the values on the x axis, the most common overall response time can be found reading the highest column of the data. For this distribution this looks to be around 2500 milliseconds or 2.5 seconds. 3.2.3 Boxplots - geom_boxplot() Boxplots are a great means for visualising the spread of your data and highlighting outliers. When looking at boxplots, you should consider: Whether the box is skewed or not? Whether the median is in the middle or to one side? Are the box whiskers a similar length on both sides? Are there any outliers - usually highlighted as a star or a dot beyond the whiskers? Looking at our data, lets look at and compare the distributions of mean reaction times for correct and incorrect responses. The code below: Repeats the first two wrangle steps as when we created a scatterplot, but additionally groups by CorrectResponse, and stores the data in menrot_box_correct Plots a boxplot (geom_boxplot()) of the overall average response times on the y-axis, Mean_Time, split by the condition CorrectResponse on the x-axis Uses an additional aes call to fill the colour of the boxplots, of the two categories, based on whether CorrectResponse was correct or incorrect: menrot_box_correct &lt;- group_by(menrot, Participant, CorrectResponse) %&gt;% summarise(Mean_Time = mean(Time, na.rm = TRUE)) %&gt;% inner_join(demog, &quot;Participant&quot;) ggplot(data = menrot_box_correct, aes(x = CorrectResponse, y = Mean_Time, fill = CorrectResponse)) + geom_boxplot() Figure 3.4: CAPTION THIS FIGURE!! 3.2.3.1 Quickfire Questions Looking at the boxplots, how many outliers were there? 1 2 3 0 Looking at the boxplots, which condition had the longer median overall average response time to the mental rotation task? Median response time was longer for the Correct responses Median response time was longer for the Incorrect responses Both medians are the same approximately Explain This - I don't get these answers There are a number of ways of determining outliers which will be discussed in more detail in your lectures. You can do it through standard deviations (usually 2.5 or 3 SD are used as cut-offs) or do it with boxplots where an outlier is determined as 1.5*IQR (inter-quartile range) above or below the top and bottom of the box. Outliers are shown as dots (unless changed) above or below the whiskers of the boxplot. As you can see in the figure there are no outliers to see in this data. The median is one of the 5 values required to make a boxplot and is shown as the thick black line within the box itself. Looking at the two conditions and comparing the position on the y axis (response time) we can see that the median response time for incorrect trials was higher than correct trials, meaning that people take longer to think on the trials that they get wrong - makes sense if you think about it, uncertainty takes longer and leads to more errors. 3.2.4 Barplots - geom_bar() or geom_col() Barplots typically show the average value of a condition, e.g. the mean, and the average spread of values via error bars, e.g. standard error. We don't show error bars here below but you will learn about them at a later point. When looking at barplots, the main considerations are whether or not there appears to be a difference between the conditions you are interested in or are all conditions about the same? It is worth knowing that barplots are now used less frequently than they were as they actually do not show a lot of information, as discussed in this blog, One simple step to improving statistical inference. However, you will still see them in the field so it is good to be able to interpret them. Lookinga at our data, let's say we are interested in whether there is a difference in the average percentage of correct and incorrect responses across male and female participants. The code below: Wrangles the data through a series of steps to establish the overall percent average for correct and incorrect responses for both sex, stored in menrot_resp_sex. Plots a barplot (geom_bar()) with the condition Sex on the x axis, the Avg_Percent on the y axis, created through the wrangle, and fill the bars based on CorrectResponse Finally, within the geom_bar it says to treat the data as final values and not to average them, stat = &quot;identity&quot;, and makes both columns visible by moving them apart position = position_dodge(.9)) - without this last step the bars would overlap and you wouldn't see everything. Try changing the .9 total_n_trials &lt;- 96 menrot_resp_sex &lt;- count(menrot, Participant, CorrectResponse) %&gt;% inner_join(demog, &quot;Participant&quot;) %&gt;% mutate(PercentPerParticipant = (n/total_n_trials)*100) %&gt;% group_by(Sex, CorrectResponse) %&gt;% summarise(Avg_Percent = mean(PercentPerParticipant)) ggplot(data = menrot_resp_sex, aes(x = Sex, y = Avg_Percent, fill = CorrectResponse)) + geom_bar(stat = &quot;identity&quot;, position = position_dodge(.9)) (#fig:dim3_1)CAPTION THIS FIGURE!! geom_col() - short for column - is an alternative to geom_bar() that does not require the part of the code where you say to not do anything to the data, i.e. stat=&quot;identity&quot;. This is shown below. Notice the difference in codes but that they produce the same figure! ggplot(data = menrot_resp_sex, aes(x = Sex, y = Avg_Percent, fill = CorrectResponse)) + geom_col(position = position_dodge(.9)) (#fig:dim3_2)CAPTION THIS FIGURE!! 3.2.4.1 Quickfire Questions Looking at the barplot and data, on average, which sex had the most correct responses? female male both the same can't tell Looking at the barplot and data, on average, which sex had the most incorrect responses? female male both the same can't tell Looking at the code, what happens if you decrease the position.dodge() value? the bars get further apart the bars start to overlap nothing changes in the figure Looking at the code, what happens if you change the aes call of fill to color? the bars will stay a different color the bars become grey and the outlines become different colors nothing changes in the figure Explain This - I don't get these answers Remember that in barplots the top of the column is the average value of that condition. This is actually why people do not like barplots; though commonly used, they really only show you one value for your data, the average, and they disregard all other information unless some indication of spread is given. With that in mind, comparing the two Correct columns we can see that females had on average more correct responses than males. Doing the same for the Incorrect columns we can see that males had more incorrect responses than females. This actually makes sense as the response option in the experiment was either correct or incorrect, so when you add all the correct and incorrect percentage responses for one sex together you should get 100%. If females gave more correct reponses then they must have given less incorrect responses. The last two questions are about playing with the code. Remember we said that plots work through a concept of layers. If you set position.dodge() to 0, you will find that one of the columns disappears because they completely overlap now. So we need to set position.dodge() to a reasonable value to have the columns separate. Why not set it at 1? In barplots you often find that the different levels (or categories) of the the same variable are touching. Note however that the value of the dodge, in this case 1, is relative to the size of the x axis - if the scale of your x-axis ran from 0 to 100 then a dodge of 1 will have very little effect. The final point shows that you can add a lot more calls than just x and y axis to change the presentation of your figures. fill changes the color of the columns, color changes the outline color of the columns. We will see more of these as we progress and we will look at the difference between putting them inside the aes() and outside of it. Have a play about with these on other figures and see what happens. 3.2.5 Themes, Labels, Guides, and facet_wraps() Before we finish we want to mention a couple of other layers you can add to your ggplot calls to make your figures look more professional. We will show you the code here but we want you to run them and teach yourself how they work by changing the code, removing parts within ggplot, and by adding them to the other figures we have shown above. Bring any questions you have about these functions to the upcoming lab. themes - changing the overall presentation of your figure. Try running the below code and comparing the figure to the barplot above. Remember, ?theme_bw() will give some information or look at the cheatsheets for different themes. labels - putting appropriate labels on your figures so readers understand what is being displayed. Try changing the text within the quotes. facet_wraps - splitting data into separate figures for clarity. This will only work when one of your conditions is categorical but it can be a really effective means of displaying information. guides - remove it and see what happens. Do you understand why we use fill in this situation but perhaps not others? Try running and editing this code. total_n_trials &lt;- 96 menrot_better_plot &lt;- count(menrot, Participant, CorrectResponse) %&gt;% inner_join(demog, &quot;Participant&quot;) %&gt;% mutate(PercentPerParticipant = n/total_n_trials) %&gt;% group_by(Sex, CorrectResponse) %&gt;% summarise(Avg_Percent = mean(PercentPerParticipant)) ggplot(data = menrot_better_plot, aes(x = Sex, y = Avg_Percent, fill = CorrectResponse)) + geom_col(position = position_dodge(.9)) + labs(x = &quot;Sex of Participant&quot;, y = &quot;Percent Average (%)&quot;) + guides(fill = FALSE) + facet_wrap(~CorrectResponse) + theme_bw() 3.2.6 A figure for all occasions As you progress through Psychology you will come across a variety of different figures and plots, each looking slightly different and giving different information. When looking at these figures, and indeed when choosing one for your own analyses, you have to think about which figure is the most appropriate for your data. For example, scatterplots are great when both variables are continuous; boxplots and histograms are great for viewing spreads of data; barplots are commonly used where one variable is categorical - but as above note that barplots can be misleading and lots of new approaches to display categorical information are being created. Always keep asking yourself, does my plot display my data correctly. We will look more at this in upcoming labs and lectures where we will also work on improving our interpretation of figures. PreClass Activity Completed! If you have any questions please post them on the Moodle forum of the slack forum rguppies.slack.com under the channel #level2_2018. Finally, don't forget to add any useful information to your Portfolio before you leave it too long and forget. 3.3 InClass Activity Mental Rotation and Visualisations Visualisation is a key part of data analysis in exploring data, checking assumptions, and displaying results. Also, it is a really important skill for when you want to share your findings with others. As you develop your skills in Psychology you will start to analyse your own datasets, and learning how best to plot in different ways will benefit you in the long run. Keep in mind that this is a skill that you can carry into your future career even if that is outside of science. For visualisaion we use ggplot2 and below we have listed some great online resources that you might want to consult if you want a fuller understanding. R Graphics Cookbook ggplot2 book ggplot2 cheatsheet ggplot2 Reference Guide It is highly recommended that you consult the ggplot2 cheat sheet when you work with the package to get familiar with it. 3.3.1 Mental Rotation: Angle and Reaction Time We will use the same dataset for this class as we did in the preclass activity. To recap, Ganis and Kievit (2016) created a modern stimuli set to test the classic concept of mental rotation first proposed by Shepard and Metzler (1971). The idea is that the more rotated a test image is from an original image, the longer it will take for participants to determine if they are the same image or if they are completely different images. Results show this the case and suggest that, when doing mental rotation tasks, people do a form of internal linear interpolation, or internal mental rotation, meaning that they rotate the image in their mind back to the original angle and then compare the two images. This would be opposed to a more rapid contrast and compare approach not involving any rotation. As a result, the more rotation an image needs, the longer the task takes. Ganis and Kievit ran 54 participants on a series of these rotated images using 4 angles of rotation (0, 50, 100, 150 degrees rotated compared to the original) and asked people to respond 'same' or 'different' on each trial. Today, to further your understanding of this task and to develop your skills in visualisation and interpretation, we will look at mean reaction time for correct trials, as a function of the angle of rotation and sex. The data can be downloaded from here. Portfolio Point - as a function means? You will come across this phrase 'as a function of' quite a bit when dealing with visualisations. It means something like 'compared by' or 'across'. So you could say we are going to look at Mean Reaction Time across the four different Angles of Rotation which would be written as Mean Reaction Time as a function of Angle of Rotation. Usually it would be plotted as y axis as a function of x axis. It is a similar idea to the functions we use in our codes in that you want to see what happens when you put y in the function x. It is good to become familiar with the terms and language that is used in reports so that a) you understand what you are reading and b) you can use the same language to give a professional feel. Remember to be developing your own notes as you go! 3.3.2 Task 1: Loading and Viewing the Data Download the data, unzip it, and save it to a folder you have access to - e.g. somewhere on your M: drive Set your working directory to the folder with your data in it. Start a new script, save it to the folder with your data in it. In your script, load in the tidyverse library. Load in both datasets exactly as we did in the preclass, storing the experiment data in menrot and the demographic data in demog. Helpful Hint library() menrot &lt;- read_csv() demog &lt;- read_csv() Remember to always be looking at your data as good practice and to make sure it is as you expect it to be. Using maybe either View() or glimpse() but do this in the console, never in your script or Rmd file. Other useful functions that you can use as checks are: str() - This shows what type your data is. Look for words like table, dataframe, character, integer, numeric. head(), tail, and ls() - These show the top six and bottom six rows with column names, or just the column names with ls(). dim() - This shows the dimensions of the data. Refer to the preclass for a list of what all the columns refer to if you are not sure. And keep in mind that from now on you need to be careful with the spelling and punctuation of the variable names at all times - e.g. Female is not the same as female. 3.3.2.1 Quickfire Questions Take a couple of minutes to try the above functions and to answer the following questions. From the options, what type of data is the variable Angle, found in the dataframe menrot? character integer numeric Type in the box the name of the dataframe that contains information regarding the sex of the participants: From the options, which of these is a column within menrot? Correctresponse CorretcResponse CorrectResponse correctReponse From the options, according to the dim() call, how many rows are there in demog? 3 8 54 5184 Explain This - I don't get these answers This is about making sure you are loading in your data correctly, using the instructed names - so that you are being reproducible - and that you understand the data you are looking at. If you completed Task 1 successfully, loading in the data to the correct dataframes, then the following answers should work in the above questions. calling str(menrot) and looking through the information that comes out you should see that Angle is an integer. demog should have been where you loaded information regarding the demographics including sex of participant. ls(menrot) will give you column names and this is about making sure of the correct spelling: CorrectResponse. dim(demog) shows you the number of rows (54) by the number of columns (3). 3.3.3 Task 2: Recreating the Figure Let's start by making a representation of the top part of Figure 2 in Ganis and Kievit (2016) - Mean Reaction Time as a function of Angle of Rotation. Copy the lines of code below into your script. Replace the NULLs in order to recreate the figure below similar to that of Ganis and Kievit (2016) Figure 2 (top). Note that this figure shows information for only correct responses just as in Ganis and Kievit (2016). Remember ggplot is a case of layers. The first layer says where is my data and what do I want on each axis. Every subsequent layer says how I want the data displayed - points (geom_point()) with a connecting line (geom_line()). You can see if you can figure out what coord_cartesian does by changing the numbers another time. menrot_angle &lt;- filter(menrot, CorrectResponse == NULL) %&gt;% inner_join(demog, NULL) %&gt;% group_by(NULL) %&gt;% summarise(mean_Resp = mean(NULL)) ggplot(data = NULL, aes(x = NULL, y = NULL)) + geom_point() + geom_line() + coord_cartesian(ylim = c(0, 3500), expand = TRUE) Helpful Hint The first four lines, to create menrot_time_angle, are all functions from the Wickham Six verbs so refer back to your portfolio to see how they work. What in CorrectResponse would allow you to keep just the correct answers? What variable will allow you to join this information to demog? You only really care about the four levels of rotation, so what will you group_by? Which column do you want the mean of for a mean response time? For the ggplot line, think of the format, data, then the x axis name, then the y axis name. See your portfolio for examples. Figure 3.5: CAPTION THIS FIGURE!! 3.3.3.1 Group Discussion Point Great, you have replicated the figure! However, do you know what it means? Spend a couple of minutes discussing with your partner or group what does the figure tell you about mean reaction time and angle of rotation, and how does it fit with the overall theory we introduced above. Answering this question may help: From the options, the figure would suggest that as angle of rotation increases: mean reaction time decreases mean reaction time stays the same mean reaction time increases Portfolio Point - Reaction Time as a function of Rotation As you can see from the figure, consistent with Shepard and Metzler (1971), the participants from Ganis and Kievit (2016) showed an increase in reaction time as the angle of rotation increased. Therefore, Ganis and Kievit (2016) have replicated the findings of Shepard and Metzler (1971). A quick note though is that, yes, mean reaction time does increase with angle of rotation but it is not a consistent increase. You will see that the difference between mean reaction times for 150 and 100 degrees is smaller than between 0 and 50. Reaction times start to plateau after a certain angle of rotation. 3.3.4 Task 3: Examining Additional Variable Effects In the preclass we looked at sex of participant a lot and that was quite interesting. It is not covered in Ganis and Kievit (2016) so let us take a look at it for them. To your pipeline of Task 2, add the variable Sex to the group_by() function to group the data by Angle and Sex. Running the code again to creates the below figure. Helpful Hint group_by(Angle, Sex) Remember, to add more grouping variables just separate them by a comma. Everything else stays the same. Figure 3.6: CAPTION THIS FIGURE!! Hmmm, that figure does not look that informative. It looks similar to the one we created but the dots have doubled - we now have 8 instead of 4 - but we do not know which is male or female, and the connecting line is confusing. We need to tell the code how to separate the data based on sex. 3.3.5 Task 4: Grouping the Figure Data In the preclass we used fill and color inside the aes() to change basic information about the figure. There is also one called group. Add a group to your figure code to separate the data by Sex. Run the code again and see what your figure looks like Helpful Hint aes(x = , y = , group = ???) Well at least we now have different lines but we still can't tell which sex is which line, can we? It just looks like two black parallel lines, one slightly higher than the other. What would be ideal would be changing the color of the points based on whether they are from male or female participants! Fortunately, the geoms can also take information as well. 3.3.6 Task 5: Identifying Groups Using aes() Add an aes() call inside your geom_point() function to color the dots by Sex. Helpful Hint geom_point(aes(??? = ???) You should now have something like this: Figure 3.7: CAPTION THIS FIGURE!! Great so we can now see that the female line is on top and the male line is on the bottom. But before we start interpreting this figure let's finish tidying it up with a few more tasks. For example the dots for each data point are perhaps a little small for our old eyes so we could increase them in size. Also color is great if you can print in color but we could also change the shape of the dots to help people distinguish between the Sex. To do this we will use the additional calls of shape and size within our geom_point(). 3.3.7 Task 6: Changing the Shape and Size of Data Points We want the two Sex to have different shapes of points, so add a call to shape within the aes() call of our geom_point() function, just like you did for color. However, we want both of our Sex to have the same size of point, so add a call to size within the geom_point() function, but not inside the aes() call. This time choose an appropriate number for the size instead of naming a variable. Maybe size = 3? Helpful Hint geom_point(aes(color = Sex, Shape = ???), size = ???) This should give you a figure like this: Figure 3.8: CAPTION THIS FIGURE!! Portfolio Point - in, out, what's the aes about? Hopefully you are beginning to spot the difference between setting a call within aes() (which stands for aesthetics) and setting outside the aes(). Outside aes() means that all conditions take the one value or color or type. Inside means that each condition takes a different value or color or type. Let's look at what we have done above to help us compare. size is called outside of the aes() and we assign it a specific value. As you can see from the Task 6 figure, each condition has now the same size of points. We could set this size to what we want but keep it smallish: 3 or 5 are ok; 50 would be artistic but not that informative. In contrast, we called shape inside the aes() and we set it based on a variable, Sex. Doing it this way ensures that each level of the Sex variable, male or female, get a different shape. Had we instead set shape outside the aes(), something like geom_point(shape = 3, size = 3) then all conditions would have the same shape and the same size. Different numbers relate to different shapes and different sizes. For example compare shape = 3 to shape = 13 Likewise had we set the size within aes(), something like geom_point(aes(shape = Sex, size = Sex)) then both male and female would have different shapes AND different sizes. You can play around with this in your code to see how things work. And of course there are other arguments you could use. For example if you wanted all points to have the same color, say red for example, then you could do geom_point(color = &quot;red&quot;). Remember to put the quotes around the color. So hopefully this is starting to make sense and you can think about implementing it in your own figures. Note that arguements are separated by a comma. e.g. `geom_point(color = &quot;red&quot;, size = 3, shape = 2) 3.3.8 Task 7: Adding Labels and Changing the Background This figure is looking really nice now. Let's finish it off by making it look a little more professional with appropriate labels and by editing the background. We introduced these to you in the preclass so hopefully you had a play with them to see how they work. To change a label we use the function, labs(), and it works like labs(x = &quot;Name&quot;, y = &quot;Name&quot;, title = &quot;Name&quot;). Add this function to your code so that the y axis indicates Mean Reaction Time (ms) and that the x axis indicates Anlge of Rotation (degrees). Don't worry about a title, we tend to not use these in Psychology; we use figure legends underneath instead. Set the figure as theme_bw() - this looks nice but there are other options you might want to try which you can explore through ?theme or the cheatsheet. Helpful Hint labs(x = &quot;...&quot;, y = &quot;...&quot;) + theme_bw() The key thing is to remember to + the layer into the ggplot chain. And don't get this confused with pipes (%&gt;%). You add (+) layers, and you pipe (%&gt;%) functions. 3.3.9 Task 8: Separating a Variable and Removing Legends Finally in the preclass we showed you two other functions that you could use to tidy up figures: facet_wrap() and guides(). facet_wrap() is really effective for splitting up figures into panels based on a variable; it works like facet_wrap(~variable) where ~ can be read as by. So &quot;split up the figure by variable&quot;, for example Sex. And if you had two variables to split the figure by, then: facet_wrap(~variable1 + variable2). guides() is handy for turning on and off legends which might be taking up space. For instance, if you use a facet_wrap() to split the panels into Female and Male, do you really need the legend on the right saying Female and Male? You will normally have a guide for each color, shape, etc, call within an aes(). It works like guide(call = FALSE). Add a facet_wrap() to have separate panels in your figure based on Sex. Turn off all guides so that you have no legend in your figure. Helpful Hint facet_wrap(~variable) guides(group = FALSE, ???? = False, ....) 3.3.9.1 Group Discussion Point If you have followed all the tasks correctly then you should have the following figure: Figure 3.9: CAPTION THIS FIGURE!! Take a few minutes with your partner or group to look at the figure and try to intepret it in terms of reaction time as a function of rotation and sex. Try answering the following questions to help your discussion: In both sex, mean reaction time decreases with increases with is unaffected by angle of rotation. Angle of Rotation influences female participants male participants more than female participants male participants Portfolio Point - Interpreting the results By looking at the figure, as angle of rotation increases (moving to the right of the x axis), the mean reaction time increases (getting higher on the y axis), indicating that participants take longer to respond the further the target image is rotated from the original. Also, as the male mean reaction times are overall quicker than the female mean reaction times, and the differences in reaction times between 0 degrees and 150 degrees is smaller in males, then you could perhaps say that males are affected less than females, or males perform the task quicker. Keep in mind that we are only looking at correct responses which would rule out male participants just responding quicker overall, they are in fact responding correctly quicker overall. This is a topic that has received much attention over the years and you should refer to the reference sections of the two main papers of this activity should you wish to follow the topic further and wish to add this to your Portfolio. 3.3.10 Additional Considerations (briefly!) Many of the options we have seen in terms of geom_point() could have been applied to geom_line() to make alterations to the line. Try playing with these options. For example, the below code line would result in both sex having a red line of equal size but the style of line being different. Give it a shot! geom_line(aes(linetype = Sex), size = .5, color = &quot;red&quot;) Finally, if you look closely at the figure, you will see that the line between points actually goes in front of the points. It looks a bit messy. How could you make it tidier by having the line run behind the data points? Remember that it is a series of layers. It draws one layer and then the next, so ggplot() + geom_line() + geom_point() would draw the lines first and then the points on top. Give it a go! Job Done! Understanding figures through ggplot can seem like trial and error until you have a lot of experience. That is fine but just remember to keep adding notes to your Portfolio to help out the future you. In this class we have looked at working with layers and a variety of calls to shape, color, fills, etc, to create professional looking figures. And the beauty of it all is that once you have a figure you really like, you run the code and you get exactly the same figure again! Amazing or what??? You should now be ready to complete the Homework Assignment for this lab. The assignment for this Lab is summative and should be submitted through the Moodle Level 2 Assignment Submission Page no later than 1 minute before your next lab. If you have any questions, please post them on the slack forum under the channel #level2_2018. Finally, don't forget to add any useful information to your Portfolio before you leave it too long and forget. 3.4 Assignment This is a summative assignment. Instructions as to how to access and submit your assignment will be made available during the course. 3.5 Solutions to Questions Below you will find the solutions to the questions for the Activities for this chapter. Only look at them after giving the questions a good try and speaking to the tutor about any issues. 3.5.1 InClass Activities 3.5.1.1 Task 1 Solution Task 1 library(&quot;tidyverse&quot;) menrot &lt;- read_csv(&quot;MentalRotationBehavioralData.csv&quot;) demog &lt;- read_csv(&quot;demographics.csv&quot;) Click the tab to see the solution 3.5.1.2 Task 2 Solution Task 2 menrot_angle &lt;- filter(menrot, CorrectResponse == &quot;Correct&quot;) %&gt;% inner_join(demog, &quot;Participant&quot;) %&gt;% group_by(Angle) %&gt;% summarise(mean_Resp = mean(Time)) ggplot(data = menrot_angle, aes(x = Angle, y = mean_Resp)) + geom_point() + geom_line() + coord_cartesian(ylim = c(0, 3500), expand = TRUE) Click the tab to see the solution 3.5.1.3 Task 3 Solution Task 3 menrot_angle_sex &lt;- filter(menrot, CorrectResponse == &quot;Correct&quot;) %&gt;% inner_join(demog, &quot;Participant&quot;) %&gt;% group_by(Sex, Angle) %&gt;% summarise(mean_Resp = mean(Time)) ggplot(data = menrot_angle_sex, aes(x = Angle, y = mean_Resp)) + geom_point() + geom_line() + coord_cartesian(ylim = c(0, 3500), expand = TRUE) Click the tab to see the solution 3.5.1.4 Task 4 Solution Task 4 menrot_grouped &lt;- filter(menrot, CorrectResponse == &quot;Correct&quot;) %&gt;% inner_join(demog, &quot;Participant&quot;) %&gt;% group_by(Sex, Angle) %&gt;% summarise(mean_Resp = mean(Time)) ggplot(data = menrot_grouped, aes(x = Angle, y = mean_Resp, group = Sex)) + geom_point() + geom_line() + coord_cartesian(ylim = c(0, 3500), expand = TRUE) Click the tab to see the solution 3.5.1.5 Task 5 Solution Task 5 menrot_grouped_color &lt;- filter(menrot, CorrectResponse == &quot;Correct&quot;) %&gt;% inner_join(demog, &quot;Participant&quot;) %&gt;% group_by(Sex, Angle) %&gt;% summarise(mean_Resp = mean(Time)) ggplot(data = menrot_grouped_color, aes(x = Angle, y = mean_Resp, group = Sex)) + geom_point(aes(color = Sex)) + geom_line() + coord_cartesian(ylim = c(0, 3500), expand = TRUE) Click the tab to see the solution 3.5.1.6 Task 6 Solution Task 6 menrot_shape_size &lt;- filter(menrot, CorrectResponse == &quot;Correct&quot;) %&gt;% inner_join(demog, &quot;Participant&quot;) %&gt;% group_by(Sex, Angle) %&gt;% summarise(mean_Resp = mean(Time)) ggplot(data = menrot_shape_size, aes(x = Angle, y = mean_Resp, group = Sex)) + geom_point(aes(color = Sex, shape = Sex), size = 3) + geom_line() + coord_cartesian(ylim = c(0, 3500), expand = TRUE) Click the tab to see the solution 3.5.1.7 Task 7 Solution Task 7 menrot_lab_theme &lt;- filter(menrot, CorrectResponse == &quot;Correct&quot;) %&gt;% inner_join(demog, &quot;Participant&quot;) %&gt;% group_by(Sex, Angle) %&gt;% summarise(mean_Resp = mean(Time)) ggplot(data = menrot_lab_theme, aes(x = Angle, y = mean_Resp, group = Sex)) + geom_point(aes(color = Sex, shape = Sex), size = 3) + geom_line() + coord_cartesian(ylim = c(0, 3500), expand = TRUE) + labs(x = &quot;Angel of Rotation (degrees)&quot;, y = &quot;Mean Reaction Time (ms)&quot;) + theme_bw() Click the tab to see the solution 3.5.1.8 Task 8 Solution Task 8 manrot_facet_guide &lt;- filter(menrot, CorrectResponse == &quot;Correct&quot;) %&gt;% inner_join(demog, &quot;Participant&quot;) %&gt;% group_by(Sex, Angle) %&gt;% summarise(mean_Resp = mean(Time)) ggplot(data = manrot_facet_guide, aes(x = Angle, y = mean_Resp, group = Sex)) + geom_point(aes(color = Sex, shape = Sex), size = 3) + geom_line() + coord_cartesian(ylim = c(0, 3500), expand = TRUE) + labs(x = &quot;Angel of Rotation (degrees)&quot;, y = &quot;Mean Reaction Time (ms)&quot;) + facet_wrap(~Sex) + guides(group = FALSE, color = FALSE, shape = FALSE) + theme_bw() Click the tab to see the solution "],
["revisiting-probability-distributions.html", "Lab 4 Revisiting Probability Distributions 4.1 Overview 4.2 PreClass Activity 1 4.3 PreClass Activity 2 4.4 InClass Activity 1 4.5 InClass Activity 2 (Additional) 4.6 InClass Activity 3 (Additional) 4.7 Assignment 4.8 Solutions to Questions", " Lab 4 Revisiting Probability Distributions 4.1 Overview In this lab you will: Revise probability concepts that we discussed in Level 1 Calculate probabilities Create probability distributions Make estimations from probability distributions. Probability is to some degree the cornerstone of any Psychological theory that is based on quantitative analysis. We establish an outcome (e.g. a difference between two events), then establish the probability of that outcome, before finally deciding if that probability is meaningful or not. You will have already seen this in your lectures and in the journal articles that you are reading. We will try to help you gain a deeper understanding of probability through the course of the next few labs and in how we use it to make an inference about a population. We will start by looking at some of the general ideas behind probability. We won't be using a lot of Psychology data or concepts here as it can be easier to understand in very concrete examples but be sure to try things out, ask questions, and think about how it might relate to Psychology examples. This preclass is a bit of a read so take your time and try to understand it fully. There are no cheatsheets for this lab as we will not be using a specific package. However you can make full use of the R help function (e.g. ?sample) when you are not clear on what a function does. Also, do not be shy to do what we do and run a Google Search for finding out more about some of the stats concepts covered here. There are loads of videos and help pages out there with clear examples to explain difficult concepts. Portfolio Point - Samples, Populations and Inference The population is the whole group that you want to know something about - everyone or everything in that group. The sample is the part of the population that you are testing. The sample is always smaller than the population as it is unlikely that you would ever be able to test everyone in a population, but the sample should be representative of the population based on random sampling. This means that even though you are not using the whole population, the sample you are using represents the whole population because you randomly sampled people into it. If this is true, that the sample is representative of the population, then testing on the sample allows you to make some inference about the population; you infer a characteristic of the population from testing on the sample. 4.1.1 Discrete or Continuous Datasets This is a short tutorial with just a couple of quick recap questions on how the level of measurement can alter the way you tackle probability - i.e. whether the data is discrete or continuous. 4.1.1.1 Quickfire Questions Discrete data can only take integer values (whole numbers). For example, the number of participants in an experiment would be discrete - we can't have half a participant! Discrete variables can also be further broken down into nominal/categorical and ordinal variables. Fill in the blanks in the below sentences using the words: ordinal, nominal, categorical. data is based on a set of categories but the ordering doesn't matter (e.g. left or right handed). This is sometimes referred to as data. For example, you could separate participants according to left or right handedness or by education level. data is a set of ordered categories; you know which is the top/best and which is the worst/lowest, but not the difference between categories. For example, you could ask participants to rate the attractiveness of different faces based on a 5-item Likert scale (very unattractive, unattractive, neutral, attractive, very attractive). Continuous data on the other hand can take any value. For example, we can measure age on a continuous scale (e.g. we can have an age of 26.55 years), also reaction time or the distance you travel to university every day. Fill in the blanks in the below sentences using the two remaining levels of measurement not offered above Continuous data can be broken into or data - more of this another time. When you read journal articles or when you are working with data in the lab, it is really good practice to take a minute or two to figure out the type of variables you are reading about and/or working with. Explain This - I don't get these answers The four level of measurements are nominal (also called categorical), ordinal, interval and ratio. Discrete data only uses categories or whole numbers and is therefore either nominal or ordinal data. Continuous data can take any value, e.g. 9.00 or 9.999999999, and so is either interval or ratio data. Activity Completed! You should have a good understanding of discrete vs continuous data as it will help in the understanding of probability and inference. If you have any questions please post them on the Moodle forum of the slack forum rguppies.slack.com under the channel #level2_2018. Finally, don't forget to add any useful information to your Portfolio before you leave it too long and forget. That is all for now. See you in the lab! 4.2 PreClass Activity 1 4.2.1 General Probability Calculations Today we will being by recapping the concepts of probability calculations from Level 1, looking at discrete distributions. If you are unsure about discrete vs continuous you should try the other quick tutorial on it, to be found on Moodle. At the end of this section there are three Quickfire Questions to test your understanding. Any that you aren't clear on bring them to class next week for discussion. Probability is the extent to which an event is likely to occur and is represented by a number between 0 and 1, and the letter p. For example, the probability of flipping a coin and it landing on 'tails', most people would say, is estimated at p = .5, i.e. the likelihood of getting tails is \\(p = \\frac {1}{2}\\ \\) as there is one desired outcome (tails) and two possibilities (heads or tails). Calculating the probability of any discrete event occuring can be formulated as: \\[p = \\frac{number \\ of \\ ways \\ the \\ event \\ could \\ arise}{number \\ of \\ possible \\ outcomes}\\] For example: 1. The probability of drawing the ten of clubs from a standard pack of cards would be 1 in 52: \\(p = \\frac {1}{52} \\ = .019\\). One outcome (ten of clubs) with 52 possible outcomes (all the cards) 2. Likewise, the probability of drawing either a ten of clubs or a seven of diamonds as the the first card that you draw would be 2 in 52: \\(p = \\frac {2}{52} \\ = .038\\). In this case you are adding to the chance of an event occurring by giving two possible outcomes, so it becomes more likely to happen than when you only had one outcome. 3. Now say you have two standard packs of cards mixed together. The probability of drawing the 10 of clubs from this mixed pack would be 2 in 104: \\(p = \\frac{2}{104}= .019\\). Two possible outcomes but more alternatives than above, 104 this time, meaning it is less likely. 4. Let's instead say you have two separate packs of cards. The probability of drawing the 10 of clubs from both packs would be: \\(p = \\frac{1}{52} \\times \\frac{1}{52}= .0004\\). The probability has gone down again because you have created an event that is even more unlikely to happen - this is called the cumulative probability - multiplying the two individual probabilities of separate events to find the combined or cumulative probability. 5. What about the probability of drawing the 10 of clubs from a pack of 52, putting it back (i.e. replacement), and subsequently drawing the 7 of diamonds? Again, this would be represented by multiplying together the probability of each of these events happening to obtain the cumulative probability: \\(p = \\frac{1}{52} \\times \\frac{1}{52}= .0004\\). 6. Finally, say you draw the 10 of clubs from a pack of 52 but this time don't replace it. What is the probability that you will draw the 7 of diamonds in your next draw (again without replacing it) and the 3 of hearts in a third draw? This time the number of cards in the pack is fewer for the second (51 cards) and third draws (50 cards) so you take that into account in your multiplication: \\(p = \\frac{1}{52} \\times \\frac{1}{51}\\times \\frac{1}{50}= .000008\\). Portfolio Point - Presenting probabilities So the probability of an event is the number of all the possible ways an event could happen, divided by all the possible outcomes. When you combine probabilities of two separate events you multiple them together to obtain the cumulative probability. But you may have noticed that sometimes we write p = .008, for example, and sometimes p = 0.008. What is the difference? Well nothing really. However, there is a convention that as probability can never go above 1, then the 0 before the decimal place is pointless. Meaning that most people will write p = .008 instead of p = 0.008. We have allowed either version in the answers to this point, but try to get in the habit of writing it without the 0 before the decimal place. 4.2.1.1 Quickfire Questions What is the probability of randomly drawing your name out of a hat of 12 names where one name is definitely your name? Enter your answer to 3 decimal places: What is the probability of randomly drawing your name out of a hat of 12 names, putting it back, and drawing your name again? Enter your answer to 3 decimal places: Tricky: In a stimuli set of 120 faces, where 10 are inverted and 110 are the right way up, what is the probability of randomly removing one inverted face on your first trial, not replacing it, and then removing another inverted face on the second trial? Enter your answer to three decimal places: Helpful Hint Out of 12 possible outcomes you are looking for one possible event. There are two separate scenarios here: in both scenarios there are 12 possible outcomes in which you are looking for one possible event. Since there are two separate scenarios, does this make it more or less likely that you will draw your name twice? Think about the first trial: there are 120 possible outcomes (faces) in which you are looking for 10 possible events (inverted faces). In the second trial you have removed the first inverted face from the stimuli set so there are now only 119 trials in total and 9 inverted faces. Remember you need to combine the probabilities of the first trial and second trial results together! Explain This - I don't get these answers p = 0.083 or p = .083. One outcome (your name) out of 12 possibilities, i.e. 1/12 p = 0.007 or p = .007. Because you replace the name on both draws it is 1/12. So 1/12*1/12 and then rounded to three decimal places p = 0.006 or p = .006. It starts off as 10 out of 120, but as you remove one inverted face the second round is 9 out of 119. So the formula is (10/120)*(9/119) 4.2.2 Creating a Simple Probability Distribution We will now recap plotting probability distributions by looking at the probability distributions of a simulated coin toss. Work through this example and then apply the logic to the quickfire questions at the end of the section. Imagine we want to know the probability of getting X number of heads in 10 coin flips. To simulate 10 coin flips we used the sample() function where we randomly sample (with replacement) from all possible events: i.e. either heads or tails. Open a new script and copy in the code lines below: library(&quot;tidyverse&quot;) # Notice that because our event labels are strings (text), # we need to enter them into the function as a vector; i.e. in &quot;quotes&quot; sample(c(&quot;HEADS&quot;, &quot;TAILS&quot;), 10, TRUE) ## [1] &quot;TAILS&quot; &quot;HEADS&quot; &quot;TAILS&quot; &quot;HEADS&quot; &quot;HEADS&quot; &quot;TAILS&quot; &quot;HEADS&quot; &quot;TAILS&quot; ## [9] &quot;HEADS&quot; &quot;TAILS&quot; The last argument in the code above provides the instruction to sample with or without replacement by setting it to TRUE or FALSE respectively. Don't worry if your sequence of heads and tails is different from the above output; this is to be expected as we are generating a random sample. Portfolio Point - Sampling and Replacement Sampling is simply choosing or selecting something - here we are randomly choosing one of the possible options. Other psychology experiment 'sampling' could include randomly selecting participants, randomly choosing which stimuli to present on a given trial, or randomly assigning participants to a condition e.g.drug or placebo...etc. Replacement is putting the sampled option back into the 'pot' of possible options. For example, on the first turn you randomly sample HEADS from the options of HEADS and TAILS with replacement, meaning that on the next you have the same two options again; HEADS or TAILS. Sampling without replacement means that you remove the option from subsequent turns. So say on the first turn you randomly sample HEADS from the options HEADS and TAILS but without replacement. Now on the second turn you only have the option of TAILS to 'randomly' sample from. On the third turn without replacement you would have no options. So replacement means putting the option back for the next turn. Why would you or why wouldn't you want to use sampling with replacement in our coin toss scenario? If you aren't sure then set replacement as FALSE and run the code again. The code will stop working after 2 coin flips. We want to sample with replacement here because we want both options available at each sampling - and if we didn't then we would run out of options very quickly since we're doing 10 flips. So far our code returns the outcomes from the 10 flips; either heads or tails. If we want to count how many 'heads' we have we can simply sum up the heads. However, heads isn't a number, so to make life easier we can re-label our events as 0 for tails and 1 for heads. Now if we run the code again we can pipe the sample into a sum() function to total up all the 1s (heads) from the 10 flips. Run this line of code a number of times, what do you notice about the output? # Now that our event labels are numeric, we don&#39;t need the vector. # 0:1 means all numbers from 0 to 1 in steps of 1. So basically, 0 and 1. sample(0:1, 10, TRUE) %&gt;% sum() ## [1] 4 The ouptut of this line changes every time we run the code as we are randomly sampling 10 coin flips each time. And to be clear, if you get an answer of 6 for example, this means 6 heads, and in turn, 4 tails. By running this code over and over again we are basically demonstrating how a sampling distribution is created. Portfolio Point - What's a sampling distribution? A sampling distribution shows you the probability of each possible outcome; e.g. the probability of 5 heads in 10 flips, or the probability of 4 heads in 10 flips, or the probability of X heads in 10 flips of the coin. But in order to create a full and accurate sampling distribution we need to replicate these 10 flips a number of times. The more replications we do the more reliable the estimates. Let's do 10000 replications of our 10 coin flips. This means we flip the coin 10 times, count how many heads, save that number, and then repeat it 10000 times. We could do it the slow way we demonstrated above, just running the same line over and over and over again and noting the outcome each time. Or we could use the replicate function. Copy this line of code into your script and run it. Here we are doing exactly as we said and saving the 10000 outputs (counts of heads) in the variable called heads10k (k is shorthand for thousand). heads10k &lt;- replicate(10000, sample(0:1, 10, TRUE) %&gt;% sum()) ## int [1:10000] 4 5 5 5 4 7 3 4 9 5 ... The results of these 10000 replications are now stored in a vector called heads10k. If you have a look at heads10k, as shown in the bottom box, it is a series of 10000 numbers between 0 and 10 each indicating the number of heads, or more specifically 1s, that you got in a set of 10 flips. Now in order to complete our distribution we need to: convert the vector (list of numbers for the heads counts) into a data frame (a table) so we can work on it. Then group the results by the number of possible heads; i.e. group all the times we got 5 heads together, all the times we got 4 heads together, etc. Finally, we work out the probability of a heads result, (e.g. probability of 5 heads), by totaling the number of observations for each possible result (e.g. 5 heads) and submitting it to our probability formula above (number of outcomes of event divided by all possible outcomes) so the number of times we got a specific number of heads (e.g. 5 heads) divided by the total number of outcomes (i.e. the number of replications - 10000). Like so: Copy the below code into your script and run it. data10k &lt;- data_frame(heads = heads10k) %&gt;% # convert to a data frame group_by(heads) %&gt;% # group by number of possibilities summarise(n = n(), p=n/10000) # count occurances of possibility, ## Warning: `data_frame()` is deprecated, use `tibble()`. ## This warning is displayed once per session. # &amp; calculate probability (p) of # each We now have a discrete probability distribution of the number of heads in 10 coin flips. Use the View() function to have look at your data10k variable. You should now see for each heads outcome, the total number of occurrences in 10000 replications (n) plus the probability of that outcome (p). It might be useful to visualize the distribution as a histogram: ggplot(data10k, aes(heads,p)) + geom_col(fill = &quot;skyblue&quot;) + labs(x = &quot;Number of Heads&quot;, y = &quot;Probability of Heads in 10 flips (p)&quot;) + theme_bw() + scale_x_discrete(limits=0:10) Figure 4.1: CAPTION THIS FIGURE!! So in our analysis, the probability of getting 5 heads in 10 flips is 0.2447. But remember, do not be surprised if you get a slightly different value. Ten thousand replications is a lot but not a huge amount compared to infinity. If you run the analysis with more replications your numbers would become more stable, e.g. 100K. Note that as the possible number of heads in 10 flips are all related to one another, then summing up all the probabilities of the different number of heads will give you a total of 1. You can use this informaiton to start asking questions such as what would be the probability of obtaining 2 or less Heads in 10 flips? Well, if the probability of no heads in this distribution is 0.001, and the probability of 1 heads is 0.0084, and the probability of 2 heads is 0.0479, then the probability of 2 or less Heads in this distribution is simply the sum of these values: 0.0573 4.2.2.1 Quickfire Questions Look at the probability values corresponding to the number of coin flips you created in the data10k sample distribution (use View() to see this): Choose from the following options, if you wanted to calculate the probability of getting 4, 5 or 6 heads in 10 coin flips you would: multiply individual probabilities together sum individual probabilities together Choose from the following options, if you wanted to calculate the probability of getting 6 or more heads in 10 coin flips you would: multiply individual probabilities together sum individual probabilities together Choose from the following options, the distribution we have created is: continuous discrete Explain This - I don't understand the answers! If you think about it, we can't get 5.5 heads or 2.3 heads, we can only get whole numbers, 2 heads or 5 heads. This means that the data and the distribution is discrete. (Don't be confused by one of the functions saying continuous) To find the probability of getting say 4, 5, or 6 heads in 10 coin flips, you are combining related scenarios together, therefore you need to find the individual probabilities of getting 4, 5 or 6 heads in 10 coin flips, then sum the probabilities together to get the appropriate probability of obtaining 4, 5 or 6 heads. It is the same with 6 or more heads, just sum the probabilities of 6, 7, 8, 9 and 10 heads to get the probability of 6 or more heads. Not sure if you should be summing or multiplying probabilities? A good way to remember, from both the coin flip examples and from the pack of cards examples earlier, is that if the scenarios are related you are summing probabilities, if scenarios are separate you are multiplying probabilities. Related scenarios are usually asking you about the probability of 'either / or' scenarios occuring, whereas separate scenarios usually ask about the probability of one scenario 'and' another scenario both occuring. Your sample distribution data10k has already completed the first part of this calculation for you (finding individual probabilities of n heads in 10 coing flips), so all you need to to is sum the required probailities together! 4.2.3 The Binomial Distribution - Creating a Discrete Distribution Great, so we are now learning how probabilities and distributions work. However, if we had wanted to calculate the probability of 8 heads from 10 coin flips we don't have to go through this entire procedure each time. Instead, because we have a dichotomous outcome, &quot;heads or tails&quot;, we can calculate probabilities using the binomial distribution - effectively what you just created. You can look up the R help page on the binomial distribution (type ?dbinom directly into the console) to understand how to use it but we will walk through some essentials here. We'll use 3 functions to work with the binomial distribution: dbinom - the density function: gives you the probability of x successes (heads) given the size (trials) and probability of success prob on a single trial (here it's 0.5, because we assume we're flipping a fair coin) pbinom - the distribution function: gives you the probability of getting a number of successes below a certain cut-off point (e.g. probability of getting 0 to 5 heads out of 10 flips), given the size and the prob. qbinom - the quantile function: is the inverse ofpbinom in that it gives you the x axis value below (and including the value) which the summation of probabilities is greater than or equal to a given probability p, plus given the size and prob. 4.2.4 dbinom - The Density Function Using the dbinom function we can create probabilities for any possible outcomes of heads (e.g. 3) in 10 flips.: dbinom(3, 10, 0.5) or all possible outcomes of heads (0:10) in 10 flips: dbinom(0:10, 10, 0.5) And if we plot the probability of all possible outcomes in 10 flips it would look like this: (#fig:binom_distrib1)CAPTION THIS FIGURE!! The dbinom (density binom) function takes the format of dbinom(x, size, prob), where the arguments we give are: x the number of 'heads' we want to know the probability of. Either a single one, 3 or a series 0:10. size the number of trials (flips) we are doing; in this case, 10 flips. prob the probability of 'heads' on one trial. Here chance is 50-50 which as a probability we state as 0.5 or .5 Now say if we wanted to know the probability of 6 heads out of 10 flips, we would write: dbinom(6, 10, 0.5) # we only have to change the first (x) argument ## [1] 0.2050781 As such the probability of 6 heads, using dbinom is p = 0.2050781. If you compare this value to the data10k value for 6 you will see they are similar but not quite the same because dbinom uses a lot more replications than the 10000 we used. In terms of visualising what we have just calculated. That number represents the height of the green bar in the plot below. (#fig:binom_plot1)CAPTION THIS FIGURE!! 4.2.4.1 Quickfire Question To three decimal places, what is the probability of 2 heads out of 10 flips? Explain This - I can't get the right answer You want to know the probability of 2 heads in 10 flips. X is therefor 2; Size is therefore 10; and the probability stays the same at .5, meaning that the code would look like this: dbinom(2, 10, 0.5) = .04394531 or rounded = .044 4.2.5 pbinom - The Cumulative Probability Function What if we wanted to know the probability of up to and including 3 heads out of 10 flips? We can either use dbinom for each outcome up to 3 heads and sum the results: dbinom(0:3, 10, 0.5) %&gt;% sum() ## [1] 0.171875 Or we can use the pbinom function; known as the cumulative probability distribution function or the cumulative density function. The first argument we give is the cut-off value up to and including which we want to know the probability of (here it's up to 3 heads). Then, as before, we tell it how many flips we want to do and the probability of heads on a single trial. Copy this line into your script and run it: pbinom(3, 10, 0.5, lower.tail = TRUE) ## [1] 0.171875 So the probability of up to and including 3 heads (as lower.tail = TRUE) out of 10 flips is 0.172. For visualization, what we have done is calculated the cumulative probability of the lower tail of the distribution up to our cut-off of 3 (in green below): (#fig:binom_plot2)CAPTION THIS FIGURE!! So the pbinom function gives us the cumulative probability of outcomes up to and including the cut-off. But what if we wanted to know the probability of outcomes including and above a certain value? Say we want to know the probability of 7 heads or more out of 10 coin flips. The code would be this: pbinom(6, 10, 0.5, lower.tail = FALSE) ## [1] 0.171875 We still specify our cut-off as 6 heads, even though we want 7 and above, but we switch the lower.tail call from TRUE to FALSE to tell pbinom we don't want the lower tail this time (to the left of and including the cut-off), we want the upper tail, to the right of the cut-off. This results in the cumulative probability for the upper tail of the distribution down to our cut-off value (shown in green below). We set the cut-off as '6' because in the discrete distribution, only the lower.tail = TRUE includes the cut-off (6 and below) whereas the lower.tail = FALSE would be everything above the cut-off but not including the cut-off (7 and above). (#fig:binom_plot3)CAPTION THIS FIGURE!! Portfolio Point - Am all in a Tail Spin! Lower TRUE or FALSE The most confusing part for people we find is the concept of lower.tail. If you look at a distribution, say the binomial, you find a lot of the high bars are in the middle of the distribution and the smaller bars are at the far left and right of the distribution. Well the far left and right of the distribution is called the tail of the distribution - they tend to be an extremity of the distribution that taper off like a.....well like a tail. A lot of the time we will talk of left and right tails but here in the pbinom function it only ever considers the data in relation to the left side of the data - this is what it calls the lower.tail. Let's consider lower.tail = TRUE, this is the default, so if you dont state lower.tail then this is what is considered to be what you want. lower.tail = TRUE means all the values to the left of your value including the value you state. So on the binomial distribution if you say give me the probability of 5 heads or less, then you would set lower.tail = TRUE and you would be counting and summing the probability of 0, 1, 2, 3, 4 and 5 heads. You can check this with dbinom(0:5, 10, .5) %&gt;% sum(). However, if you say give me the probability of 7 or more heads, then you need to do lower.tail = FALSE, to consider the right-hand side tail, but also, you need to set the code as pbinom(6, 10, .5, lower.tail = FALSE). Why 6 and not 7? Because the pbinom function, when lower.tail = FALSE, starts at the value plus one to the value you state; it always considers the value you state as being part of the lower.tail so if you say 6, it includes 6 in the lower.tail and then gives you 7, 8, 9 and 10 as the upper tail. If you said 7 with lower.tail = FALSE, then it would only give you 8, 9 and 10. This is tricky but worth keeping in mind when you are using pbinom function. And remember, you can always check it by using dbinom(7:10, 10, .5) %&gt;% sum() and seeing whether it matches pbinom(6, 10, 0.5, lower.tail=FALSE) or pbinom(7, 10, 0.5, lower.tail=FALSE) 4.2.5.1 Quickfire Questions Using the format of the above pbinom function, enter the code that would determine the probability of up to and including 5 heads out of 20 flips, assuming a probability of 0.5: To two decimal places, what is the probability of obtaining more than but not including 50 heads in 100 flips? Helpful Hint You are looking to calcuate the probability of 5 or less heads (x) out of 20 flips (size), with the probability of 'heads' in one trial (prob) remaining the same. Do you need the lower.tail call here if you are calculating the cumulative probability of the lower tail of the distribution? You are looking to calculate the probability of 51 or more heads (x), out of 100 flips (size), with the probability of 'heads' in one trial (prob) remaining the same. Do you need the lower.tail call here if you are calculating the cumulative probability of the upper tail of the distribution? Remember, because you are not looking at the lower.tail, the value of heads that you enter in pbinom will not be included in the final calculation, e.g. entering pbinom(3, 100, lower.tail = FALSE) will give you the probability for 4 and above heads. If you were instead looking at the lower.tail, entering pbinom(3, 100, lower.tail = TRUE) would give you the probability of 3 and below heads. Explain This - I can't get these answers The code for the first one would be: pbinom(5, 20, 0.5) or pbinom(5, 20, 0.5, lower.tail = TRUE) The code for the second one would be: pbinom(50, 100, 0.5, lower.tail = FALSE), giving an answer of .46. Remember you can confirm this with: dbinom(51:100, 100, 0.5) %&gt;% sum() 4.2.6 qbinom - The Quartile Function Finally, the qbinom function is useful if you want to know the minimum number of successes ('heads') that would be needed to achieve a particular probability. This time the cut-off we specify is not the number of 'heads' we want but the probability we want. For example say we want to know the minimum number of 'heads' out of 10 flips that would result in a 5% heads success rate (a probability of .05), we would use the following code. qbinom(.05, 10, 0.5) ## [1] 2 As you can see we would need at least 2 Heads in 10 flips to maintain a probability of .05. Portfolio Point - I don't understand qbinom arguments We found a lot of students asking about qbinom and how it works when you are inputting two different probabilities as the arguments for qbinom. Let us try and make things clearer. The function is set up as qbinom(p, size, prob). First of all, you have used prob in the previous two functions, dbinom and pbinom, and it represents the probability of success on a single trial (here it is the probability of 'heads' in one coin flip, .5). Now, prob represents the probability of success in one trial, whereas p represents the overall probability of success across all trials and gives you the number of heads that would give that probability. So you ask it for the minimum number of successes (e.g. heads) to maintain an overall probability of .05, in 10 flips, when the probability of a success on any one flip is .5. And it tells you the answer is 2. qbinom also uses the lower.tail argument and it works in a similar fashion to pbinom. 4.2.6.1 Quickfire Question Type in the box, the number of heads required to maintain an overall probability rate of 10% (.1) in 17 flips: Explain This - I can't get the answer The answer would be 6 because the code would be: qbinom(0.1, 17, 0.5, lower.tail = TRUE) Remember that you want an overall probability of 10% (p = .1), you have 17 flips in each go (size = 17), and the probability of heads on any one flip is .5 (prob = .5). And you want the minimum number so the lower.tail is TRUE. PreClass Activity Completed! Excellent! We have now recapped some general probability concepts as well as going over the binomial distribution again. We will focus on the normal distribution in the lab but it would help to read the brief section on it which can be found on Moodle. Keep in mind that what you are trying to get an understanding of is that every value in a distribution has a probability of existing in that distribution. That probability may be very large, meaning that it is from the middle of the distribution, or that probability might be rather low, meaning it is from the tail, but ultimately every value of a distribution has a probability. If you have any questions please post them on the Moodle forum of the slack forum rguppies.slack.com under the channel #level2_2018. Finally, don't forget to add any useful information to your Portfolio before you leave it too long and forget. That is all for now. See you in the lab! 4.3 PreClass Activity 2 4.3.1 Continuous Data - Brief Recap on The Normal Distribution This a very short recap on the normal distribution which will help you in the lab if you read it before arriving. In the main PreClass activity we have seen how we can use a distribution to estimate probabilities and determine cut-off values (these will play an important part in hypothesis testing in later labs!), but we have looked only at the discrete binomial distribution. Many of the variables we will encounter will be continuous and tend to show a normal distribution (e.g. height, weight, IQ). Let's say we're interested in the height of the population of level 2 psychology students, which we estimate to be between 146cm and 194cm. If we plotted this as a continuous, normal distribution, it will look like: Figure 4.2: CAPTION THIS FIGURE!! The figure shows the hypothetical probability density of heights ranging from 146cm to 194cm in the population of level 2 psychology students (black curve). This data is normally distributed - we know this as it has the following properties: 4.3.2 Properties of the Normal distribution: 1. The distribution is defined by its mean and standard deviation: The mean (\\(\\mu\\)) describes the center, and therefore peak density, of the distribution. This is where the largest number of the people in the population will be in terms of height. The standard deviation (\\(\\sigma\\)) describes how much variation there is from the mean of the distribution - on the figure, the standard deviation is the distance from the mean to the inflection point of the curve (the part where the curve changes from a upside-down bowl shape to a right-side-up bowl shape). 2. Distribution is symmetrical around the mean: The mean lies in the middle of the distribution and divides the area under the curve into two equal sections - so we get the typical bell-shaped curve. 3. Total area under the curve is equal to 1: If we were to add up the probabilities (densities) for every possible height, we would end up with a value of 1. 4. The mean, median and mode are all equal: A good way to check if a given dataset is normally distributed is to calculate each measure of central tendency and see if they are approximately the same (normal distribution) or not (skewed distribution). 5. The curve approaches, but never touches, the x axis: You will never have a probability of 0 for a given x axis value. 6. The normal distribution follows the Empirical Rule: The Empirical Rule states that 99.7% of the data within the normal distribution falls within three standard deviations (\\(\\pm3\\sigma\\)) from the mean, 95% fall within two standard deviations (\\(\\pm2\\sigma\\)), and 68% fall within one standard deviation (\\(\\pm\\sigma\\)). Activity Completed! We will look at the normal distribution more in the lab. If you have any questions please post them on the Moodle forum of the slack forum rguppies.slack.com under the channel #level2_2018. Finally, don't forget to add any useful information to your Portfolio before you leave it too long and forget. That is all for now. See you in the lab! 4.4 InClass Activity 1 Today we are going to continue our study of distibrutions and probability to help gain an understanding of how we can end up with an inference about a population from a sample. In the preclass activity we focussed a lot on basic probability and binomial distributions. In the lab today we will focus more on the normal distribution, is one of the key distributions in Psychology. It will be really beneficial to you when it comes to doing the assignment to have run through both the preclass and inclass activities. The assignment for this lab is formative and should not be submitted. This of course does not mean that you should not do it. As will all the formative labs, completing them and understanding them, will benefit you when it comes to completing the summative labs. You may also want to refer to the Lecture series for help. If you are unclear at any stage please do ask; probability is challenging to grasp at first. Portfolio Point - Always be adding to your knowledge in your own words Remember to add useful information to you Portfoilio! One of the main reasons we do this is because there is a wealth of research that says the more interactive you are with your learning, the deeper your understanding of the topic will be. This relates to the Craik and Lockhart (1972) levels of processing model you will hear about in lectures. Always make notes about what you are learning to really get an understanding for the concepts. And, most importantly, in your own words! Craik, F. I. M., &amp; Lockhart, R. S. (1972). Levels of processing: A framework for memory research. Journal of Verbal Learning and Verbal behavior, 11, 671-684. 4.4.1 Continuous Data and the Normal Distribution Many of the variables we will encounter in Psychology will: be continuous as opposed to discrete. tend to show a normal distribution. look similar to below - the bell-shaped curve - when plotted. But can you name any? Take a couple of minutes as a group to think of variables that we might encounter that are normally distributed. Figure 4.2: CAPTION THIS FIGURE!! 4.4.2 Estimating from the Normal Distribution We won't ask you to create a normal distribution as it is more complicated than the binomial distribution you estimated in the preclass. Unlike coin flips, the outcome in the normal distribution is not just 50/50. Instead, just as with the binomial distribution (and other distributions) there are functions that allow us to estimate the normal distribution and to ask questions about the distribution. These are: dnorm() - the density function pnorm() - the probability or distribution function qnorm() - the quantile function They work in a similar way to their binomial counterparts. If you are unsure about how a function works you can call the help on it by typing in the console, for example, ?dnorm or ?dnorm(), but the brackets aren't essential for the help. 4.4.2.1 Quickfire Questions Type in the box the binomial counterpart to dnorm()? Type in the box the binomial counterpart to pnorm()? Type in the box the binomial counterpart to qnorm()? Explain This - I don't get the answers The counterpart functions all start with the same letter, d, p, q, it is just the distribution name that changes, binom, norm, t - though we haven't quite come across the t-distribution yet. dbinom() is the binomial equivalent to dnorm() pbinom() is the binomial equivalent to pnorm() qbinom() is the binomial equivalent to qnorm() There is also rnorm() and rbinom() but we will look at them another time. 4.4.3 dnorm() - The Density Function for the Normal Distribution Using dnorm, like we did with dbinom, we can plot a normal distribution. This time however we need: x, a vector of quantiles (in other words, values on the x axis) the mean of our data and standard deviation sd of our data. We will use IQ as an example. Many Psychologists are interested in studying IQ, perhaps in terms of heritability, or interested in controlling for IQ in their own studies to rule out any effect (e.g. clinical and autism studies). 4.4.3.1 Task 1: Standard Deviations and IQ Score Distribution Copy the below code into a new script and run it. Remember that you will need to call tidyverse to your library first. e.g. library(&quot;tidyverse&quot;). This code creates the below plot showing a normal distribution of IQ scores (M = 100, SD = 15) ranging from 40 to 160. These are values considered typical for the general population. # First we set up our range of values from 40 to 160 IQ_data &lt;- tibble(IQ_range = c(40, 160)) # Then we plot the distribution of IQ_data, where we have M = 100 and SD = 15 ggplot(IQ_data, aes(IQ_range)) + stat_function(fun = dnorm, args = list(mean = 100, sd = 15)) + labs(x = &quot;IQ Score&quot;, y = &quot;probability&quot;, title = &quot;Distribution of IQ Scores&quot;) + theme_classic() Figure 4.3: CAPTION THIS FIGURE!! Which part of the code do you need to change to alter the SD of your plot? mean = 100 sd = 15 (40, 160) Now copy and edit the above code to plot a distribution with mean = 100 and sd = 10, and compare the first plot to your own plot. 4.4.3.2 Group Discussion Point What does changing the sd do to the shape of the distribution? Spend a few minutes editing the code and discussing with your group to answer the following questions: What happens to the shape of the distribution if you change the sd from 10 to 20? the distribution gets narrower the distribution gets wider What happens to the shape of the distribution if you change the sd from 10 to 5? the distribution gets narrower the distribution gets wider What does a small or large standard deviation in your sample tell you about the data you have collected? Explain This - I don't get Standard Deviations! Changing the SD from 10 to 20 means a larger standard deviation so you will have a wider distribution. Changing the SD from 10 to 5 means a smaller standard deviation so you will have a narrower distribution. Smaller SD results in a narrower distribution meaning that the data is less spread out; larger SD results in a wider distribution meaning the data is more spread out. A note on the Standard Deviation: You will know from your lectures that you can estimate data in two ways: point-estimates and spread estiamtes. The mean is a point-estimate and condenses all your data down into one data point - it tells you the average value of all your data but tells you nothing about how spread out the data is. The standard deviation however is a spread estimate and gives you an estimate of how spread out your data is from the mean - it is a measure of the standard deviation from the mean. So imagine we are looking at IQ scores and you test 100 people and get a mean of 100 and an SD of 5. This means that the vast majority of your sample will have an IQ around 100 - probably most will fall within 1 SD of the mean, meaning that most of your participants will have an IQ of between 95 and 105. Now if you test again and find a mean of 100 and an SD of 20, this means your data is much more spread out. If you take the 1 SD approach again then most of your participants will have an IQ of between 80 and 120. So one sample has a very tight range of IQs and the other sample has a very wide range of IQs. All in, from the point-estimate and spread estimate of your data you can tell the shape of your sample distribution. So far so good! But in the above example we told dnorm() the values at the limit of our range and it did the rest; we said give us a range of 40 to 160 IQ scores. However, we could plot it another way by telling dnorm() the sequence, or range, of values we want and how much precision we want between them. 4.4.3.3 Task 2: Changing Range and Step Size of The Normal Distribution Copy the code below in to your script and run it. Here we plot the standard Normal Distribution from -4 to 4 in steps of 0.01. We have also stated a mean of 0 and a sd of 1. ND_data &lt;- tibble(ND_range = seq(-4, 4, 0.01)) ggplot(ND_data, aes(ND_range)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 1)) + labs(x = &quot;SD units&quot;, y = &quot;probability&quot;, title = &quot;The Normal Distribution&quot;) + theme_classic() (#fig:ND_example)CAPTION THIS FIGURE!! 4.4.3.4 Quickfire Questions Fill in the box to show what you would type to get a range and step size of -10 to 10 in steps of .05: ND_data &lt;- Now that you know what to change, plot the normal distribution with the range of -10 to 10, in steps of 0.05, with a mean of 0, and a standard deviation of 1. Compare your new plot the the original one we created. What change is there in the distribution? Distribution widens No change in distribution Distribution narrows To three decimal places, what is the probability of the value 0 in the normal distribution (M = 0, SD = 1): Explain This - I don't understand the answer! To change the distribution you would write: ND_data &lt;- tibble(ND_range = seq(-10, 10, 0.05)) However, when comparing the plots, whilst the plot itself may look thinner, the distribution has not changed. The change in appearance is due to the range of sd values which have been extended from -4 and 4 to -10 and 10. The density of values within those values has not changed however and you will see, more clearly in the second plot, that values beyond -3 and 3 are very unlikely. You can test this by looking at the dnorm() function. Note that it only looks at the mean and the standard deviation. So if you asked for dnorm(x = 0, mean = 0, sd = 1) you will get p = .399 every time because the range does not matter! 4.4.4 pnorm() - The Probability or Distribution Function Just as dnorm() works like dbinom(), pnorm() works just like pbinom(). So, pnorm(), given the mean and sd of our data, returns the cumulative density function (cumulative probability) that a given probability (p) lies at a specified cut-off point and below, unless lower.tail = FALSE is specified in which case it is from the cut-off point and above. We will use height to give a concrete example. Say that we test a sample of students (M = 170cm, SD = 7). If we want to calculate the probability that a given student is 150cm or shorter we would do the following: # lower.tail = TRUE means lower than and including the value of X # TRUE is the default so we don&#39;t actually need to declare it pnorm(150, 170, 7, lower.tail = TRUE) This tells us that finding the probability of someone 150cm or smaller in our class is p = 0.0021374 or 0.21%. This is a very small probability and suggests that it is pretty unlikely to find someone of 150cm in our class. This is mainly because of how small the standard deviation of our distribution is. Think back to what we said earlier about narrow standard deviations round the mean! 4.4.4.1 Task 3: Calculating Cumulative Probability of Height Edit the pnorm code above to calculate the probability that a given student is 190cm or taller. To three decimal places, as in Task 3, what is the probability of a student being 190cm or taller in this class? Explain This - I don't understand the answer! The answer is .002. See the solution code at the bottom of the page to see how to complete Task 3. There is a difference in where you need to specify the cut-off point in the pbinom (discussed in the preclass activity) and pnorm functions for values above x, i.e. when lower.tail = FALSE. If you had discrete data, say the number of coin flips that result in heads, and wanted to calculate the probability above x, you would apply pbinom and have to specify your cut-off point as x-1 to include x in your calculation. For example, to calculate the probability of 4 or more 'heads' occuring in 10 coin flips, you would specify pbinom(3, 10, 0.5, lower.tail = FALSE) as lower.tail includes the value you state. For continuous data, however, such as height, you would be applying pnorm and therefore can specify your cut-off point simply as x. In the above example, for the cut-off point of 190, a mean of 170 and standard deviation of 7, you can write pnorm(190, 170, 7, lower.tail = FALSE). The way to think about this is that setting x as 189 on a continuous scale, when you only want all values greater than 190, would also include all the possible values between 189 and 190. Setting x at 190 starts it at 190.0000000...001. This is a tricky difference between pbinom and pnorm to recall easily, so best include this explanation point in your portfolio to help you carry out the correct analyses in the future! 4.4.4.2 Task 4: Using Figures to Calculate Cumulative Probability Have a look at the distribution below: Figure 4.4: CAPTION THIS FIGURE!! Using the information in the figure, and the mean and SD as above, calculate the cumulative probability of the shaded area. Helpful Hint You already have your mean and standard deviations to input in pnorm, look at the shaded area to obtain your cut-off point. What should the lower.tail call be set to according to the shaded area? 4.4.4.3 Quickfire Question To three decimal places, what is the cumulative probability of the shaded area in Task 4? Explain This - I don't get this answer The answer should be .016. See the solution code at the bottom of the page for Task 4. lower.tail is set to FALSE as you want the area to the right. 4.4.5 qnorm() - The Quantile Function Using qnorm we can do the inverse of pnorm(), and instead of finding out the cumulative probability from a given set of probabilities, we can find a cut-off value given a desired probability. For example, what is the maximum IQ a student would have if they were in the bottom 10% of the IQ distribution (M = 100 &amp; SD = 15)? qnorm(0.1, 100, 15) # note that we need to convert 10% to a probability (0.1). So anyone with an IQ of 80.8 or lower would be in the bottom 10%. To recap, we have calculated the inverse cumulative density function (or inverse of the cumulative probability) of the lower tail of the distribution, with a cut-off of a probability of 0.1 (10%), illustrated in purple below: Figure 4.5: CAPTION THIS FIGURE!! 4.4.5.1 Task 5: Using pnorm and qnorm to find probability and cut-off Values Calculate the lowest IQ score a student must have to be in the top 5% of the above distribution. More challenging: Using the appropriate normal distribution function, calculate the probability that a given student will have an IQ between 105 and 110, on a normal distribution of mean = 100, sd = 15. Helpful Hint Part 1: Remember to include the lower.tail call if required! If you are unsure, visualise what you are trying to find (i.e. the lowest IQ score you can have to be in top 5%) by sketching it out on a normal distribution curve. It may help to reverse the question to sound more like the previous example. Part 2: For the second part, each function, not necessarily qnorm, gives one value, so you are looking to do separate calculations for each IQ. Then you have to combine these two values, but are you summing or subtracting them? Is it more or less likely for students to have an IQ that falls between this range than above or below a cut-off? 4.4.5.2 Quickfire Questions To one decimal place, enter your answer for Task 5 part 1: What is the lowest IQ score a student must have to be in the top 5% of the distribution? To two decimal places, enter your answer for Task 5 part 2: What is the probability that a student will have an IQ between 105 and 110, on a normal distribution of mean = 100, sd = 15? Explain This - I dont get this answer The question can be rephrased as what value would give you 95% of the distribution - and the answer would be 124.7. See the solution code for Task 5 at the bottom of the page. You could use pnorm() to establish the probability of an IQ of 110. And you could use pnorm() again to establish the probability of an IQ of 105. The answer is the difference between these two probabilities and should be .12. So a 12% probability. See the solution code for Task 5 at the bottom of the page. Job Done! You have now recapped probability and the binomial and normal distributions. You should now be ready to complete the Homework Assignment for this lab. The assignment for this Lab is FORMATIVE and is NOT to be submitted and will NOT count towards the overall grade for this module. However you are strongly encouraged to do the assignment as it will continue to boost your skills which you will need in future assignments. If you have any questions, please post them on the slack forum under the channel #level2_2018. Finally, don't forget to add any useful information to your Portfolio before you leave it too long and forget. Excellent work! You are Probably an expert! Now go try the assignment! 4.5 InClass Activity 2 (Additional) In the preclass activity we focussed a lot on basic probability and binomial distributions. If you have followed it, and understood it correctly, you should be able to have a go at this following scenario to help deepen your understanding of the binomial distribution. Have a go and see how you get on. Solutions are at the bottom of the page. 4.5.1 Return of the Binomial You design a psychology experiment where you are interested in how quickly people spot faces. Using a standard task you show images of faces, houses, and noise patterns, and ask participants to respond to each image by saying 'face' or 'not face'. You set the experiment so that, across the whole experiment, the number of images per stimuli type are evenly split (1/3 per type) but they are not necessarily evenly split in any one block. Each block contains 60 trials. To test your experiment you run 1 block of your experiment and get concerned that you only saw 10 face images. As this is quite a low number out of a total of 60 you think something might be wrong. You decide to create a probability distribution for a given block of your experiment to test the likelihood of seeing a face (coded as 1) versus not seeing a face (coded as 0). You start off with the below code but it is incomplete. blocks_5k &lt;- replicate(n = NULL, sample(0:1, size = NULL, replace = NULL, c(2/3,1/3)) %&gt;% sum()) Copy the code to a script and replace the NULLs with the correct value and statement to establish how many different number of faces you might seen in 1 block of 60 trials, over 5000 replications. Answering the following questions to help you out: 4.5.1.1 Quickfire Questions The n NULL relates to: Number of replications Number of blocks in the experiment Number of trials in the experiment Number of faces in the experiment The size NULL relates to: Number of faces in a block Number of houses in a block Number of trials in a block Number of noise patterns in a block replace should be set to: TRUE FALSE Explain This - I don't understand these answers replicate is how many times you want to run the sample, n = 5000. The number of trials is the size, 60. And in order for the code to work, and to not run out of options, you need to set replace as TRUE. You saw this exact same code in the preclass so be sure to have had a look there. The solution code is also below at the bottom of the page under Binomial Task. Portfolio Point - Why the odd weight of probability? Note that the probability (prob) is set to weight not seeing a face (2/3) more than seeing a face (1/3). Why is that? Remember that in this instance you have three different stimuli (face, house and noise) and therefore have a greater chance of not seeing a face in the experiment (i.e. you see a house or noise) than seeing a face in the experiment. You need to remember when using sample (?sample) to set the probability appropriately or you will end up with an incorrect calculation. Your blocks_5k code has created 5000 values where each value represents the number of faces you 'saw' in each individual block of 60 trials - like the number of heads you see in 10 flips. If your code has worked then running the below chunk should produce a probability distribution of your data. Run it and see. If not, something has gone wrong; debug or compare to the solution at the bottom of the page. data5k &lt;- tibble(faces_per_block = blocks_5k) %&gt;% count(faces_per_block) %&gt;% mutate(p = n / 5000) ggplot(data5k, aes(faces_per_block, p)) + geom_col() + theme_light() If you view data5k, you will now see three columns: faces_per_block tells you all the number of face trials seen in a block of 60 trials. Note that it does not run from 0 to 60. n tells you the number of blocks in which the corresponding number of face trials appeared, e.g. in 226 blocks there were 15 face trials. (Remember this is sampled data so your values may be different!). p tells you the probability of the corresponding number of faces_per_block. From your data5k tibble, what was the probability of seeing only 10 faces in a block? Ours was 0.003 but remember that we are using a limited number of replications here so our numbers will vary from yours. To find this value you can use View() or glimpse() on data5k. 4.5.1.2 Group Discussion Point Discuss in your groups the following questions: If the probability of seeing only 10 faces was p = 0.003, what does that say about whether our experiment is coded correctly or not? We only have data for 27 potential different number of face trials within a block. Why does this value not stretch from 0 to 60, or at least have more potential values than 27? Portfolio Point - What does it say about your experimental design? On the first point, it says that the probability of having a block with only 10 faces is pretty unlikely - it has only a small probability. However, it is still possible. Does that say there is an error in your code? Not necessarily as it is possible you just happened to get a block with only 10 faces. Despite it being a low probability it is not impossible. What it might say however is that if this is something that concerns you, this potentially low number of faces in a block, then you need to think about the overall design of your experiment and consider having an equal weighting of stimuli in each block. On the second point, you need to think about the width of the distribution. The probability of a face is 1/3 or p = .33. If there are 60 trials per block then that works out at approx on average 20 faces per block. The further you move away from that average, 20, the less probable it is to see that number of faces. E.g. 10 faces had a very low probability as will 30 faces. 40 faces will have almost no probability whatsoever. In only 5000 trials not all values will be represented. 4.5.1.3 Quickfire Questions Using the appropriate binom() function, to three decimal places, type in the actual probability of 10 faces in 60 trials: Using the appropriate binom() function, to three decimal places, type in the actual probability of seeing more than, but not including, 25 faces in a block of 60 trials? Using the appropriate binom() function, type in what would be the minimum number of faces needed in a block of 60 trials to maintain at least 5% of trials being faces. Helpful Hint You are looking for the probability of one specific value of faces appearing, i.e. 10 faces in a total of 60 trials. You are not looking for the cumulative probability or a range of values. What does this tell you about the apporpriate binom() function you require? Also, in the sample function, we created a vector of probability dependent on the appearance of both faces and other stimuli. For the binom() function, you need to include only the probability of faces appearing. You are looking for the cumulative probability for seeing more than but not including 25 faces in a block of 60 trials. What does this tell you about the apporpriate binom() function you require? Remember to set an appropriate call for lower.tail! The probability level has been provided for you (5% 0r .05). With this you are looking to find the minimum number of faces required to maintain this probability value. What does this tell you about the appropriate binom() function you require? If you are looking for the minimum number of faces required, is lower.tail set to TRUE or FALSE? Explain This - I don't get these answers The answer would be .002 as you require the density, dbinom(), of 10 faces out of a possible 60 given a probability of 1/3. See the Binomial Quickfire Questions solution below for the code. The answer would be .068 as you require the cumulative probability, pbinom(), of more than 25 faces out of 60 given a probability of 1/3. As it is more than 25, setting 25 with lower.tail set as FALSE will give you 26 and above (remember this is a binomial distribution, so where you set your cut-off is very important). See the Binomial Quickfire Questions solution below for the code. The answer would be 14 using the qbinom() function based on wanting to maintain 5% probability of faces in 60 trials. See the Binomial Quickfire Questions solution below for the code. Well done on completing this activty! Once you have completed the main body inclass assignment on the Normal Distribution, you should be ready to complete the assignment for this lab. Good luck and remember to refer back to the codes and questions in this lab and the preclass if you get stuck!. 4.6 InClass Activity 3 (Additional) One thing we looked at in the lecture and in the preclass activity were some rather &quot;basic&quot; ideas about probability using cards. Here is a couple of very quick questions using dice (or die if you prefer) to make sure you understand this. Have a go and see how you get on. If you don't get the right answers, be sure to ask in the lab or post on the slack forum rguppies.slack.com under the channel #level2_2018. Finally, don't forget to add any useful information to your Portfolio before you leave it too long and forget. 4.6.1 Back to Basics Show your answer either as a fraction (e.g. 1/52) or to 3 decimal places (e.g. 0.019). What is the probability of rolling a 4 on a single roll of a six-sided dice? What is the probability of rolling a 4, 5 or 6 on a single roll? What is the probability of rolling a 4 and then a 6 straight after? Explain this - I dont get these answers Remember: probability is the number of times or ways an event can occur divided by the total number of all possible outcomes. When you have two separate events the probability of them both happening is the cumulative probability which is calculated by mutliplying all the individual probabilities together. .167 as you have 1 outcome over 6 possibilities .5 as you have 3 outcomes over 6 possibilities .028 as you have 1 out of 6 on both occasions so the cumulative is used. Well done on completing this activty! Once you have completed the main body inclass assignment on the Normal Distribution, you should be ready to complete the assignment for this lab. Good luck and remember to refer back to the codes and questions in this lab and the preclass if you get stuck!. 4.7 Assignment This is a formative assignment but we strongly encourage you to do the assignment as the knowledge gained from the practical activities in this lab will be super important to your future-self! Lab 4: Formative Probability Assignment! In order to complete this assignment yoU first have to download the assignment .Rmd file which you need to edit - titled GUID_Level2_Lab4.Rmd. This can be downloaded within a zip file from the below link. Once downloaded and unzipped you should create a new folder that you will use as your working directory; put the .Rmd file in that folder and set your working directory to that folder through the drop-down menus at the top. Download the Assignment .zip file from here or on Moodle. Now open the assignment .Rmd file within RStudio. You will see there is a code chunk for each of the 10 questions. Much as you did in the previous assignments follow the instructions on what to edit in each code chunk. This will often be entering code or a single value and will be based on the skills learnt in the current lab as well as all previous labs. 4.7.1 Probability Assignment In the lab you recapped and expanded on your understanding of probability, including a number of binom and norm functions as well as some more basic ideas on probability. You will need these skills to complete the following assignment so please make sure you have carried out the PreClass and InClass activities before attempting this formative assignment. Remember to follow the instructions and if you get stuck at any point to post questions on rguppies.slack.com channel #level2_2018. 4.7.2 Before starting let's check The .Rmd file is saved into a folder on your computer that you have access to and you have manually set this folder as your working directory. For assessments we ask that you save it with the format GUID_Level2_Lab4.Rmd where GUID is replaced with your GUID. Though this is a formative assessment it may be good practice to do the same here. Note: You should complete the below code chunks by replacing the NULL (except the library chunk where the appropriate code should just be entered). Not all answers require coding. On those that do not require code you can enter the answer as either code, mathematical notation, or an actual single value. Pay attention to the number of decimal places required. 4.7.3 Part 1: Library There is a good chance that you will need the tidyverse library at some point in this exercise so load it in the code chunk below: # hint: something to do with library() and tidyverse 4.7.4 Part 2: Basic probability and the binomial distribution You are conducting an auditory discrimination experiment in which participants have to listen to a series of sounds and determine whether the sound was human or not. On each trial participants hear one brief sound (100 ms) and must report whether the sound was from a human (coded as 1) or not (coded as 0). The sounds were either: a person, an animal, a vehicle, or a tone. 4.7.5 Question 1 On a single trial, what is the probability that the sound will not be a person? Replace the NULL in the t1 code chunk with either mathematical notation or a single value. If entering a single value, give your answer to two decimal places. t1 &lt;- NULL 4.7.6 Question 2 Over a sequence of 4 trials, with replacement, what is the probability of the following sequence of sounds: animal, animal, vehicle, tone? Replace the NULL in the t2 code chunk with either mathematical notation or a single value. If entering a single value, give your answer to three decimal places. t2 &lt;- NULL 4.7.7 Question 3 Over a sequence of four trials, without replacment, what is the probability of the following sequence of sounds: person, tone, animal, person? Replace the NULL in the t3 code chunk with either mathematical notation or a single value. t3 &lt;- NULL 4.7.8 Question 4 Replace the NULL below with code using the appropriate binomial distribution function to determine the probability of hearing 17 'tone' trials in a sequence of 100 trials. Assume the probability of a tone on any single trial is 1 in 4. Store the output in t4. t4 &lt;- NULL 4.7.9 Question 5 Replace the NULL below with code using the appropriate binomial distribution function to determine what is the probability of hearing 30 'vehicle' trials or more in a sequence of 100 trials. Assume the probability of a vehicle trial on any one trial is 1 in 4. Store the output in t5. Hint: do we want the upper or lower tails of the distribution. t5 &lt;- NULL 4.7.10 Question 6 If a block of our experiment contained 100 trials, enter a line of code to run 10000 replications of one block, summing how many living sounds were heard in each replication. Code 1 for living sounds (person/animal) and 0 for non living sounds (vehicle/tone) and assume the probability of a living sound on any given trial is \\(p = .5\\). t6 &lt;- NULL 4.7.11 Part 2: The Normal Distribution Previously, in Lab 2, we looked at an ageing research project investigating differences in visual processing speed between younger (M = 22 years) and older adults (M = 71 years). One check in this experiment, prior to further analysis, is to make sure that older participants do not show signs of mild cognitive impairment (early symptoms of Alzheimer's disease). To do this we carry out a battery of cognitive tests to screen for such symptoms. One of the tests is the D2 test of attention which is a target cancellation task (i.e. participants cross out all letter d's with two dashes from a line of letters - see Lab 2 for an example). It is designed to test people's selective and sustained attention and visual scanning speed. The results of the test give a single score of Concentration Performance for each participant. The key piece of information for this analysis is that the distributions of D2 test scores are typically normally distributed (M = 100, SD = 10). 4.7.12 Question 7 Replace the NULL below with code using the appropriate function to determine the probability that a given participant will have a D2 score of exactly 90? Store the output in t7 t7 &lt;- NULL 4.7.13 Question 8 Replace the NULL below with code using the appropriate function to determine the probability that a given participant will have a D2 score of 120 or more? Store the output in t8 t8 &lt;- NULL 4.7.14 Question 9 Replace the NULL below with code using the appropriate function(s) to determine what is the difference in scores that cut off the top and bottom 5% of the distribution? Store the output in t9 t9 &lt;- NULL 4.7.15 Question 10 Finally, if a participant says to you that they are worried because they have heard that their Concentration Performance was in the bottom 2% of scores on the distribution, what is the maximum D2 score that they can have? Replace the NULL below with a single value to two decimal places. Do not enter code. Store this in t10 t10 &lt;- NULL Finished Well done, you are finshed! Now you should go check your answers against the solution file which can be found on Moodle. You are looking to check that the resulting output from the answers that you have submitted are exactly the same as the output in the solution - for example, remember that a single value is not the same as a coded answer. Where there are alternative answers it means that you could have submitted any one of the options as they should all return the same answer. If you have any questions please post them on the moodle forum or on the rguppies.slack.com forum #level2_2018. See you in the next lab! 4.8 Solutions to Questions Below you will find the solutions to the questions for the Activities for this chapter. Only look at them after giving the questions a good try and speaking to the tutor about any issues. 4.8.1 InClass Activities 4.8.1.1 Task 1 Solution Task 1 ggplot(IQ_data, aes(IQ_data)) + stat_function(fun = dnorm, args = list(mean = 100, sd = 10)) + labs(x = &quot;IQ Score&quot;, y = &quot;probability&quot;, title = &quot;Distribution of IQ Scores&quot;) + theme_classic() Click the tab to see the solution 4.8.1.2 Task 2 Solution Task 2 ND_data &lt;- tibble(ND_range = seq(-10,10,0.05)) ggplot(ND_data, aes(ND_range)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 1)) + labs(x = &quot;SD units&quot;, y = &quot;probability&quot;, title = &quot;The Normal Distribution&quot;) + theme_classic() Click the tab to see the solution 4.8.1.3 Task 3 Solution Task 3 # Key thing is to set lower.tail to FALSE to calculate the above area. pnorm(190, 170, 7, lower.tail = FALSE) Click the tab to see the solution 4.8.1.4 Task 4 Solution Task 4 # the area is 185cm and above. # Key thing is to set lower.tail to FALSE to calculate the above area. pnorm(185, 170, 7, lower.tail = FALSE) Click the tab to see the solution 4.8.1.5 Task 5 Solution Task 5 # Question 1 qnorm(0.95, 100, 15, lower.tail = TRUE) # Question 2 pnorm(105, 100, 15, lower.tail = FALSE) - pnorm(110, 100, 15, lower.tail = FALSE) Click the tab to see the solution 4.8.2 InClass Binomial Activities 4.8.2.1 Binomal CODE - Replace The Nulls Solution Binomial Task blocks_5k &lt;- replicate(n = 5000, sample(0:1, size = 60, replace = TRUE, c(2/3, 1/3)) %&gt;% sum()) Click the tab to see the solution 4.8.2.2 Binomial Quickfire Questions Solutions Solution Binomial Quickfire Questions # this time we switch it back to 1/3 prob because a face only has 1/3 prob of being shown 1. dbinom(10, 60, 1/3) ## NB: asks for more than 25, not 25 and more. Lower.tail is set to FALSE 2. pbinom(25, 60, 1/3, lower.tail = FALSE) ## lower.tail is true here becase you want lower than the cut-off. qbinom(.05, 60, 1/3) Click the tab to see the solution 4.8.3 Assignment Solution Part 1: Library There is a good chance that you will need the tidyverse library at some point in this exercise so load it in the code chunk below: # hint: something to do with library() and tidyverse library(tidyverse) Part 2: Basic probability and the binomial distribution You are conducting an auditory discrimination experiment in which participants have to listen to a series of sounds and determine whether the sound was human or not. On each trial participants hear one brief sound (100 ms) and must report whether the sound was from a human (coded as 1) or not (coded as 0). The sounds were either: a person, an animal, a vehicle, or a tone. 4.8.3.1 Question 1 On a single trial, what is the probability that the sound will not be a person? Replace the NULL in the t1 code chunk with either mathematical notation or a single value. If entering a single value, give your answer to two decimal places. t1 &lt;- 3/4 t1 &lt;- 0.75 t1 ## [1] 0.75 4.8.3.2 Question 2 Over a sequence of 4 trials, with replacement, what is the probability of the following sequence of sounds: animal, animal, vehicle, tone? Replace the NULL in the t2 code chunk with either mathematical notation or a single value. If entering a single value, give your answer to three decimal places. t2 &lt;- (1/4) * (1/4) * (1/4) * (1/4) t2 &lt;- .004 t2 ## [1] 0.004 4.8.3.3 Question 3 Over a sequence of four trials, without replacment, what is the probability of the following sequence of sounds: person, tone, animal, person? Replace the NULL in the t3 code chunk with either mathematical notation or a single value. t3 &lt;- (1/4) * (1/3) * (1/2) * (0/1) t3 &lt;- 0 t3 ## [1] 0 4.8.3.4 Question 4 Replace the NULL below with code using the appropriate binomial distribution function to determine the probability of hearing 17 'tone' trials in a sequence of 100 trials. Assume the probability of a tone on any single trial is 1 in 4. Store the output in t4. t4 &lt;- dbinom(17, 100, 1/4) t4 ## [1] 0.01651564 4.8.3.5 Question 5 Replace the NULL below with code using the appropriate binomial distribution function to determine what is the probability of hearing 30 'vehicle' trials or more in a sequence of 100 trials. Assume the probability of a vehicle trial on any one trial is 1 in 4. Store the output in t5. Hint: do we want the upper or lower tails of the distribution. t5 &lt;- pbinom(29, 100, 1/4, lower.tail = FALSE) t5 &lt;- dbinom(30:100, 100, 1/4) %&gt;% sum() t5 ## [1] 0.149541 4.8.3.6 Question 6 If a block of our experiment contained 100 trials, enter a line of code to run 10000 replications of one block, summing how many living sounds were heard in each replication. Code 1 for living sounds (person/animal) and 0 for non living sounds (vehicle/tone) and assume the probability of a living sound on any given trial is \\(p = .5\\). t6 &lt;- replicate(10000, sample(0:1, 100, TRUE, c(.5,.5)) %&gt;% sum()) # Here we are only showing the first 10 values of 10000. # Your numbers will vary from ours due to random sampling # What would be assessed is the actual codeline exists and is accurate. glimpse(t6) ## int [1:10000] 49 46 55 47 44 37 41 54 48 48 ... Part 2: The Normal Distribution Previously, in Lab 2, we looked at an ageing research project investigating differences in visual processing speed between younger (M = 22 years) and older adults (M = 71 years). One check in this experiment, prior to further analysis, is to make sure that older participants do not show signs of mild cognitive impairment (early symptoms of Alzheimer's disease). To do this we carry out a battery of cognitive tests to screen for such symptoms. One of the tests is the D2 test of attention which is a target cancellation task (i.e. participants cross out all letter d's with two dashes from a line of letters - see Lab 2 for an example). It is designed to test people's selective and sustained attention and visual scanning speed. The results of the test give a single score of Concentration Performance for each participant. The key piece of information for this analysis is that the distributions of D2 test scores are typically normally distributed (M = 100, SD = 10). 4.8.3.7 Question 7 Replace the NULL below with code using the appropriate function to determine the probability that a given participant will have a D2 score of exactly 90? Store the output in t7 t7 &lt;- dnorm(90, 100, 10) t7 ## [1] 0.02419707 4.8.3.8 Question 8 Replace the NULL below with code using the appropriate function to determine the probability that a given participant will have a D2 score of 120 or more? Store the output in t8 t8 &lt;- pnorm(120, 100, 10, lower.tail = FALSE) t8 ## [1] 0.02275013 4.8.3.9 Question 9 Replace the NULL below with code using the appropriate function(s) to determine what is the difference in scores that cut off the top and bottom 5% of the distribution? Store the output in t9 t9 &lt;- qnorm(.95, 100, 10) - qnorm(.05, 100, 10) t9 ## [1] 32.89707 4.8.3.10 Question 10 Finally, if a participant says to you that they are worried because they have heard that their Concentration Performance was in the bottom 2% of scores on the distribution, what is the maximum D2 score that they can have? Replace the NULL below with a single value to two decimal places. Do not enter code. Store this in t10 t10 &lt;- 79.46 t10 ## [1] 79.46 Finished Well done, you are finshed! Now you should go check your answers against the solution file which can be found on Moodle. You are looking to check that the resulting output from the answers that you have submitted are exactly the same as the output in the solution - for example, remember that a single value is not the same as a coded answer. Where there are alternative answers it means that you could have submitted any one of the options as they should all return the same answer. If you have any questions please post them on the moodle forum or on the rguppies.slack.com forum #level2_2018. See you in the next lab! "],
["permutation-tests-a-skill-set.html", "Lab 5 Permutation Tests - A Skill Set 5.1 Overview 5.2 PreClass Activity 5.3 InClass Activity 5.4 Assignment 5.5 Solutions to Questions", " Lab 5 Permutation Tests - A Skill Set 5.1 Overview In this week's lab you will perform your first hypothesis test using a procedure known as a permutation test. We will help you learn how to do this through building and running data simulation procedures. In order to complete this lab you will require the following skills which we will teach you today: Skill 1: Generating random numbers with base::rnorm() Skill 2: Permuting values with base::sample() Skill 3: Creating a &quot;tibble&quot; (a type of data table) using tibble::tibble() Skill 4: Computing and extracting a difference in group means using dplyr::pull() and purrr::pluck() Skill 5: Creating your own custom functions using base::function() Skill 6: Repeating operations using base::replicate() To many, a lot of statistics must seem a bit like blind faith as it deals with estimating quantities we haven't observed (or can't observe), e.g. the mean of a whole population. As such we have to know if we can trust our procedures for making estimations and inferences because we rarely get a chance to compare the estimated values to the true values to see if they match up. One way to test a procedure, and in turn learn about statistics, is through data simulation. In simulations we create a population and then draw samples and run tests on the data, i.e. on this known population. By running lots of simulations we can test our procedures and make sure they are acting as we expect them to. This approach is known as a Monte Carlo simulation, named after the city famous for the many games of chance that are played there. Portfolio Point - Monte Carlo or Bust You can go read up on the Monte Carlo approach if you like. It can however get quite indepth, as having a brief glance at the wikipedia entry on it highlights. The main thing to keep in mind is that the method involves creating a population and continually taking samples from that population in order to make an inference. This is what we will show you in the lab. Data simulation and &quot;creating&quot; your own datasets, to see how tests work, is a great way to understand statistics. When doing this lab, keep in mind how easy it really is to find a significant result if even randomly created data can give a significant result. This may help dispell any notion that there is something inherently important about a significant result, in itself. 5.2 PreClass Activity We will now take each skill in turn. Be sure to try them all out. It looks a lot of reading but it is mainly just showing you the output of the functions so you can see you are doing it correctly. The key thing is to try them yourselves and don't be scared to change things to see what might happen if you do it slightly differently. We will also ask a couple of questions along the way to make sure you understand the skills. 5.2.1 Skill 1: Generating Random Numbers The base::rnorm() function generates values from a normal distribution and takes the following arguments: n: the number of observations to generate mean: the mean of the distribution (default 0) sd : the standard deviation of the distribution (default 1) To generate 10 or even 50 random numbers from a standard normal distribution (M = 0, SD = 1), you would use rnorm(10) or rnorm(50) respectively. Type rnorm(50) into your console and see what happens. Use the below example for rnorm(10) to help you. Try increasing n to 1000. rnorm(10) ## [1] -0.296868007 0.763014206 0.008244985 0.845809306 -0.991485725 ## [6] -0.059350973 -0.483682464 -1.896801122 0.505644421 -1.019781824 5.2.1.1 Quickfire Question If you enter rnorm(50) again you will get different numbers. Why? I have made a mistake The numbers are random R has made a mistake Phil has made a mistake If you want to change the mean or sd, you would need to pass additional arguments to the function as shown below. rnorm(n = 10, mean = 1, sd = .5) Try changing the mean and sd values a couple of times and see what happens. You get different numbers again that will be around the mean you set! Set a mean of 10, then a mean of 100, to test this. Finally, for this Skill, you can concatenate (i.e. link) numbers together into a single vector using the c() function from base R. For instance, say you wanted to create a vector with two sets of 50 random numbers from two separate samples: one set of 50 with a mean of 75 and the other with a mean of 90, you would use: random_numbers &lt;- c(rnorm(50, 75), rnorm(50, 90)) 5.2.1.2 Quickfire Question In the above example code, what is the standard deviation of the two samples you have created? 50 75 90 1 Explain This - I don't get this answer! What is the default sd of the function? Both populations would have an sd of 1, because that is the default, although you could easily change that. Try it out! It is always good to check that your new vector has the right number of data points in it - i.e. the total of the two samples; a sanity check if you will. The new vector random_numbers should have 100 elements. You could verify this using the length() function: length(random_numbers) ## [1] 100 5.2.1.3 SKILL 1 COMPLETE - Now take a look at Skill 2! 5.2.2 Skill 2: Permuting Values Another thing that is useful to be able to do is to generate permutations of values. Portfolio Point - What are Permutations? A permutation is just a different ordering of the same values. For example, the numbers 1, 2, 3 can be permuted into the following 6 sequences: 1, 2, 3 1, 3, 2 2, 1, 3 2, 3, 1 3, 1, 2 3, 2, 1 The more values you have, the more permutations of the order you have. The number of permutations can be calculated by, for example, 3*2*1, where 3 is the number of values you have. Or through code: factorial(3) = 6. This assumes that each value is used once in the sequence and that each value never changes, i.e. 1234 cannot suddenly become 1235. We can create random permutations of a vector using the sample() function. Let's use one of R's built in vectors: letters. Type letters into the console, as below, and press RETURN/ENTER. You will see it contains all the lowercase letters of the English alphabet. Now, I bet you are wondering what LETTERS does, right? letters ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; ## [18] &quot;r&quot; &quot;s&quot; &quot;t&quot; &quot;u&quot; &quot;v&quot; &quot;w&quot; &quot;x&quot; &quot;y&quot; &quot;z&quot; We can combine base::sample() with letters to put the letters into a random order: Run the below line. Run it again. And again. What do you notice? And why is our output different from yours? (The answer is below) sample(letters) ## [1] &quot;h&quot; &quot;k&quot; &quot;u&quot; &quot;e&quot; &quot;t&quot; &quot;y&quot; &quot;g&quot; &quot;i&quot; &quot;z&quot; &quot;d&quot; &quot;v&quot; &quot;p&quot; &quot;x&quot; &quot;l&quot; &quot;a&quot; &quot;f&quot; &quot;b&quot; ## [18] &quot;q&quot; &quot;m&quot; &quot;c&quot; &quot;w&quot; &quot;r&quot; &quot;s&quot; &quot;o&quot; &quot;j&quot; &quot;n&quot; 5.2.2.1 Quickfire Question If month.name contains the names of the twelve months of the year, how many possible permutations are there of sample(month.name)? 1 12 144 479001600 Portfolio Point - Different samples with sample() Each time you run sample(letters) it will give you another random permutation of the sequence. That is what sample() does - creates a random permutation of the values you give it. Try repeating this command many times in the console. Because there are so many possible sequences, it is very unlikely that you will ever see the same sequence twice! An interesting thing about sample() is that sample(c(1,2,3,4)) is the same as sample(4). And to recap, there would be 24 different permutations based on factorial(4), meaning that each time you type sample(4) you are getting one of those 24 different orders. So what would factorial(12) be? Top Tip: Remember that you can scroll up through your command history in the console using the up arrow on your keyboard; this way, you don't ever have to retype a command you've already entered. 5.2.2.2 SKILL 2 COMPLETE - Now take a look at Skill 3! 5.2.3 Skill 3: Creating Tibbles Tables are important because most of the data we want to analyze comes in a table, i.e. tabular form. There are different ways to get tabular data into R for analysis. One common way is to load existing data in from a data file (for example, using readr::read_csv() which you have seen before). But other times you might want to just type in data directly. You can do this using the tibble::tibble() function. Being able to create a tibble is a useful data analysis skill because sometimes you will want to create some data on the fly just to try certain codes or functions. 5.2.3.1 Entering Data into a Tibble The tibble() function takes named arguments - this means that the name you give each argument within the tibble function, e.g. Y = rnorm(10) will be the name of the column that appears in the table, i.e. Y. It's best to see how it works through an example. tibble(Y = rnorm(10)) ## # A tibble: 10 x 1 ## Y ## &lt;dbl&gt; ## 1 -0.447 ## 2 -0.335 ## 3 -1.66 ## 4 0.537 ## 5 0.382 ## 6 0.216 ## 7 0.448 ## 8 1.08 ## 9 0.519 ## 10 1.26 The above command creates a new table with one column named Y, and the values in that column are the result of a call to rnorm(10): 10 randomly sampled values from a standard normal distribution (mean = 0, sd = 1) - See Skill 1. If however we wanted to sample from two different populations for Y, we could combine two calls to rnorm() within the c() function. Again this was in Skill 1, here we are now just storing it in a tibble. See below: tibble(Y = c(rnorm(5, mean = -10), rnorm(5, mean = 20))) ## # A tibble: 10 x 1 ## Y ## &lt;dbl&gt; ## 1 -8.77 ## 2 -9.16 ## 3 -10.7 ## 4 -9.68 ## 5 -10.6 ## 6 18.6 ## 7 20.7 ## 8 17.6 ## 9 18.3 ## 10 20.8 Now we have sampled a total of 10 observations - the first 5 come from a group with a mean of -10, and the second 5 come from a group with a mean of 20. Try changing the values in the above example to get an idea of how this works. Maybe even add a third group! But, of course, it would be good to know which population each data point refers to and so we should add some group names. We can do this with some additional trickery using the rep() function. 5.2.3.2 Repeating Values to Save Typing Before finalising our table let's learn a little about the base R function, rep(). This is most useful for automatically repeating values in order to save typing. For instance, if we wanted 20 letter &quot;A&quot;s in a row, we would type: rep(&quot;A&quot;, 20) ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; ## [18] &quot;A&quot; &quot;A&quot; &quot;A&quot; The first argument to rep() is the vector containing the information you want repeated, A, and the second argument, times, is the number of times to repeat it; in this case 20. If you wanted to add more information, e.g. if the first argument has more than one element, say &quot;A&quot; and &quot;B&quot;, it will repeat the entire vector that number of times; A B, A B, A B, ... . Note that we enclose &quot;A&quot; and &quot;B&quot; in the c() function so that it is seen as a single argument. rep(c(&quot;A&quot;, &quot;B&quot;), 20) ## [1] &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; ## [18] &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; ## [35] &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; But sometimes we want a specific number of As followed by a specific number of Bs; A A A B B B. If the times argument has the same number of elements as the vector given for the first argument, it will repeat each element of the first vector as many times as given by the corresponding element in the times vector. In other words, for example, times = c(2, 4) for vector c(&quot;A&quot;, &quot;B&quot;) will give you 2 As followed by 4 Bs. rep(c(&quot;A&quot;, &quot;B&quot;), c(2, 4)) ## [1] &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; 5.2.3.3 Quickfire Questions The best way to learn about this function is to play around with it in the console and see what happens. From the dropdown menus, the correct output of the following function would be: rep(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;),(2, 3, 1)) - A A A B B C A A B B B C A A B B C C A B C A B C rep(1:5, 5:1) - 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 5 5 5 5 5 4 4 4 4 3 3 3 2 2 1 1 1 1 1 1 2 2 2 2 3 3 3 4 4 5 1 1 1 1 1 1 1 1 5 5 5 5 5 5 5 5.2.3.4 Bringing it Together in a Tibble Now we know rep(), we can complete our table of simulated data by combining what we've learned about generating random numbers and repeating values. We want our table to look like this: ## # A tibble: 10 x 2 ## group Y ## &lt;chr&gt; &lt;dbl&gt; ## 1 A -9.89 ## 2 A -10.8 ## 3 A -11.9 ## 4 A -9.01 ## 5 A -8.60 ## 6 B 18.5 ## 7 B 19.2 ## 8 B 18.2 ## 9 B 19.8 ## 10 B 18.9 You now know how to create this table. Have a look at the code below and make sure you understand it. We have one column called group where we create As and Bs through rep(), and one column called Y, our data, all in our tibble(): tibble(group = rep(c(&quot;A&quot;, &quot;B&quot;), c(5, 5)), Y = c(rnorm(5, mean = -10), rnorm(5, mean = 20))) Be sure to play around with the code chunk to get used to it. Try adding a third group or even a third column? Perhaps you want to give every participant a random age with a mean of 18, and a sd of 1; or even a participant number. Helpful Hint Try row_number() to create participant numbers. Don't forget, if you wanted to store your tibble, you would just assign it to a name, such as my_data: my_data &lt;- tibble(ID = row_number(1:10), group = rep(c(&quot;A&quot;, &quot;B&quot;), c(5, 5)), Y = c(rnorm(5, mean = -10), rnorm(5, mean = 20)), Age = rnorm(10, 18, 1)) 5.2.3.5 SKILL 3 COMPLETE - Now take a look at Skill 4! 5.2.4 Skill 4: Computing Differences in Group Means You have already learned how to calculate group means using group_by() and summarise(). For example, you might want to calculate sample means for a randomly generated dataset like so: my_data &lt;- tibble(group = rep(c(&quot;A&quot;, &quot;B&quot;), c(20, 20)), Y = c(rnorm(20, 20, 5), rnorm(20, -20, 5))) my_data_means &lt;- my_data %&gt;% group_by(group) %&gt;% summarise(m = mean(Y)) my_data_means ## # A tibble: 2 x 2 ## group m ## &lt;chr&gt; &lt;dbl&gt; ## 1 A 21.2 ## 2 B -20.6 Sometimes what we want though is to calculate the differences between means rather than just the means; so we'd like to subtract the second group mean -20.6 from the first group mean of 21.2, to get a single value, the difference: 41.8. We can do this using the dplyr::pull() and purrr::pluck() functions. pull() will extract a single column from a dataframe and turn it into a vector. pluck() then allows you to pull out an element (i.e. a value or values) from within that vector. vec &lt;- my_data_means %&gt;% pull(m) vec ## [1] 21.21508 -20.60371 We have now created vec which is a vector containing only the group means; the rest of the information in the table has been discarded. Now that we have vec, we can calculate the mean difference as below, where vec is our vector of the two means and [1] and [2] refer to the two means: vec[1] - vec[2] ## [1] 41.8188 But pluck() is also useful, and can be written as so: pluck(vec, 1) - pluck(vec, 2) ## [1] 41.8188 It can also be incorporated into a pipeline as below where we still pull() the means column, m, and then pluck() each value in turn and subtract them from each other. ## whole thing in a pipeline my_data_means %&gt;% pull(m) %&gt;% pluck(1) - my_data_means %&gt;% pull(m) %&gt;% pluck(2) ## [1] 41.8188 However, there is an alternative way to extract the difference between means which may make more intuitive sense. You already know how to calculate a difference between values in the same row of a table using dplyr::mutate(), e.g. mutate(new_column = column1 minus column2). So if you can get the observations in my_data_means into the same row, different columns, you could then use mutate() to calculate the difference. Previously you learned gather() to bring columns together. Well the opposite of gather is the tidyr::spread() function to split columns apart - as below. my_data_means %&gt;% spread(group, m) ## # A tibble: 1 x 2 ## A B ## &lt;dbl&gt; &lt;dbl&gt; ## 1 21.2 -20.6 The spread function (?spread) splits the data in column m by the information, i.e. labels, in column group and puts the data into separate columns. A call to spread() followed by a mutate() can be used to calculate the difference in means - see below: my_data_means %&gt;% spread(group, m) %&gt;% mutate(diff = A - B) ## # A tibble: 1 x 3 ## A B diff ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 21.2 -20.6 41.8 5.2.4.1 Quickfire Question What is the name of the column containing the differences between the means of A and B? means group m diff Finally, if you then wanted to just get diff and throw away everything else in the table: my_data_means %&gt;% spread(group, m) %&gt;% mutate(diff = A - B) %&gt;% pull(diff) ## [1] 41.8188 Portfolio Point - Reading pipes and verbalising tasks Keep in mind that a very useful technique for establishing what you want to do to a dataframe is to verbalise what you need, or to write it down in words, or to say it out loud. Take this last code chunk. What we wanted to do was to spread() the data in m into the groups A and B. Then we wanted to mutate() a new column that is the difference, diff, of A minus B. And finally we wanted to pull() out the value in diff. Often step 1 of writing code or understanding code is knowing what it is you want to do in the first place. After that you just need the correct functions. Fortunately for us a lot of the tidyverse names its functions based on what they specifically do! 5.2.4.2 SKILL 4 COMPLETE - Now take a look at Skill 5! 5.2.5 Skill 5: Creating Your Own Functions In Skills 1 to 4, we have looked at creating and sampling data, storing it in a tibble, and extracting information from that tibble. Now say we wanted to do this over and over again. For instance, we might want to generate 100 random datasets just like the one in Skill 4. It would be a pain to have to type out the tibble() function 100 times or even to copy and paste it 100 times. We'd likely make an error somewhere and it would be hard to read. To help us, we can create a custom function that performs the action you want; in our case, creating a tibble of random data. Remember, a function is just a procedure that takes an input and gives you the same output each time - like a toaster! A function has the following format: name_of_function &lt;- function(arg1, arg2, arg3) { ## body of function goes between these curly brackets; i.e. what the function does for you. ## Note that the last value calculated will be returned if you call the function. } First you define your own function name (e.g. name_of_function) and define the names of the arguments it will take (arg1, arg2, ...) - an argument is the information that you feed into your function, e.g. data. Finally, you state the calculations or actions of the function in the body of the function (the portion that appears between the curly braces). One of the most basic possible functions is one that takes no arguments and just prints a message. Here is an example: hello &lt;- function() { print(&quot;Hello World!&quot;) } So this function is called hello. It can be run by typing hello() in your console and it will give the output of Hello World! every single time you run it; it has no other actions or information. Test this in the console now by typing: hello() ## [1] &quot;Hello World!&quot; Awesome right? Ok, so not very exciting. Let's make it better by adding an argument, name, and have it say Hello to name. hello &lt;- function(name = &quot;World!&quot;) { paste(&quot;Hello&quot;, name) } This new function is again called hello() and replaces the one you previously created. This time however you are supplying what is called a default argument, `name = &quot;World!&quot;, but it still has the same action as the previous function of putting &quot;Hello&quot; and &quot;World!&quot; together. So if you run it you get &quot;Hello World!&quot;. Try it yourself! hello() ## [1] &quot;Hello World!&quot; The difference this time however is that because you have added an argument to the input, you can change the information you give the argument and therefore change the output of the function. More flexible. More exciting. 5.2.5.1 Quickfire Questions Test your understanding by answering these questions: Typing hello(&quot;Phil&quot;) in the console with this new function will give: Hello Heather Hello Phil Hello Niamh Hello Kevin Typing the argument as &quot;is it me you are looking for&quot; will give: Hello is it me you are looking for I just called to say Hello You had me at Hello Hello seems to be the hardest word What argument would you type to get &quot;Hello Dolly!&quot; as the output: Dolly Molly Holly Dolly! Most of the time however we want to create a function that computes a value or constructs a table. For instance, let's create a function that returns randomly generated data from two samples, as we learned in the previous skills - see below. All we are doing is taking the tibble we created in Skill 4 and putting it in the body (between the curly brackets) of the function. gen_data &lt;- function() { tibble(group = rep(c(&quot;A&quot;, &quot;B&quot;), c(20, 20)), Y = c(rnorm(20, 20, 5), rnorm(20, -20, 5))) } This function is called gen_data() and when we run it we get a randomly generated table of two groups, each with 20 people, one with M = 20, SD = 5, the other with M = -20, sd = 5. Try running this gen_data() function in the console a few times; remember that as the data is random, the numbers will be different each time you run it. But say we want to modify the function to allow us to get a table with smaller or larger numbers of observations per group. We can add an argument n and modify the code as follows. Create this function and run it a couple of times through gen_data(). The way to think about this is that every place that n appears in the body of the function (between the curly brackets) it will have the value of whatever you gave it in the arguments, i.e. in this case, 20. gen_data &lt;- function(n = 20) { tibble(group = rep(c(&quot;A&quot;, &quot;B&quot;), c(n, n)), Y = c(rnorm(n, 20, 5), rnorm(n, -20, 5))) } 5.2.5.2 Quickfire Questions How many total participants would there be if you ran gen_data(2)? 2 4 20 40 What would you type to get 100 participants per group? gen_data(50) gen_data(10) gen_dota(100) gen_data(100) Challenge Question: Keeping in mind that functions can take numerous arguments, and that each group in your function have separate means, can you modify the function gen_data to allow the user to change the means for the two calls to rnorm? Have a try before revealing the solution below. Solution To Challenge Question gen_data &lt;- function(n = 20, m1 = 20, m2 = -20) { tibble(group = rep(c(&quot;A&quot;, &quot;B&quot;), c(n, n)), Y = c(rnorm(n, m1, 5), rnorm(n, m2, 5))) } # m1 is the mean of group A, # m2 is mean of group B and would look like: # The function would be called by: gen_data(20, 20, -20) # Giving 20 participants in each group, # The first group having a mean of 20, # The second group having a mean of -20. # Likewise, a call of: gen_data(4, 10, 5) # Would give two groups of 4, # The first having a mean of 10, # The second having a mean of 5. Portfolio Point - Two important facts about functions Here are two important things to understand about functions. Functions obey lexical scoping. What does this mean? It's like what they say about Las Vegas: what happens in the function, stays in the function. Any variables created inside of a function will be discarded after the function executes and will not be accessible to the outside calling process. So if you have a line, say a varibale my_var &lt;- 17 inside of a function, and try to print my_var from outside of the function, you will get an error: object 'my_var' not found. Although the function can 'read' variables from the environment that are not passed to it through an argument, it cannot change them. So you can only write a function to return a value, not change a value. Functions return the last value that was computed. You can compute many things inside of a function but only the last thing that was computed will be returned as part of the calling process. If you want to return my_var, which you computed earlier but not as the final computation, you can do so explicitly using return(my_var) at the end of the function (before the second curly bracket). 5.2.5.3 SKILL 5 COMPLETE - Now take a look at Skill 6! 5.2.6 Skill 6: Replicating Operations The last skill you will need for the upcoming lab is knowing how to repeat an action (or expression) multiple times. You saw this in Lab 4 so we will only briefly recap here. Here, we use the base function replicate(). For instance, say you wanted to calculate the mean from rnorm(100) ten times, you could write it like this: ## bad way rnorm(100) %&gt;% mean() rnorm(100) %&gt;% mean() rnorm(100) %&gt;% mean() rnorm(100) %&gt;% mean() rnorm(100) %&gt;% mean() rnorm(100) %&gt;% mean() rnorm(100) %&gt;% mean() rnorm(100) %&gt;% mean() rnorm(100) %&gt;% mean() But it's far easier and more readable to wrap the expression in replicate() function where the first argument is the number of times you want to repeat the expression stated as the second argument, i.e. replicate(times, expression). Here below we replicate the mean of 100 randomly generated numbers from the normal distribution, and we do this 10 times: replicate(10, rnorm(100) %&gt;% mean()) Also you'll probably want to store the results in a variable, for example, ten_samples: ten_samples &lt;- replicate(10, rnorm(100) %&gt;% mean()) ten_samples ## [1] 0.03240989 -0.08081112 -0.10043427 -0.27516544 0.02688148 ## [6] -0.01613623 -0.03865805 -0.08161202 -0.05686676 0.02170882 Each element (value) of the vector within ten_samples is the result of a single call to rnorm(100) %&gt;% mean(). 5.2.6.1 Quickfire Question Assuming that your hello() function from Skill 5 still exists, and it takes the argument name = Goodbye, what would happen in the console if you wrote, replicate(1000, hello(&quot;Goodbye&quot;))? Hello World would appear a thousand times hello Goodbye would appear a thousand times Hello Goodbye would appear a thousand times - Try it and see if it works! Solution To Quickfire Question # the function would be: hello &lt;- function(name = &quot;World!&quot;){ paste(&quot;Hello&quot;, name) } # and would be called by: replicate(1000, hello(&quot;Goodbye&quot;)) 5.2.6.2 SKILL 6 COMPLETE PreClass Activity Completed! To recap, we have shown you the following six skills: Skill 1: Generating random numbers with base::rnorm() Skill 2: Permuting values with base::sample() Skill 3: Creating a &quot;tibble&quot; (a type of data table) using tibble::tibble() Skill 4: Computing and extracting a difference in group means using dplyr::pull() and purrr::pluck() Skill 5: Creating your own custom functions using base::function() Skill 6: Repeating operations using base::replicate() You will need these skills in the coming lab to help you perform a real permutation test. Through these skills and the permutation test you will learn about null hypothesis significance testing. If you have any questions please post them on the Moodle forum or the slack forum rguppies.slack.com under the channel #level2_2018. Finally, don't forget to add any useful information to your Portfolio before you leave it too long and forget. Great work today. That is all for now. See you in the lab! 5.3 InClass Activity 5.3.1 Permutation Tests of Hypotheses A common statistical question when comparing two groups might be, &quot;Is there a real difference between the group means?&quot; From this we can establish two contrasting hypotheses: The null hypothesis which states that the group means are equivalent and is written as: \\(H_0: \\mu_1 = \\mu_2\\) where \\(\\mu_1\\) is the population mean of group 1 and \\(\\mu_2\\) is the population mean of group 2 Or the alternative hypothesis which states the groups means are not equivalent and is written as: \\(H_1: \\mu_1 \\ne \\mu_2\\). Using the techniques you read about in the PreClass and in previous labs, today you will learn how to test the null hypothesis of no difference between two independent groups. We will first do this using a permutation test before looking at other tests in later labs. A permutation test is a basic inferential procedure that involves a reshuffling of group labels or values to create new possible outcomes of the data you collected to see how your original mean difference compares to all possible outcomes. The test can in fact be applied in many situations, this is just one, and it provides a good starting place for understanding hypothesis testing. The steps for the exercise below, and really the logic of a permutation test for two independent groups, are: Calculate the real difference \\(D_{orig}\\) between the means of two groups (e.g. Mean of A minus Mean of B). Randomly shuffle the group labels (i.e. which group each participant belonged to - A or B) and re-calculate the difference, \\(D&#39;\\). Repeat step 2 \\(N_{r}\\) times, where \\(N_r\\) is a large number (typically greater than 1000), storing each \\(D_i&#39;\\) value to form a null hypothesis distribution. Locate the difference you observed in step 1 (the real difference) on the null hypothesis distribution of all possible differences. Decide whether the original difference is sufficiently extreme to reject the null hypothesis of no difference (\\(H_0\\)). This logic works because if the null hypothesis is true (there is no difference between the groups) then the labeling of the observations/participants into groups is arbitrary, and we can rearrange the labels in order to estimate the likelihood of our original difference under the \\(H_0\\). In other words if you know the original value of the difference between two groups (or the true difference) falls in the middle of your permuted distribution then there is no significant difference between the two groups. If however the original difference falls in the tail of the permuted distribution then there might be a significant difference depending on how far into the tail it falls. Let's get started! 5.3.2 Step 1: Load in Add-on Packages and Data 1.1. Open a new script and call the tidyverse into your library. ** FUTURE PHIL - NEED TO ADD RNGversion(&quot;3.5&quot;)** 1.2. Now type the statement set.seed(1011) at the top of your script after your library call and run it. (This 'seeds' the random number generator so that you will get the same results as everyone else. The number 1011 is a bit random but if everyone uses it then we all get the same outcome. Different seeds give different outcomes) 1.3. Download the data file from here and read the data in perm_data.csv into a variable called dat. 1.4. Let's give every participant a participant number as well by adding a new column to dat. Something like this would work: mutate(subj_id = row_number()) Helpful Hint Something to do with library() set.seed(1011) Something to do with read_csv() pipe %&gt;% on the mutate line shown Portfolio Point - Different uses of row_number You will see that in the example here to put a row number for each of the participants we do not have to state the number of participants we have. In the Preclass however we did. What is the difference? Well, in the Preclass we were making a tibble and trying to create a column in that tibble using row_numbers. If you want to do that you have to state the number of rows, e.g. 1:20. However, in this example in the lab today the tibble already exists, we are just adding to it. If that is the case then you can just mutate on a column of row numbers without stating the number of participants. In summary: When creating the tibble, state the number of participants in row_numbers(). If tibble already exists, just mutate on row_numbers(). No need for specific numbers. Have a look at the resulting tibble, dat. The column Y is your dependent variable (DV) The column group is your independent variable (IV). The columns subj_id is the participant number. 5.3.3 Step 2: Calculate the Original Mean Difference - \\(D_{orig}\\) We now need to write a pipeline of five functions that calculates the mean difference between the groups in dat, Group A minus Group B. Broken down into steps this would be: 2.1.1. Use a pipe of two dplyr one-table verbs (e.g. Lab 2) to create a tibble where each row contains the mean of one of the groups. Name the column storing the means as m. 2.1.2. Continue the pipe to spread() your data from long to wide format, based on the columns group and m. 2.1.3. Now add a pipe that creates a new column in this wide dataset called diff which is the value of group A's mean minus group B's mean. 2.1.4. Pull out the value in diff (the mean of group A minus the mean of group B) to finish the pipe. Helpful Hint dat %&gt;% group_by(?) %&gt;% summarise(m = ?) %&gt;% spread(group, m) %&gt;% mutate(diff = ? - ?) %&gt;% pull(?) 5.3.3.1 Quickfire Question Check that your value for d_orig is correct, without using the solution, by typing your d_orig value to two decimal places in the box. Include the sign, e.g. -1.23. The box will go green if you are correct. The above steps have created a pipeline of five functions to get one value. Nice! We now need to turn this into a function because we are going to be permuting the data set (specifically the grouping labels) and re-calculating the difference many, many times. 2.2. Wrap your pipeline in a function called calc_diff but swap dat for x. This function will take a single argument named x, where x is the tibble that you want to calculate group means from. As in the previous step, the function will return a single value which is the difference between the group means. The start will look like this below: calc_diff &lt;- function(x){ x %&gt;%..... } Helpful Hint calc_diff &lt;- function(x) { x %&gt;% group_by(group) %&gt;% the_rest_of_your_pipe... } 2.3. Now call your new function where x = dat as the argument and store the result in a new variable called d_orig. Make sure that your function returns the same value as you got above and that your function returns a single value rather than a tibble. You can test this: is.tibble(d_orig) should give you FALSE and is.numeric(d_orig) should give you TRUE. Helpful Hint d_orig &lt;- function_name(x = data_name) # or d_orig &lt;- function_name(data_name) # Then type the following in the Console and look at answer: is.tibble(d_orig) # True (is a tibble) or False (is not a tibble) is.numeric(d_orig) # True (is numeric) or False (is not numeric; it is a character or integer instead.) So we now have the original difference between the groups stored in d_orig. Next we need to create a distribution of all possible differences to see where our original difference lies in this distribution. But first we need to shuffle the group letters (A or B) in our dataset and find the difference...a few hundred times! 5.3.4 Step 3: Permute the Group Labels 3.1. Create a new function called permute() that takes as input a dataset x and returns the same dataset transformed such that the group labels (the values in the column group) are shuffled: started below for you. This will require using the sample() function within a mutate(). You have used mutate() twice already today and you saw how to sample() letters in the PreClass. permute &lt;- function(x){ x %&gt;%..... } Helpful Hint Might be easier to think of these steps in reverse. Start with a mutate() function that rewrites the column group every time you run it, e.g. dat %&gt;% mutate(variable = sample(variable)) Now put that into your permute() function making the necessary adjustments to the code so it starts x %&gt;%.... Again x should be in the function and not dat. 3.2. Try out your new permute() function by calling it on dat (i.e. x = dat) a few times. You should see the group labels in the group column changing randomly. The most common mistake is that people mutate a new column by mispelling group. You want to overwrite/change the information in the group column not make a new one, so be careful with the spelling. 5.3.4.1 Group Discussion Point Now would be an excellent time to spend five minutes as a group recapping what you are doing. You have the original difference between groups. You have a function that calculates and stores this difference. You have a function that reshuffles the labels of the group. Do you understand why? If not, go back to the principles of the permutation test at the start of the lab then read on... 5.3.5 Step 4: Create the Null-Hypothesis Distribution (NHD) for the Difference Now that we have the original difference and our two functions, one to shuffle group labels and one to calculate the difference between two groups, we need to actually create the distribution of possible differences and see where the original difference lies in it. 4.1.1. Write a a single pipeline that takes dat as the input, permutes the group labels with a call to your function permute(), and then calculates the difference in means between these new groups with a call to your function calc_diff(). 4.1.2. Run this line manually a few times and watch the resulting value change as the labels get permuted. Helpful Hint Think about verbalising your pipelines. In a single pipeline: I want to permute the data into two new groups. Then I want to calculate the difference between these two new groups. The functions you have created do these steps. You just have to put them in order and pipe the data through it. 4.2. Now take your pipeline of functions and repeat it 1000 times using the replicate() function. Store the output in a variable called nhd. nhd will contain 1000 values where each value is the mean difference of each of the 1000 random permutations of the data. (Warning: This will probably take a while to run, perhaps 10 seconds.) Helpful Hint # replace expression with the pipeline you created in 4.1 nhd &lt;- replicate(times, expression) You now have 1000 possible values of the difference between the permuted groups A and B - your permuted distribution. 4.3 Let's visualise this distribution through a frequency histogram of the values in nhd. This shows us the likelihood of various mean differences under \\(H_0\\). One thing to note however is that nhd is not a tibble and ggplot needs it to be a tibble. You need to convert it. You might start by do something like: ggplot(data = tibble(x = NULL), aes(x)) + NULL Helpful Hint Remember that ggplot works as: ggplot(data, aes(x)) + geom.... Here you need to convert nhd into a tibble and put that in as your data. Look at the example above and keep in mind that, in this case, the first NULL could be replaced with the data in nhd. 5.3.5.1 Group Discussion Point Looking at the histogram, visually locate where your original value would sit on this distribution. Would it be extreme, in the tail, or does it look rather common, in the middle? is in the middle so looks common is in the tail so looks extreme Before moving on stop to think about what this means - that the difference between the two original groups is rather uncommon in this permuted distribution, i.e. is in the tails! Again, if unsure, go back to the principles of NHST or discuss it with your tutor! 5.3.6 Step 5: Compare the Observed Mean Difference to the NHD If the null hypothesis is false, and there is a real difference between the groups, then the difference in means we observed for the original data (d_orig) should be somewhere in either tail of the null-hypothesis distribution we just estimated; it should be an &quot;extreme&quot; value. How can we test this beyond a visual inspection? First we have to decide on a false positive (Type I error) rate which is the rate at which we will falsely reject \\(H_0\\) when it is true. This rate is referred to by the Greek letter \\(\\alpha\\) (&quot;alpha&quot;). Let's just use the conventional level used in Psychology: \\(\\alpha = .05\\). So the question we must ask is, if the null hypothesis was true, what would be the probability of getting a difference in means as extreme as the one we observed in the original data? We will label this probability p. 5.3.6.1 Group Discussion Point Take a few moments as a group to see if you can figure out how you might compute p from the data before we show you how. We will then show you the process in the next few, final, steps. 5.1. Replace the NULLS in the code below to create a logical vector which states TRUE for all values of nhd greater than or equal to d_orig regardless of sign. Note: A logical vector is one that returns TRUE when the expression is true and FALSE when the expression is false. lvec &lt;- abs(NULL) &gt;= abs(NULL) Portfolio Point - abs and the case of one or two tails In the code above, the function abs() says to ignore the sign and use the absolute value. For instance, if d_orig = -7, then abs(d_orig) = 7. Why do we do this here? Can you think why you want to know how extreme your value is in this distribution regardless of whether the value is positive or negative? The answer relates to whether you are testing in one or two tails of your distribution; the positive side, the negative side, or both. You will have heard in your lectures of one or two-tailed tests. Most people would say to run two-tailed tests. This means looking at the negative and positive tails of the distribution to see if our original value is extreme, and the simplest way to do this is to ignore the sign of the values and treat both sides equally. If you wanted to only test one-tail, say that your value is extreme to the negative side of the tail, then you would not use the abs() and set the expression to make sure you only find values less than your original value. To test only on the positive side of the distribution, make sure you only get values higher than the original. But for now we will mostly look at two-tailed tests. 5.2. Replace the NULL in the code below to sum() the lvec vector to get the total number of values equal to or greater than our original difference, d_orig. Fortunately R is fine with summing TRUEs and FALSEs so you do not have to convert the data at all. n_exceeding_orig &lt;- NULL 5.3. Replace the NULL in the code below to calculate the probability of finding a value of d_orig in our nhd distribution by dividing n_exceeding_orig, the number of values greater than or equal to your original value, by the length() of your whole distribution nhd. Note: the length of nhd is the same as the number of replications we ran. Using code reduces the chance of human error p &lt;- NULL 5.4. Finally, complete the sentence below determining if the original value was extreme or not in regards to the distribution. Use inline coding, shown in Lab 1, to replace the XXXs. For example, when formatted without the space before the first r, r length(nhd) would present as 1000. &quot; The difference between Group A and Group B (M = XXX) was found to be have a probability of p = XXX. This means that the original mean difference was ...... and the null hypothesis is .....&quot; Job Done! Well done in completing this lab. Let's recap before finishing. We had two groups, A and B, that we had tested in an experiment. We calculated the mean difference between A and B and wanted to know if this was a significant difference. To test this we created a distribution of all possible differences between A and B using the premise of permutation tests and then found the probability of our original value in that permuted distribution. The more extreme the value in a distribution the more likely that the difference is significant. And that is exactly what we found; an \\(\\alpha &lt; .05\\). Next time we will look at using functions and inferential tests to perform this analysis but by understanding the above you now know how probability is determined. You should now be ready to complete the Homework Assignment for this lab. The assignment for this Lab is summative and should be submitted through the Moodle Level 2 Assignment Submission Page no later than 1 minute before your next lab. If you have any questions, please post them on the slack forum under the channel #level2_2018. Finally, don't forget to add any useful information to your Portfolio before you leave it too long and forget. 5.4 Assignment This is a summative assignment. Instructions as to how to access and submit your assignment will be made available during the course. 5.5 Solutions to Questions Below you will find the solutions to the questions for the Activities for this chapter. Only look at them after giving the questions a good try and speaking to the tutor about any issues. 5.5.1 InClass Activities 5.5.1.1 Step 1 Solution To Step 1 library(&quot;tidyverse&quot;) RNGversion(&quot;3.5&quot;) set.seed(1011) ## Note: ## Here&#39;s how we created the data for today&#39;s task: ## dat &lt;- tibble(group = rep(c(&quot;A&quot;, &quot;B&quot;), each = 50), ## Y = c(rnorm(50, 100, 15), ## rnorm(50, 110, 15))) ## ## write_csv(dat, &quot;perm_data.csv&quot;) ## You could create a new dataset yourself and try it again. dat &lt;- read_csv(&quot;perm_data.csv&quot;) %&gt;% mutate(subj_id = row_number()) Click the tab to reveal the solution to the task. 5.5.1.2 Step 2 Solution To Step 2.1.1 - dat pipeline dat %&gt;% group_by(group) %&gt;% summarise(m = mean(Y)) ## # A tibble: 2 x 2 ## group m ## &lt;chr&gt; &lt;dbl&gt; ## 1 A 101. ## 2 B 109. Click the tab to reveal the solution to the task. Solution To Step 2.1.2 - using spread() to change format dat %&gt;% group_by(group) %&gt;% summarise(m = mean(Y)) %&gt;% spread(group, m) ## # A tibble: 1 x 2 ## A B ## &lt;dbl&gt; &lt;dbl&gt; ## 1 101. 109. Click the tab to reveal the solution to the task. Solution To Step 2.1.3 - creating column of mean differences dat %&gt;% group_by(group) %&gt;% summarise(m = mean(Y)) %&gt;% spread(group, m) %&gt;% mutate(diff = A - B) ## # A tibble: 1 x 3 ## A B diff ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 101. 109. -7.39 Click the tab to reveal the solution to the task. Solution To Step 2.1.4 - completed pipeline using pull() dat %&gt;% group_by(group) %&gt;% summarise(m = mean(Y)) %&gt;% spread(group, m) %&gt;% mutate(diff = A - B) %&gt;% pull(diff) ## [1] -7.388401 Click the tab to reveal the solution to the task. Solution To Step 2.2 - calc_diff calc_diff &lt;- function(x) { x %&gt;% group_by(group) %&gt;% summarise(m = mean(Y)) %&gt;% spread(group, m) %&gt;% mutate(diff = A - B) %&gt;% pull(diff) } Click the tab to reveal the solution to the task. Solution To Step 2.3 - d_orig d_orig &lt;- calc_diff(dat) is.tibble(d_orig) ## Warning: `is.tibble()` is deprecated, use `is_tibble()`. ## This warning is displayed once per session. is.numeric(d_orig) ## [1] FALSE ## [1] TRUE Click the tab to reveal the solution to the task. 5.5.1.3 Step 3 Solution To Step 3 - Permute function permute &lt;- function(x) { x %&gt;% mutate(group = sample(group)) } permute(dat) ## # A tibble: 100 x 3 ## group Y subj_id ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 B 113. 1 ## 2 A 91.0 2 ## 3 A 89.2 3 ## 4 B 110. 4 ## 5 A 118. 5 ## 6 B 104. 6 ## 7 B 100. 7 ## 8 A 94.1 8 ## 9 B 94.8 9 ## 10 B 92.5 10 ## # ... with 90 more rows Click the tab to reveal the solution to the task. 5.5.1.4 Step 4 Solution To Step 4.1 dat %&gt;% permute() %&gt;% calc_diff() ## [1] -1.53662 Click the tab to reveal the solution to the task. Solution To Step 4.2 nhd &lt;- replicate(1000, dat %&gt;% permute() %&gt;% calc_diff()) Click the tab to reveal the solution to the task. Solution To Step 4.3 ggplot(tibble(x = nhd), aes(x)) + geom_histogram(binwidth = 1) Figure 5.1: CAPTION THIS FIGURE!! Click the tab to reveal the solution to the task. 5.5.1.5 Step 5 Solution To Step 5 ## 5.1 - create logical vector: TRUE if element of nhd is greater than/equal to d_orig lvec = abs(nhd) &gt;= abs(d_orig) ## 5.2 n_exceeding_orig &lt;- sum(lvec) ## 5.3 p &lt;- n_exceeding_orig / length(nhd) print(p) ## [1] 0.015 Click the tab to reveal the solution to the task. "],
["nhst-and-one-sample-t-tests.html", "Lab 6 NHST and One-Sample t-tests 6.1 Overview 6.2 PreClass Activity 1 6.3 PreClass Activity 2 (Additional) 6.4 InClass Activity 6.5 Assignment 6.6 Solutions to Questions", " Lab 6 NHST and One-Sample t-tests 6.1 Overview 6.2 PreClass Activity 1 6.3 PreClass Activity 2 (Additional) 6.4 InClass Activity 6.5 Assignment 6.6 Solutions to Questions "],
["within-subjects-t-test.html", "Lab 7 Within-Subjects t-test 7.1 Overview 7.2 PreClass Activity 1 7.3 InClass Activity 7.4 Assignment 7.5 Solutions to Questions", " Lab 7 Within-Subjects t-test 7.1 Overview 7.2 PreClass Activity 1 7.3 InClass Activity 7.4 Assignment 7.5 Solutions to Questions "],
["apes-alpha-power-effect-sizes-sample-size.html", "Lab 8 APES - Alpha, Power, Effect Sizes, Sample Size 8.1 Overview 8.2 PreClass Activity 1 8.3 InClass Activity 8.4 Assignment 8.5 Solutions to Questions", " Lab 8 APES - Alpha, Power, Effect Sizes, Sample Size 8.1 Overview 8.2 PreClass Activity 1 8.3 InClass Activity 8.4 Assignment 8.5 Solutions to Questions "],
["reflections.html", "Lab 9 Reflections 9.1 Overview 9.2 PreClass Activity 1 9.3 InClass Activity 9.4 Assignment 9.5 Solutions to Questions", " Lab 9 Reflections 9.1 Overview 9.2 PreClass Activity 1 9.3 InClass Activity 9.4 Assignment 9.5 Solutions to Questions "]
]
