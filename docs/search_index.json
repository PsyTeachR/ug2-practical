[
["index.html", "Level 2 Practical Overview", " Level 2 Practical 2020-05-11 Overview Materials for the University of Glasgow School of Psychology Year 2 Research Methods and Statistics Practical Course. This course covers data skills such as R Markdown, data wrangling with tidyverse, and data visualisation with ggplot2. It also introduces statistical concepts such as permutation tests, NHST, alpha, power, effect size, and sample size. Semester 2 focusses on correlations and the general linear model. "],
["starting-with-r-markdown.html", "Lab 1 Starting with R Markdown 1.1 Overview 1.2 PreClass Activiy 1.3 InClass Activiy 1.4 Assignment 1.5 Solutions to Questions 1.6 InClass Comparison", " Lab 1 Starting with R Markdown Welcome to the Level 2 Practical Lab Series 1.1 Overview Over the course of this year’s practical lab sessions we will help you learn a whole host of skills and methods based around being a Psychologist, starting right now! A key goal of any researcher is to carry out an experiment and to tell others about it. One of the main ways we as Psychologists do this is through publication of a journal article. There are numerous ways that people combine different software to create a journal article, but a more recent innovation in the field that we want you to know about is creating reports and articles through R Markdown. If you like, you can see an example from a research team in our school in this recent PLOS article. A link within the article methods section (this one - https://osf.io/eb9dq/) allows you to see the one file that creates the whole manuscript. Obviously you won’t be writing full journal articles just yet but you will use R Markdown throughout this lab series to do assignments and you could even also use it in other subjects to write reports, or to even make yourself a portfolio of hints, tips, and study aids. We will start by showing you some of skills in using R Markdown efficiently. In this lab you will learn: What is R Markdown? How to create an R Markdown file and knit it. How to add code and edit rules in your R Markdown file. How to format your text. 1.1.1 What is R Markdown? R Markdown (abbreviated as Rmd) is a great way to create dynamic documents through embedded chunks of code. These documents are self-contained and fully reproducible which makes it very easy to share. For more information about R Markdown feel free to have a look at their main webpage sometime: The R Markdown Webpage. The key advantage of R Markdown is that it allows you to write code into a document, along with regular text, and then knit it using the package knitr() to create your document as either a webpage (HTML), a PDF, or Word document (.docx). Explain This - We Knit what? Throughout the labs you will see little tabs that give more information, answers to quick questions, helpful hints, solutions to tasks, or suggestions for information you want to note down somewhere. You do not have to read them all and you will find they get less as the course progresses, but they might help you if you are stuck on something. Knit is what we say when we want to turn our R Markdown file into either a webpage, PDF, or a Word document. Often in the labs you will hear someone say, “Have you tried knitting it?” or “What happens when you knit it?”. This simply means what happens when you try turning your file into a pdf or webpage. 1.1.2 Advantages of using R Markdown The output is one file that includes figures, text and citations. No additional files are needed so it’s easy to keep all your work in one place. R code can be put directly into an R Markdown report so it is not necessary to keep your writing (e.g. a Word document) and your analysis (e.g. your R script) separate. Including the R code directly lets others see how you did your analysis - this is a good thing for science! It is both Reproducible and Open! You write your report in plain text, a non software-specific format that is easy to share, so it’s not necessary to learn any new coding such as HTML. 1.1.3 Creating an R Markdown (.Rmd) File In this lab you’re going to create your own R Markdown document. Knowing how to do this will: help you with submitting homework assignments, and help you to create your own reports using it. If at any point you are unsure about how to do something remember to think about where you can get help. There is an R Markdown Cheatsheet on the top menu under Help &gt;&gt; Cheatsheets or do what we do, Google it. For example, if I forget how to put words in bold, I could simply go to Google and type “rmarkdown bold” and no doubt get a lot of useful hints. Quickfire Questions We have put questions throughout to help you test your knowledge. When you type in or choose the correct answer, the dashed box will change color and become solid. From the following options, why are we creating an R Markdown document instead of simply using an R script? R Markdown can combine report writing and analysis R Scripts can’t run code Reproducible Science! Explain This Answer! So there’s more than one answer to this question! R Markdown can combine report writing and analysis, providing open access for others to examine data, and create more Reproducible Science. But what about the incorrect answer? R Scripts do in fact run R code as you may remember from Level 1 labs. The key difference is that R Scripts cannot really be used for documentation and creating reports - this is where R Markdown is used to ensure your code can be added to all the other information of your research and can be reproduced by others. 1.1.4 One last thing before beginning! Before working through the rest of this Lab you may want to watch the short videos on the Moodle pages and the Level 1 PsyTeachR Data Skills book reminding you about the skills we want you to learn through using R and RStudio, and the various file formats you will be working with. 1.2 PreClass Activiy Having read the Overview for this Lab, and the reason behind using R, we are now going to show you how to make a reproducible code. If you have a laptop, it is best to install R and RStudio on that for you to use. Dale Barr’s video gives a reminder of how to install R and Rstudio. If you don’t have it installed yet you can just read along today and try it out when you have access to a machine in the labs. 1.2.1 Let’s Begin Create a new R Markdown file (.Rmd) by opening Rstudio, and then on the top menu, selecting File &gt;&gt; New File &gt;&gt; R Markdown.... You should now see the following dialog box: Figure 1.1: Starting an R Markdown file Click Document on the left-hand panel and then give your document a Title. This is your file so call it what you want but make sure it is informative to you and your reader. Put your name or your student ID in the Author field as you are the author. For now we will focus on making an HTML output, so make sure that is selected as shown in Figure 1.1 then hit OK when you have done so. You should now have an .Rmd file open in Rstudio. The first thing you will see in your R Markdown file is a header section enclosed at the top and bottom by ---. Technically called the yaml header, this section lists the title, author, date and output format. The layout of the header is very precise and will look like that shown in Figure 1.2, which is currently set to output as HTML. Figure 1.2: An Rmd yaml header By default the file header includes the info shown in Figure 1.2 but there are many other options available. You can learn more about this in your spare time if you like through these links: http://rmarkdown.rstudio.com/html_document_format.html for .html options or http://rmarkdown.rstudio.com/pdf_document_format.html for .pdf options. BUT WAIT!! What if you spelt your name wrong? How would you change this? Explain This - I spelt my name wrong! The long way would be to close the file and start again. The shorter way would be to just correct the info in the header - just remember to keep between the quotes. E.g. “Si Cologe” instead of “Untitled” 1.2.2 Code Chunks Immediately below the header information you will see the default setup code chunk as shown in Figure 1.3. Most of the time you will not edit the information in this chunk and you will add information, text, and code, below it. Figure 1.3: The defualt setup code chunk In RMarkdown you can type any text you want directly in the document just as you would in a word document. However, if you want to include code you need to include it in one of these code chunks similar to Figure 1.3. Code chunks start with a line that contains three backwards apostrophes ` (these are called grave accents - often in the top-left of keyboards), and then a set of curly brackets with the letter r inside: ```{r}``` You will always need both of these parts to create a code chunk: The three back ticks ` are the part of the Rmd file that says this is code being inserted into my document. The {r} part says that you are specifically including R code. The default setup code chunk provides some basic options for your RMarkdown file for when it knits your work. As above, for now, it is best to leave this particular code chunk alone. Instead we will show you how to use RMarkdown by editing the code chunks that come after this default chunk. The next code chunk in your file will look a bit like this: ```{r cars} summary(cars) ``` Within the curly brackets, on the first line of the chunk, the word cars is included after the letter r. This is simply the name or the label for the code chunk and it really could have been called anything. For example, you could have called this code chunk cars1 and a later chunk cars2 to show it was the first and second chunk relating to cars. Whilst it is always advisable you name your code chunks, you do not need to name them. However, if you do put in names for the chunks do not use the same name twice as this will cause your script to crash when you knit it, e.g. Do not use data and data; instead maybe use personality_data and participant_info or whatever makes sense to what you are doing in the chunk. Explain This - You can crash whilst knitting? Remember knitting just means converting or rendering your file as a pdf, webpage, etc. Crashing means that you had an error in your code that stopped your knitting from working or finishing. You can usually find the problem line of code from the error message you’ll see. The second line in the above code chunk is the R code we have written: summary(cars). In this case, we are just asking for a summary() of the inbuilt dataset cars. R has a lot of inbuilt datasets for you to practice on; cars is one of these. The third line closes off the code chunk, again with the three backwards apostrophes. This means that whatever is contained between the first and third lines will be the code that is run. Quickfire Questions From the following options what was the name, or label, of the default setup code chunk (i.e. the first code chunk in an R Markdown file)? include r setup FALSE Explain This Answer If you look at the default setup code chunk you can see the code chunk has the name setup. include=FALSE is a rule which we will explain in a little bit. 1.2.3 Knitting Code Now would be a good time to try knitting your file to see what the code chunks do. You can do this using the Knit button at the top of the RStudio screen: Figure 1.4: The knit button. Clicking this will knit your file. When you click Knit it will ask you to save the file as an .Rmd file. Call the file L2Psych_Lab1_Preclass.Rmd and save it in a folder where you will keep all the information for this lab. When working in the Psychology labs or the University Library you need to save in a location or drive space that you have full access to and can save files to. The best one on campus is your M: drive. If using your own device then anywhere you can save the file should work. Helpful Hint - One folder for all your work It would be very beneficial to create a folder in your M: drive that will contain all your practical lab work for the rest of Level 2. Maybe something like Psychology Level 2 Lab Work and then have folders within that for each lab, e.g Lab1. The clearer the structure of these folders the easier it will be to find and use your files again! This is important as one thing we will keep telling you to do is Look Back at what you previously did. A good way to think about this is if you have an exam, it isn’t helpful to be told the location of your exam is ‘Glasgow Uni’ (i.e. a large folder of many locations). Instead you would need to be told the specific building (a folder within your larger folder), but more specifically the room number in the building where your exam is taking place (the folder which you are working from). After saving the file, a webpage should appear. The first thing to notice is that some lines in the code chunks have disappeared: the ```{r} and the closing ``` in your code chunk have gone. Whenever you knit an RMarkdown file these lines will disappear leaving only the code within. You’ll also notice that the output of the code is also now showing in your webpage. In the next section we will show you how to control showing the data or not through adding rules. Figure 1.5: The knitted summary output 1.2.4 Adding Code Chunk Rules and Options It can often be a good idea or even necessary to show the data or the outcome of a test in your report, for example if you were writing a report and wanted to include a table of results. But what if your code displayed a table that was 10,000 lines long? In that case we might want to not show the output and only show the code. You can do this by including a rule within the first line of your code chunk - your ```{r name, rule = option} line. You have already seen a rule before in the standard default chunk, the include rule, but there are a number of others. To hide the output but show the code we use the results = &quot;hide&quot; rule: Figure 1.6: The results Rule Add this rule into your example code chunk, as shown above, and knit the file again. What happens? Note that there is a comma separating the name of the chunk and the rule. You should now see the code only and not the data. Alternatively, we can Hide the code, but show the ouput by using the echo = FALSE rule: Figure 1.7: The echo Rule In your template Rmd file, the rule echo is set to FALSE meaning to show the figure and not the code. Change the rule in your code to echo and set it as TRUE, then knit the file again. What happens? Explain This - Why would I hide my code? Remember from Level 1 where we called in libraries to our environment. The “echo = FALSE” option is useful for commands like library() when you are just calling a package into the library but don’t necessarily want to display that in your final report or in your final HTML file. Another example might be if you wanted to make a plot but didn’t want to include the code, you just want to show the plot in your report. You might want to hide both the code AND the output by using the include rule: Figure 1.8: The include Rule Change the rule to your example code chunk, as shown above, to include = FALSE and then knit the file again. What happens? Note that here the code still runs. It just does not show you anything. Finally, you can use the eval rule which specifies whether or not you want the code chunk you have written to be evaluated when you knit the RMarkdown file. Evaluated means to run or carry out the code. Here, the eval = FALSE rule will stop the code from being evaluated. The code will be shown because there is no rule stopping it but there will be no output because it won’t get evaluated because of the eval rule being FALSE. Figure 1.9: The eval Rule This might be useful in cases where you want to show the code relating to how you programmed your stimuli for an experiment, but you don’t necessarily want it to run as part of the RMarkdown file. Quickfire Questions You’ve got a large dataset of thousands of participants’ personality and happiness scores that you want to analyse and present in RMarkdown. You want to show the code you are running in your analysis but not show the output as this would be too much to display. Note that you want the code to run. Type in the box (e.g. rule = set) how you would set the results rule to do this? You create a plot of happiness versus neuroticism scores but you want to hide the code and only show the output. How can you do this? echo = TRUE include = FALSE code = HIDE echo = FALSE Explain This - I don’t understand these answers The first answer should be results = “hide” as you want to show the code and run the code but not necessarily show the output of the code. In the second question, include = FALSE technically would hide the code, but this also hides the output! echo = FALSE allows you to still see your plot while hiding the code you want hidden. code = HIDE - if only it were that simple! The aim of these questions aren’t to help you memorise these codes (no one can do that!); they’re to help you gain a better understanding of how to apply these codes when you come across them in the future. True or False, writing echo = TRUE has the same effect on the output of a code chunk as if you had no echo rule at all: TRUE FALSE Explain This - Echo True or Not at all All of the code chunk rules have a default option. For example, echo, include, and eval are usually by default set to TRUE. As a result, if you don’t set any echo rule, i.e. you don’t specifically set echo = FALSE in your code chunk, then it is the same as setting echo = TRUE. So not specifying an option will give you the default setting for that option. True or False, there is no difference between setting results = &quot;hide&quot; and eval = FALSE as they both hide the output: TRUE FALSE Explain This - What’s the difference? With setting results = “hide”, the code is evaluated and results are produced but the output is hidden. With setting eval = FALSE, the code is not evaluated and therefore no results or output have been produced. If you need your output for a later part of the code then you would might use results = “hide”. If you don’t need the output and just want to show the code as an example then you might use eval = FALSE. 1.2.5 Adding Inline Code An alternative way to add code to a report is through what is called using inline code. With inline code you don’t use a code chunk. Instead the code appears inline with the text. Inline code can be inserted using a back-tick, then the letter r, followed by a space, the code you want to include, then another back-tick. For example, writing `r 2 + 2` would return the answer 4 when you knit the file instead of showing the code. Note that you do not do this inside a code chunk, you do this in line with your text, e.g.: “We ran `r 2+2` people”. Which when knitted becomes: “We ran 4 people”. So inline coding is really useful if you want to do calculations within your text or insert values into text, say from a dataframe, to make an informative sentence. Quickfire Questions You need Two One Three back tick(s) to insert code chunks Why is this inline code, `{r} 6 * 8` , not going to show the calculated answer when you knit the file? Try editing the code line in Rmarkdown and knitting it to get it to work. You need a space between each back tick and the code Inline code cannot complete calcuations Curly brackets are only needed for code chunks Explain This - Why are these answers correct? All code chunks start and end with three back-ticks. Inline coding does not use the curly brackets around the r. All you need is a back-tick, r, space, code, and a final back-tick. 1.2.6 Formatting the R Markdown File The last thing we want to show you in this preclass activity is how to format your text. When you’re not writing in code chunks you can format your document in lots of different ways just like you would in a Word document. The R Markdown cheatsheet provides lots of information about how to do this but we will show you a couple of things that you might want to try out. We can make some text bold by including two ** (two asterisks) at the start and end of the text we want to present in bold font. For example: “We ran **4 people**. Which when knitted becomes: ”We ran 4 people&quot;. Now write some text in your Rmd file and put it in bold. Knit the file to check it worked. You could also try using italics by putting a single * (asterisk) at the start and end of the word or sentence. Try this now Finally, you might want to add headings and sub-headings to your file. For example, maybe you are writing a Psychology journal article and want to put in a header for the Introduction, Methods, Results, or Discussion sections. We do this using the # (hashtag) symbol as shown in Figure 1.10. Figure 1.10: Inputting different Header levels using #s Now, type the four main sections found in a Psychology journal article in your RMarkdown file, typing each one in a separate line. These are mentioned above. Knit the file. What do these look like? Now add a different number of #’s before each heading, with a space between the heading and the hashtag (e.g. # Introduction) and knit the file again. What do you notice about the different number of hashtags? Quickfire Questions If * puts words into italics, and ** puts words into bold, type in the box what might you put before (and technically after) a word to put it into italics with bold? True or False: The more ’#’s you include, the smaller the header is: TRUE FALSE From the options, the most common order of headings found in a Psychology Journal are: Discussion, Introduction, Methods, Results Discussion, Results, Methods, Introduction Introduction, Methods, Results, Discussion Introduction, Results, Methods, Discussion Explain This - I don’t get these answers If * at the start and end of the word puts it in italics (e.g. italics) and ** puts it in bold (e.g. bold), then putting three *** at the start and end will put it in italics with bold (e.g. italics-bold). It is true that the more #’s you use, the smaller the heading is. Word and other document writers use different headings as well. Here, # gives the biggest heading, and it gets smaller and smaller with every extra #. Finally, in Psychology, the vast majority of journal articles are written in the format of: Introduction, Methods, Results, Discussion. In Semester 1 of Level 2 Psychology, you will write a report based on just the Introduction and the Methods. In Semester 2 you will write a report including all four sections. More on that to follow. Job Done - Activity Complete! Well done on working your way through this activity. Be sure to make notes for yourself, and to post any questions on the forums that you may have. See you in the lab! 1.3 InClass Activiy In order to complete this activity you should have worked through the Lab PreClass Activity. You may also need The R Markdown Cheatsheet. Everything you need to complete this inclass activity can be found in those documents. 1.3.1 R Markdown and The Experimental Design Portfolio In the preclass activity, we asked you to start playing about with R Markdown. Now, in the lab, we are going to continue learning about R Markdown as creating your own files from scratch is a great start to creating reproducible science! We will also start you off in creating your own Experimental Design Portfolio through R Markdown. The aim of this portfolio is to consolidate your learning in experimental design, allowing you to reflect back on how your learning has progressed. You should add to it whenever you think “Oh that is a good tip!” or “That is something I want to remember!”. Do this after a lab or a lecture, when you are collating all your notes together. Your portfolio is for you, unless you choose to share it of course, and will not be assessed or marked in anyway. It is your learning aid to help you develop your understanding of Research Methods in Psychology. By the end of these labs you should be able to use your portfolio for exam revision, particularly if you incorporate lecture material, so it’s worth maintaining a clear structure to your portfolio! In this first lab, across 9 tasks, we will help you to understand how to structure and format R Markdown files; you can then apply what you learn here to your portfolio in your own time. We suggest having your portfolio open in every lab so you can add to it as you go along. Let’s begin! Portfolio Point - Things you could include Throughout the semester you will see these Portfolio Points. They aren’t always necessary to complete the lab but sometimes they will be and sometimes they are just things you might consider putting in your Portfolio to remember. What you keep in your portfolio is up to you but here are some examples of the kind of things we would recommend you include: Key points about classic experiments their main goal, outcome, authors, year a top tip is to write a short summary after every paper you read, including the authors’ names to help you consolidate that information Design aspects of your Registered Report what decisions you made and why; how they compare to other studies. Glossary points for R code functions For codes you find more challenging to understand the function of For codes you might use more frequently in future activities Reflection Points on what you have learned in your labs each week. 1.3.2 The Ponzo Illusion and Age The activities in this lab will make use of an open dataset. Explain This - What is an open dataset? An open dataset is made available for everyone to see and is stored on the internet for other researchers to use. In the PreClass activity, you saw an example of this at the very start in the PLOS One article. Many journals now ask researchers to make their data available or to post it somewhere accessible like the Open Science Framework. Interestingly, the art of making your data available was standard in classic older articles. The data we are using today comes from 1967. Sometime between then and more recent times, data started being made unavailable - closed. We believe all data should be made available and will encourage you to do that over the coming years. The data we will use today is from a paper looking at the Ponzo illusion and Age: Leibowitz, H. W. &amp; Judisch, J. M. (1967). The Relation between Age and the Magnitude of the Ponzo Illusion. The American Journal of Psychology, 80(1), 105-109. It can be accessed on campus (University of Glasgow) through this link. The basics of the Ponzo illusion (Wikipedia page) is that two lines of the same size are viewed as being of different length based on surrounding information - like sleepers on a traintrack. See Figure 1 of Leibowitz and Judisch (1967) for an example (P106). The authors showed people two vertical lines surrounded by differing horizontal lines running at angles behind the main vertical lines. The authors varied the size of one of the vertical lines (left line) and asked the participants to judge which of the two vertical lines was bigger or longer; the left line (variable) or the right one (standard). The paper also tested how this illusion was influenced by age. For more info, see the paper. For the dependent variable, they measured what size the left line had to be to be considered the same size as the standard line on the right. The data we will be using can be seen on page 107, and includes: Which Group people were assigned to according to age, with each group being made of 10 people of the same sex The Sex of the Group The Mean Age of the Group The Mean Length of the left vertical line 1.3.3 Task 1: Setting up Your R Markdown Portfolio As above our overall goal is to make a reproducible “report” summarising the data in the Leibowitz and Judisch (1967) paper. As we go along, remember to refer back to the PreClass activity and cheatsheets to help you. Let’s begin! Create a new R Markdown document. Give it a title, e.g. My Psychology Research Methods Portfolio Enter your GUID or name as the author Set the output as HTML. Helpful Hint Throughout the labs you will see these Helpful Hints. Usually the solutions are nearby or at the end of the chapter to prevent temptation. In setting up this Rmd file, if you have followed these steps correctly, you will probably see a new R Markdown file with a header containing the title, author, date and output information as shown in the PreClass activity. If you don’t see the document header, then you’ve probably created an R Script instead. Refer back to the PreClass activity and try again. Look further down the list of File options on the top menu. If stuck, speak to a tutor. You can now remove the parts of the generic R Markdown code that we do not need; anything after the setup code chunk can be removed (see Figure 1.3). So anything after line 11 can be removed. Leave the first code chunk however - lines 8 to 10 - as these lines make R Markdown show code chunks unless otherwise specified. Portfolio Point - Code Chunk Reminders After the lab it might be handy to write a reminder somewhere in your portfolio about what a code chunk is. Writing it in your own notes somewhere accessible to you will mean you can find it more easily than searching through all the labs for the right information. This is something to keep in mind for any information you come across that you might need to recall later on! 1.3.4 Task 2: Give your Report a Heading We are going to start off your portfolio with creating a brief report on the Leibowitz and Judisch paper, so we should give it a heading. After the setup code chunk, give your report a heading, e.g. Lab 1 - The Magnitude of the Ponzo Illusion varies as a function of Age. Using hashtags, give this heading a Header 1 size. Helpful Hint Remember that the fewer the number of hashtags the larger the heading size. 1.3.5 Task 3: Creating a Code Chunk We are going to need the data soon so best to bring it in at the start. Set your working directory: Session &gt;&gt; Set Working Directory &gt;&gt; Choose Directory Download the data for this lab in a zip file by clicking this link. Unzip it and save it to the folder you are working in. Create a new code chunk in your R Markdown script, give this code chunk the name load_data. Copy and paste the code below into your code chunk. Spend a couple of minutes with a partner reminding yourself what the code does. The answer is in the hint below. Now, add or change the echo rule in your code chunk so that when you knit the file, the code will not be included in the final document. library(&quot;tidyverse&quot;) ponzo_data &lt;- read_csv(&quot;PonzoAgeData.csv&quot;) Knit the document now and see what the output looks like. It will ask you to save the file somewhere. Remember that on the Boyd Orr Lab PCs this is best done on your M: drive, given available space. Important: There is a good chance that, on the webpage that you have knitted, you will see either some warnings or messages. You can suppress these using the message and warning rules within the code chunks as well. Try this now - the PreClass Activities and the R-Markdown cheatsheet will help. Helpful Hint Hints: echo can equal TRUE or FALSE. Remember to separate rules in the code chunk with commas. E.g. {r, rule1 = FALSE, rule2 = TRUE} What does the code do? Loads the tidyverse packages and all associated packages e.g. dplyr, readr and ggplot2. Remember, you used these in Level 1. Loads in the data using the read_csv() function and stores it in ponzo_data. Important points to note: ponzo_data could have been called anything but best to call it something that makes it clear what it is. read_csv() is actually in the readr package and is available to you only after you have loaded in tidyverse through library(tidyverse). We will always tell you to use read_csv() to read in data from a csv file. remember &lt;- essentially means assign this to that. Assigning the ponzo data to the table ponzo_data can actually can be written the other way around - read_csv(“PonzoAgeData.csv”) -&gt; ponzo_data - but convention usually puts it the way we have in the code. Portfolio Point - Set Working Directory One of the most common issues we see with people using RStudio is that they forget to set their working directory to the folder containing the data file they are working on. This means that when you try to knit or run a code line it won’t work because RStudio doesn’t know where the data is. Remember to set your working directory at the start of each session, using Session &gt;&gt; Set Working Directory &gt;&gt; Choose Directory. Avoid using code to set your working directory as this will only work on your machine and not others and is therefore not fully reproducible without editing the script. 1.3.6 Task 4: Writing your Report Let’s start giving this brief report some information and structure as we would a full report. Underneath the code chunk you entered, put a new heading called Introduction and give it a Header 2 size. Next, do a little research with your group on the Ponzo Illusion and write a sentence or two describing its function; include a citation to support your research. There is a link to the wikipedia page on the illusion at the top of this lab which might help. Finally, copy the text in the box below into your report and finish the text by putting the names of two hypotheses behind the illusion below the sentence in an ordered list style; i.e. 1… 2…, etc. The two hypotheses are The Framing hypothesis and The Perspective hypothesis. &quot;There are two underlying hypotheses that may explain the Ponzo Illusion. These are: ...&quot; Quickfire Question Here are a couple of questions to try out in your group to remind you about using citations: When writing a report, how would you cite: Papers with five authors for the first time? Author 1, Author 2, Author 3, Author 4, &amp; Author 5 Author 1, Author 2, Author 3, Author 4, &amp; Author 5, Year Author 1 et al., Year Papers with five authors for the second time? Author 1, Author 2, Author 3, Author 4, &amp; Author 5 Author 1, Author 2, Author 3, Author 4, &amp; Author 5, Year Author 1 et al., Year Papers with seven authors for the first time? Author 1, Author 2, Author 3, Author 4, Author 5, Author 6, &amp; Author 7 Author 1, Author 2, Author 3, Author 4, Author 5, Author 6, &amp; Author 7, Year Author 1 et al., Year Two papers in one paretheses? Order chronologically according to year, separated by a semi-colon Order alphabetically according to first author surname, separated by a semi-colon Two papers of the same author? Order chronologically according to year, separated by a semi-colon Order chronologically according to year, separated by a comma Order alphabetically by adding a letter to each year For more information on how to format citations, you can look at this helpful APA formatting and style guide. 1.3.7 Task 5: Making Text Bold or Italicized Sometimes we want to add some emphasis to text. In your report, format the line There are two underlying hypotheses... in bold. Answering the below question might help you remember how. Quickfire Question Bold text and italicized text are created similarly, how do you create italicized text? * (before text) ** (before and after text) * (before and after text) ** (before text) It’s a good idea to knit the file at this point to make sure the bold font is working correctly. 1.3.8 Task 6: Adding Links to the Data in your Methods Good practice in a Report is to include information about where we got the data from. 1. Create a new heading below your list of the two hypotheses and call it Methods. Set it as Header 2 size. 2. Below Methods write a new heading called Data and set it as Header 3 size. 3. Underneath the Methods heading, copy and paste in the below sentence and turn the citation into an internet link to the paper. “The data in this report was obtained from within the original paper, (Lebowitz and Judisch, 2016).” Now knit your document again to make sure your formatting is working. Titles should be bigger than normal text and the list should be indented and have numbers at the start of each line. Helpful Hint You can get the web address by following the link to the paper shown towards the beginning of this lab activity. Include the https part. Use the R Markdown cheatsheet to see how to insert links. It has something to do with square brackets [] and circular brackets () next to each other. 1.3.9 Task 7: Adding an Image to your Methods For certain studies, you may want to add an image to the Methods section, either of the stimuli, of the materials, or of the procedure. If you look at the R Markdown cheatsheet you’ll see that adding an image is very similar to adding a link, the only difference is the ! beforehand. For now we will just add an image of the illusion taken from the internet to illustrate how to do this. Below the sentence you added for Task 6, add a new heading called Stimuli and set it as Header 3 size. Below the Stimuli heading, insert the image at the following web address: https://upload.wikimedia.org/wikipedia/en/8/89/Ponzo_Illusion.jpg Portfolio Point - A good methods section Remember that a good methods section will contain all the necessary information that would be required for another researcher to replicate your experiment exactly! It would normally be split into three or four sections including Ethics, Participants, Stimuli, and Procedure. 1.3.10 Task 8: Adding a Table to your Results Another benefit of R Markdown is that you can insert tables of results directly into your report without having to format them - though for aesthetics you will want to learn how to format tables eventually. But for now… Create a new heading below your methods sentence, called Results and format it as Header 2 size. Add a new code chunk and give it the name table, and include the code shown below. Add an echo rule so that the code IS NOT included in the final document but the ouput table is included. group_by(ponzo_data, Sex) %&gt;% summarise(NofGroups=n(), mean_length = mean(ComparisonLength)) Now, knit your document to see what you have produced. You should not see the above code, just the output table. 1.3.11 Task 9: Adding a Figure to your Results Nearly all research reports have a figure so we will want to add one as well. Underneath your table code chunk, add a new code chunk and give it the name plot. Add the below code to the chunk and set the include rule so that both the code and the plot are included in the final report. ggplot(ponzo_data, aes(x = Mean_Age, y = ComparisonLength, color = Sex)) + geom_point() Portfolio Point - autocompletes You can use RStudio’s autocomplete (the tab button) to see the different options for the different rules. For example, type include = and then hit the tab button on your keyboard. You should see the options of TRUE or FALSE. This works for a lot of functions you can’t quite remember how to spell as well. Again, knit your document to make sure it is working correctly. Below your table you should now have the ggplot code followed by the nice scatterplot. We will learn more about how to improve the visualisations as we progress but for now you have completed the bones of your first report! Compare your report to the one we have created to see if they match, which can be found at the end of this chapter using menu on the left (i.e. InClass Comparison) or click here to download the .Rmd file in a zip folder. Fix anything that is not formatted as in our template. Portfolio Point - The Power of R Markdown and the ggplot Package Here is a real-world scenario of why plotting in R Markdown can save a lot of effort. Say you carried out an experiment, made a figure of the results using an R Script, and wrote up the report using Microsoft Word. Then you realised you forgot to include 2 participants. To fix this, you would have to re-run the R script, make a new plot, save the plot, and then transfer that to your Word document. However, had you used R Markdown to begin with and both analysis and report were in the same place, then you can simply update the code within the document and a new figure will be created in the exact same place as the old one. Magic! The code above uses the ggplot2 package you used in Level 1. This is the main package we use for plots, figures, visualisations, or however you like to call them. It can be called into the library by itself, or is automatically called in when you call in the tidyverse package. Later this semester we will revist ggplot2 in more detail. For now, we are using it to make a scatterplot (geom_point) of Age (Mean_Age) and Comparison Length (ComparisonLength), and splitting the data for males and females. Group Discussion Point In your group, have a brief discussion about the figure to answer the following question. “Based on the distibrution of the data, shown in the above Figure, …” as age increases, people perceive a shorter vertical line to be of same length as the standard vertical line as age increases, people perceive a longer vertical line to be of same length as the standard vertical linge There is no relationship between Age and the Ponzo illusion This figure tells me nothing about the relationship between Age and the Ponzo illusion Helpful Hint What does each dot represent in the Figure, and what is the pattern of the dots? Job Done - Activity Complete! Great work! We have now created a rough layout of a report. The only section we are missing is the Discussion where you relate the information from previous research to what your study showed. Feel free to add one in your own time; read the short summary at the end of the actual paper to help get your thoughts together. We will talk more about the structuring of reports all throughout the year so you will have a great idea of how to write one by the end of Level 2. Well done on succesfully creating your own R Markdown file! You should now be ready to complete the Homework Assignment for this lab. The assignment for this Lab is SUMMATIVE and should be submitted through the Moodle Level 2 Assignment Submission Page no later than 1 minute before your next lab. If you have any questions, please post them on the forums. Finally, don’t forget to add any useful information to your Portfolio before you leave it too long and forget. 1.4 Assignment This is a summative assignment. Instructions as to how to access and submit your assignment will be made available during the course. 1.5 Solutions to Questions Below you will find the solutions to the questions for the Activities for this chapter. Only look at them after giving the questions a good try and speaking to the tutor about any issues. 1.5.1 InClass Activities 1.5.1.1 Task 2: Give your Report a Heading You should have used only one hashtag to give the biggest heading size. # Lab 1 - The magnitude of the Ponzo Illusion varies as a function of Age Return to Task 1.5.1.2 Task 3: Creating a Code Chunk The echo rule, warning rule and message rule should all be set to FALSE. As such, the start of the code chunk should look like: ```{r load_data, echo = FALSE, warning = FALSE, message = FALSE}``` Return to Task 1.5.1.3 Task 4: Writing your Report Task 4 is about setting a title to Header 2 style. This is done via two ## at the start of the line - before the word Introduction in this case but don’t forget the space. ## Introduction Worth noting that, in basic R Scripts, # at the start of the line would result in turning the line into a comment. Here, in R Markdown, # sets the header size much like a Word document header For the second part, create an ordered list by putting 1 followed by a . then a space before the first piece of information. A 2 then a . before the second, and so on. Note that lists will only work if there is a empty line above the list as well: 1. The Perspective Hypothesis 2. The Framing Hypothesis Return to Task 1.5.1.4 Task 5: Making Text Bold or Italicized To turn text to bold you need to put two ** at the start and end of the word or sentence you want as bold, e.g. **make me bold** Return to Task 1.5.1.5 Task 6: Adding Links to the Data in your Methods To set a header as Header 2 style use ## at the start of the line. To set a header as Header 3 style use ### at the start of the line. A link is created by putting the words you want to act as the link between [] and then the link immediately after in (). For example: [Lebowitz and Judisch (2016)](https://www.jstor.org/stable/1420548?seq=1#page_scan_tab_contents) 1.5.1.6 Task 7: Adding an Image to your Methods To set a header as Header 3 style use ### at the start of the line. An image is created by putting the words you want to act as the name of the image [] and then the link to the image immediately after in (). The key thing is to start with an exclamation mark !. For example: ![name](link) and therefore ![The Ponzo Illusion](https://upload.wikimedia.org/wikipedia/en/8/89/Ponzo_Illusion.jpg) Return to Task 1.5.1.7 Task 8: Adding a Table to your Results To set a header as Header 2 style use ## at the start of the line. The code chunk heading should read as follows: ```{r table, echo = FALSE}``` Return to Task 1.5.1.8 Task 9: Adding a Figure to your Results The code chunk heading should read as follows: ```{r plot, include = TRUE}``` Return to Task Chapter Complete! 1.6 InClass Comparison This section shows the output that would be expected if you were to follow the inclass activities correctly. Note: Headings in this comparison will appear one size smaller than if you were to knit the Rmd due to rendering. Do not worry if yours look a bit bigger, it is more that you have them as headers is the key part.Your output should match the output of knitting the .Rmd document found here Lab 1 - The Magnitude of the Ponzo Illusion varies as a function of Age Introduction The Ponzo Illusion is where… There are two underlying hypotheses that may explain the Ponzo Illusion. These are: The Framing hypothesis The Perspective hypothesis Methods Data The data in this report was obtained from within the original paper, Lebowitz and Judisch (2016) Stimuli PonzoIllusion Results ## # A tibble: 2 x 3 ## Sex NofGroups mean_length ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Female 15 3.57 ## 2 Male 26 3.61 ggplot(ponzo_data, aes(x = Mean_Age, y = ComparisonLength, color = Sex)) + geom_point() Figure 1.11: You won’t have a caption. We will cover that later! "],
["data-wrangling-a-key-skill.html", "Lab 2 Data-Wrangling: A Key Skill 2.1 Overview 2.2 PreClass Activity 2.3 InClass Activity 2.4 Assignment 2.5 Solutions to Questions 2.6 Additional Material", " Lab 2 Data-Wrangling: A Key Skill 2.1 Overview One of the key skills in any researcher’s toolbox is the ability to work with data. When you run an experiment you get lots of data in various files. For instance, it is not uncommon for an experimental software to create a new file for every participant you run and for each participant’s file to contain numerous columns and rows of data, only some of which are important. Being able to wrangle that data, manipulate it into different layouts, extract the parts you need, and summarise it, is one of the most important skills we will help you learn in the coming weeks. The next few labs are aimed at refreshing and consolidating your skills in working with data. This lab focuses on organizing data using the tidyverse package. Over the course of the activities, you will recap the main functions and how to use them, and we will use a number of different datasets to give you a wide range of exposure to what Psychology is about, and to reiterate that the same skills apply across different datasets. The skills don’t change, just the data! There are some questions to answer as you go along to test your skills: use the example code as a guide and the solutions are at the bottom. Finally, remember to be pro-active in your learning, work together as a community, and if you get stuck use the cheatsheets. The key cheatsheet for this activity is the Data Transformation with dplyr. In this lab you will recap from Level 1 on: Data-Wrangling with the Wickham six verbs. Additional useful tools such as count, gather and joins Piping and making efficient codes. Note: This preclass is a bit of a read but it is important that you have all this information in the one place so you can quickly refer back to it. Also, you did a very similar task in Level 1 so it is about recapping more than learning afresh. But take your time to try to understand the information and be sure to bring any questions to class with you. Portfolio Point - Getting Help Remember to open up your Portfolio that you created in Lab 1 so you can add useful information to it as you work through the tasks! Also summarising the information we give in this preclass, in your own words, is a great way to learn! You don’t have to read all of these but they might help from time to time to explain parts further. For instance, do you remember how to get help in R Studio? You can call the help function (e.g. ?gather) to view the reference page for each function. This example shows how to get help on the gather function, which we will use in later labs. 2.2 PreClass Activity Revisiting Tabular Data Remember from your previous experience that nearly all data in research methods is stored in two-dimensional tables, either called data-frames, tables or tibbles. There are other ways of storing data that you will discover in time but mainly we will be using tibbles (if you like more info, type vignette(&quot;tibble&quot;) in the console). A tibble is really just a table of data with columns and rows of information. But within that table you can get different types of data, i.e. double, integer, and character. Type of Data Description Double Numbers including decimals Integer Numbers without decimals Character Tends to contain letters or be words Note: Double is sometimes referred to as Numeric. They key point is though that these terms refer to values that have decimals. Quickfire Questions What type of data would these most likely be: Male = Character Double Integer 7.15 = Character Double Integer 137 = Character Double Integer Portfolio Point - Data types and levels of measurement There are lots of different types of data as well as different levels of measurements and it can get very confusing. It’s important to try to remember which is which because you can only do certain types of analyses on certain types of data and certain types of measurements. For instance, you can’t take the average of Characters just like you can’t take the average of Categorical data. Likewise, you can do any maths on Double (‘Numeric’) data and Integer data, just like you can on Interval and Ratio data. Integer data is funny in that sometimes it is Ordinal and sometimes it is Interval, sometimes you should take the median, sometimes you should take the mean. The main point is to always know what type of data you are using and to think about what you can and cannot do with them. 2.2.1 Revisiting the Wickham Six The main way we teach data-wrangling skills is by using the Wickham Six verbs. These are part of the tidyverse package which we introduced to you in Level 1, and more specifically from the dplyr package that is contained within the tidyverse. These six verbs are often referred to as the Wickham Six “one-table” dplyr verbs as the perform actions on a single table of data. We will look at some of the basics again here but try to look back at last year’s exercises to see how we used these verbs. The Wickham Six are: Function Description select() Include or exclude certain variables (columns) filter() Include or exclude certain observations (rows) mutate() Creates new variables (columns) arrange() Changes the order of observations (rows) group_by() Organises the observations (rows) into groups summarise() Derives summary variables for groups of observations (rows) 2.2.2 Learning to Wrangle: Is there a Chastity Belt on Perception Today we are going to be using data from this recent paper: Is there a Chastity Belt on Perception. You can read the full paper if you like, particularly if you are thinking about doing the action-perception registered report option, but we will summarise the paper for you. The paper asks, does your ability to perform an action influence your perception? For instance, does your ability to hit a tennis ball influence how fast you perceive the ball to be moving? Or to phrase another way, do expert tennis players perceive the ball moving slower than novice tennis players? This experiment does not use tennis players however, they used the Pong task: “a computerised game in which participants aim to block moving balls with various sizes of paddles”. A bit like a very classic retro arcade game. Participants tend to estimate the balls as moving faster when they have to block it with a smaller paddle as opposed to when they have a bigger paddle. You can read the paper to get more details if you wish but hopefully that gives enough of an idea to help you understand the wrangling we will do on the data. We have cleaned up the data a little to start with. Let’s begin! Download the data as a zip file from this link and save it somewhere you have access. In the lab, use your M: drive. Set your working directory to the same folder as the data. Session &gt;&gt; Set Working Directory &gt;&gt; Choose Directory Open a new script and copy and paste the two lines below. Here we are a) loading the tidyverse library into our session and then b) loading in the data through the read_csv() function and storing it in the tibble called pong_data. library(&quot;tidyverse&quot;) pong_data &lt;- read_csv(&quot;PongBlueRedBack 1-16 Codebook.csv&quot;) Help my data is not loading? The three most common mistakes we see are: Make sure you have spelt the data file name exactly as it is shown. Spaces and everything. Do not change the name of the csv file, fix your code instead. The reason being is that if you have a different name for your file than someone else then your code is not reproducible. Remember when uploading data we use read_csv which has an underscore, whereas the data file itself will have a dot in its name, filename.csv. Check that the datafile is actually in the folder you have set as your working directory. DO NOT install packages in the Boyd Orr labs; they are already there and just need called in through library(). However, If you are using a computer at home and you haven’t previously installed the tidyverse package on your own machine before, you will have to install it first, e.g. install.packages(“tidyverse”). Let’s have a look at the pong_data and see how it is organized. Type View(pong_data) or glimpse(pong_data) in your Console window. In the dataset you will see that each row (observation) represents one trial per participant and that there were 288 trials for each of the 16 participants. The columns (variables) we have in the dataset are as follows: Variable Type Description Participant integer participant number JudgedSpeed integer speed judgement (1=fast, 0=slow) PaddleLength integer paddle length (pixels) BallSpeed integer ball speed (2 pixels/4ms) TrialNumber integer trial number BackgroundColor character background display colour HitOrMiss integer hit ball=1, missed ball=0 BlockNumber integer block number (out of 12 blocks) We will use this data to master our skills of the Wickham Six verbs, taking each verb in turn and looking at it briefly. You should develop your skills by setting yourself new challenges based on the ones we set. There are 6 verbs to work through and then after that we will briefly recap on two other functions before finishing with a quick look at pipes. Try everything out and let us know anything you can’t quite get. Portfolio Point - The Wickham Six You will use the Wickham Six very frequently for wrangling your data so this would definitely be something you should be making notes about - not just the names, but how they work and any particular nuances that you spot. 2.2.3 The select() Function - to keep only specific columns The select() function lets us pick out the variables within a dataset that we want to work with. For example, say in pong_data we wanted to only keep the columns Participant, JudgedSpeed, PaddleLength, BallSpeed, TrialNumber, and HitOrMiss but we don’t need BackgroundColor or BlockNumber. We can do this in two ways: We can tell the function what variables we want to include select(pong_data, Participant, JudgedSpeed, PaddleLength, BallSpeed, TrialNumber, HitOrMiss) Or we can do it the opposite way by excluding columns through -ColumnName approach (i.e. minus the ColumnName) select(pong_data, -BackgroundColor, -BlockNumber) In this latter example, -BackgroundColor means ‘not BackgroundColor’, so here you are saying all columns except BackgroundColor and BlockNumber. The minus sign is the key part! ``` Task 1: Using the Select Function Either by inclusion or exclusion, select only the columns Participant, PaddleLength, TrialNumber, BackgroundColor and HitOrMiss from pong_data. select() can also be used to reorder columns. Use select() to keep only the columns Participant, JudgedSpeed, BallSpeed, TrialNumber, and HitOrMiss but have them display in alphabetical order, left to right. Helpful Hint Have you remembered to include the dataset pong_data? Pay attention to upper/lower case letters and spelling! Think about how you first entered the column names as they appeared. But what happens if you change the order that you enter the column names? 2.2.4 The arrange() Function - to sort and arrange columns The arrange() function sorts the rows in the tibble according to what column you tell it to sort by. # You can arrange by one column e.g. by `BallSpeed` arrange(pong_data, BallSpeed) # Or by multiple columns e.g. by `BallSpeed` (fastest first) and `BackgroundColor` arrange(pong_data, desc(BallSpeed), BackgroundColor) Explain this - where did desc come from? What does desc() do? desc() is how to sort by largest to smallest - i.e. descending order. Compare the output of the two lines above on the BallSpeed column. Does desc() also work for BackgroundColor? Task 2: Arranging Data Arrange the data by two variables: HitOrMiss (putting hits - 1 - first), and JudgedSpeed (fast judgement - 1 - first). 2.2.5 The filter() Function - to keep only parts of the data The filter() function lets us parse out a subset of the data, meaning we keep only parts of the data. # we might want only the red `BackgroundColor` filter(pong_data, BackgroundColor == &quot;red&quot;) # or higher speeds above 4 pixels filter(pong_data, BallSpeed &gt; 4) # or both! filter(pong_data, BackgroundColor == &quot;red&quot;, BallSpeed &gt; 4) # this can also be written as: filter(pong_data, BackgroundColor == &quot;red&quot; &amp; BallSpeed &gt; 4) # or we might want to see specific Participants: filter(pong_data, Participant %in% c(&quot;1&quot;, &quot;3&quot;, &quot;10&quot;, &quot;14&quot;, &quot;16&quot;)) # The c() creates a little container of items called a vector. # the `%in%` is called `group membership` and means keep each of these Participants # or excluding a specific Participant filter(pong_data, Participant != &quot;7&quot;) # you can read != as &#39;does not equal&#39;. So keep all Participants except 7. Task 3: Using the Filter Function Use filter() to extract all Participants that had a fast speed judgement, for speeds 2, 4, 5, and 7, but missed the ball. Store this remaining data in a variable called pong_fast_miss Helpful Hint There are three parts to this filter so it is best to think about them individually and then combine them. Filter all fast speed judgements (JudgedSpeed) Filter for the speeds 2, 4, 5 and 7 (BallSpeed) Filter for all Misses (HitOrMiss) You could do this in three filters where each one uses the output of the preceeding one, or remember that filter functions can take more than one arguement - see the example above. Also, because the JudgedSpeed and HitOrMiss are Integer you will need == instead of just =. Portfolio Point - And not Or The filter function is very useful but if used wrongly can give you very misleading findings. This is why it is very important to always check your data after you perform an action. Let’s say you are working in comparative psychology and have run a study looking at how cats, dogs and horses perceive emotion. Let’s say the data is all stored in the tibble animal_data and there is a column called animals that tells you what type of animal your participant was. Ok, so imagine you wanted all the data from just cats filter(animal_data, animals == “cat”) Exactly! But what if you wanted cats and dogs? filter(animal_data, animals == “cat”, animals == “dog”) Right? Wrong! This actually says “give me everything that is a cat and a dog”. But nothing is a cat and a dog, that would be weird - like a dat or a cog! What you actually want is everything that is either a cat or a dog, which is stated as: filter(animal_data, animals == “cat” | animals == “dog”) The vertical line | is the symbol for Or. TOP TIP: Always pay attention to what you want and most importantly to what your code produces. 2.2.6 The mutate() Function - for adding new columns The mutate() function lets us create a new variable in our dataset. For example, let’s add a new column to pong_data in which the background color is represented by numbers, where red will be represented as 1, and blue will be represented as 2. pong_data &lt;- mutate(pong_data, BackgroundColorNumeric = recode(BackgroundColor, &quot;red&quot; = 1, &quot;blue&quot; = 2)) The code here is is a bit complicated but: BackgroundColorNumeric is the name of your new column, BackgroundColor is the name of the old column and the one to take information from, and 1 and 2 are the new codings of red and blue respectively. The mutate() function is also handy for making some calculations on or across columns in your data. For example, say you realise you made a mistake in your experiment where your participant numbers should be 1 higher for every participant, i.e. Participant 1 should actually be numbered as Participant 2, etc. You would do something like: mutate(pong_data, Participant = Participant + 1) Note here that you are giving the new column the same name as the old column Participant. In the resulting table, the Participant column will have the new values which will differ from the values in the original pong_data table. While it may seem like you have overwritten these values, in reality you have created a copy of the table with altered values, but you have not lost anything: the original values are still there in pong_data. In general it is good practice not to overwrite pong_data with a new version of pong_data, but to store the altered table in a new tibble, e.g., pong_data2, like this: pong_data2 &lt;- mutate(pong_data, Participant = Participant + 1) Task 4: Mutating Variables You realise another mistake in that all your trial numbers are wrong. The first trial (trial number 1) was a practice so should be excluded. And your experiment actually started on trial 2. Tidy this up by: Creating a new variable and filtering out all trials with the number 1 (TrialNumber column) from pong_data, and then use the mutate() function to renumber all the remaining trial numbers, starting them at one again instead of two, and store the result as pong_data2. Helpful Hint Step 1: filter(TrialNumber does not equal 1) - remember to store this output in a variable? Step 2: mutate(TrialNumber = TrialNumber minus 1) 2.2.7 The group_by() Function - to group parts of data altogether The group_by() function groups the rows in a dataset according to a category you specify, e.g. grouping all Male data together and all Female data together. # In this data we could group trials by `BackgroundColor` group_by(pong_data2, BackgroundColor) # or by multiple criteria e.g. `HitOrMiss` and `BackgroundColor` group_by(pong_data2, HitOrMiss, BackgroundColor) Note that nothing actually appears to change in the data, unlike with the other functions, but a big operation has taken place. Look at the output in your console when you run group_by(pong_data2, BackgroundColor). At the top of the output notice that the 2nd line of the output tells us the grouping criteria and how many groups now exist: see the line Groups: BackgroundColor [2]: we grouped by BackgroundColor and there are [2] groups - one for red and one for blue. Task 5: Grouping Data Group the data by BlockNumber and by BackgroundColor, in that order, and then enter the number of groups (i.e. a number) you get as a result: Helpful Hint It is the same procedure as this but with different column names: group_by(pong_data2, HitOrMiss, BackgroundColor) The number of groups should be between the sum (i.e. multiplication) of the number of background colors (red and blue) and the number of blocks (12). group_by() is incredibly useful as, once the data is organised into groups, you can then apply other functions (filter, arrange, mutate…etc.) to the groups within your data that you are interested in, instead of to the entire dataset. For instance, a common second step after group_by might be to summarise the data… 2.2.8 The summarise() Function - to do some calculations on the data The summarise() function lets you calculate some descriptive statistics on your data. For example, say you want to know the number of hits there were for different paddle lengths, or number of hits there were when the background color was red or blue. # First we group the data accordingly, storing it in `pong_data2_group` pong_data2_group &lt;- group_by(pong_data2, BackgroundColor, PaddleLength) # And then we summarise it, storing the answer in `total_hits` pong_data2_hits &lt;- summarise(pong_data2_group, total_hits = sum(HitOrMiss)) # And then for fun we can filter just the red, small paddle hits pong_data2_hits_red_small &lt;- filter(pong_data2_hits, BackgroundColor == &quot;red&quot;, PaddleLength == 50) summarise() has a range of internal functions that make life really easy, e.g. mean, sum, max, min, etc. See the dplyr cheatsheets for more examples. Task 6: Summarising Data Run the first two lines of code in the box above to create pong_data_hits and then enter the number of hits made with the small paddle (50) and the red color background in this box: Note: The name of the column within pong_data2_hits is total_hits; this is what you called it in the above code. You could have called it anything you wanted but always try to use something sensible. Make sure to call your variables something you (and anyone looking at your code) will understand and recognize later (i.e. not variable1, variable2, variable3. etc.), and avoid spaces (use_underscores_never_spaces). Portfolio Point - the ungroup function After grouping data together using the group_by() function and then peforming a task on it, e.g. filter(), it can be very good practice to ungroup the data before performing another function. Forgetting to ungroup the dataset won’t always affect further processing, but can really mess up other things. Again just a good reminder to always check the data you are getting out of a function a) makes sense and b) is what you expect. Quickfire Questions Which of the Wickham Six would I use to sort columns from smallest to largest: select filter mutate arrange group_by summarise Which of the Wickham Six would I use to calculate the mean of a column: select filter mutate arrange group_by summarise Which of the Wickham Six would I use to remove certain observations - e.g. remove all males: select filter mutate arrange group_by summarise 2.2.9 Two Other Useful Functions The Wickham Six verbs let you to do a lot of things with data, however there are thousands of other functions at your disposal. If you want to do something with your data that you are not sure how to do using these functions, do a Google search for an alternative function - chances are someone else has had the same problem and has a help guide. For example, two other functions to note are the bind_rows() function and the count() function. The bind_rows() function is useful if you want to combine two tibbles together into one larger tibble that have the same column structure. For example: # a tibble of ball speeds 1 and 2 slow_ball&lt;- filter(pong_data2, BallSpeed &lt; 3) # a tibble of ball speeds 6 and 7 fast_ball &lt;- filter(pong_data2, BallSpeed &gt;= 6) # a combined tibble of extreme ball speeds extreme_balls &lt;- bind_rows(slow_ball, fast_ball) Finally, the count() function is a shortcut that can sometimes be used to count up the number of rows you have for groups in your data, without having to use the group_by() and summarise() functions. For example, in Task 6 we combined group_by() and summarise() to calculate how many hits there were based on background color and paddle length. Alternatively we could have done: count(pong_data2, BackgroundColor, PaddleLength, HitOrMiss) The results are the same, just that in the count() version we get all the information, including misses, because we are just counting rows. In the summarise() method we only got hits because that was the effect of what we summed. So two different methods give similar answers - coding can be individualised and get the same result! 2.2.10 Last but not least - Pipes (%&gt;%) to make your code efficient By now you’ll have noticed thattidyverse functions generally take the following grammatical structure (called syntax): function_name(dataset, arg1, arg2,..., argN) where the dataset is the entire tibble of data you are using, and each argument (arg) is some operation on a particular column or variable, or the column name you want to work with. For example: # function_name(dataset, arg1, arg2, ....) filter(pong_data2, PaddleLength == &quot;50&quot;, BallSpeed &gt; 4) group_by(pong_data2, BallSpeed, Participant) In the first example, we are filtering the whole pong_data2 dataset by a particular paddle length, then by particular speeds. In the second, we are grouping by BallSpeed and then by Participant. Note that the order of arguments is specific as it performs argument1 then argument2, etc. Changing the order of arguments may give a different output. So the order you work in is important, and this is called your pipeline. For example, here is one we used above to find how many hits there were with the small paddle length and the red background. # First we group the data accordingly, storing it in `pong_data2_group` pong_data2_group &lt;- group_by(pong_data, BackgroundColor, PaddleLength) # And then we summarise it, storing the answer in `total_hits` pong_data2_hits &lt;- summarise(pong_data2_group, total_hits = sum(HitOrMiss)) # And filter just the red, small paddle hits pong_data2_hits_red_small &lt;- filter(pong_data2_hits, BackgroundColor == &quot;red&quot;, PaddleLength == 50) Pipelines allow us to quickly, accurately, and reproducibly, perform an action that would take much longer manually. However, we can make our code even more efficient, using less code, by stringing our sequence of functions together using ‘pipes’, written as %&gt;%. This would look like: # Same pipeline using pipes pong_data_hits_red_small &lt;- pong_data2 %&gt;% group_by(BackgroundColor, PaddleLength) %&gt;% summarise(total_hits = sum(HitOrMiss)) %&gt;% filter(BackgroundColor == &quot;red&quot;, PaddleLength == 50) Both these chunks show exactly the same procedure, but adding pipes can make code easier to read and follow once you understand piping. Code without a pipe would look like function_name(dataset, arg1, arg2,...,argN) but a pipe version would look like dataset %&gt;% function_name(arg1, arg2,...,argN) The premise is that you can pipe (%&gt;%) between functions when the input of a function is the output of the previous function. Alternatively, you can use a pipe to put the data into the first function, as shown directly above. You can think of the pipe (%&gt;%) as saying ‘and then’ or ‘goes in to’. E.g. the data goes into this function and then this function and then this function. We will expand on this in the lab where you can ask more questions, but try comparing the two chunks of code above and see if you can match them up. One last point on pipes is that they can be written in a single line of code but it’s much easier to see what the pipe is doing if each function takes its own line. Every time you add a function to the pipeline, remember to add a %&gt;% first and Note that when using separate lines for each function, the %&gt;% must appear at the end of the line and not the start of the next line. Compare the two examples below. The first won’t work but the second will because the second puts the pipes at the end of the line where they need to be! # Piped version that wont work data_arrange &lt;- pong_data2 %&gt;% filter(PaddleLength == &quot;50&quot;) %&gt;% arrange(BallSpeed) # Piped version that will work data_arrange &lt;- pong_data2 %&gt;% filter(PaddleLength == &quot;50&quot;) %&gt;% arrange(BallSpeed) Portfolio Point - Pipes are good for the Environment Where piping becomes most useful is when we string a series of functions together, rather than using them as separate steps and having to save the data each time under a new variable name and getting ourselves all confused. In the non-piped version we have to create a new variable each time, for example, data, data_filtered, data_arranged, data_grouped, data_summarised just to get to the final one we actually want, which was data_summarised. This creates a lot of variables and tibbles in our environment and can make everything unclear and eventually slow down our computer. The piped version however uses one variable name, saving space in the environment, and is clear and easy to read. With pipes, we skip unnecessary steps and avoid cluttering our environment. Quickfire Questions What does this line of code say? data %&gt;% filter() %&gt;% group_by() %&gt;% summarise(): take the data and group it and then filter it and then summarise it take the data and filter it and then group it and then summarise it take the data and summarise it and then filter it and then group it take the data and group it and then summarise it and then filter it Job Done - Activity Complete! We have now recapped a number of functions and verbs that you will need as the semester goes on. You will use them in the lab next week so be sure to go over these and try them out to make yourself more comfortable with them. Remember to also start looking back at your first year labs and remembering some of the work you did there. If you have any questions please post them on the Moodle forum or the slack forum rguppies.slack.com under the channel #level2_2018. Finally, don’t forget to add any useful information to your Portfolio before you leave it too long and forget. Happy Wrangling! 2.3 InClass Activity Data Wrangling In the PreClass activity we looked at using one-table Wickham verbs to filter, arrange, group_by, select, mutate and summarise. Now we will focus on working with data across two or more tables using verbs you will have come across in Level 1. The two main verbs we will add to the Wickham six today are gather() and inner_join(). gather() allows us to transform a table from wide format to long format (more on this below). inner_join() allows us to combine two tables together based on common columns. Portfolio Point - Still not sure what a function is and how to remember them? A function is a tool that takes an input, performs some action, and gives an output. They are nothing more than that. If you think about it, your toaster is a function: it takes bread as an input; it perfoms the action of heating it up (nicely sometimes; on both sides would be a luxury); and it gives an output, the toast. A good thing about the Wickham six functions is that they are nicely named as verbs to describe what they do - mutate() mutates (adds on a column); arrange() arranges columns, summarise() summarises, etc. In terms of remembering all the functions, the truth is you don’t have to know them all. However, through practice and repetition, you will quickly learn to remember which ones are which and what package they come from. Sort of like where to find your spoons in your kitchen - you don’t look in the fridge, and then the washing machine, and then the drawer. Nope, you learnt, by repetition, to look in the drawer first time. It’s the same with functions. Keep in mind that research methods is like a language in that the more you use it and work with it the more it makes sense. A Note on Tidy Data In the style of programming we teach, the most efficient format/layout of data is what is known as Tidy Data, and any data in this format is easily processed through the tidyverse package. You can read more about this type of data layout in this paper: Tidy Data (Wickham, 2014). It is a surprisingly good read. However, the data you work with will not always be formatted in the most efficient way possible. If that happens then our first step is to put it into Tidy Data format. There are two fundamental principles defining Tidy Data: Each variable must have its own column. Each observation must have its own row. Tidy Data (Wickham, 2014) adds the following principle: Each type of observation unit forms a table. And Grolemund and Wickham (2017) restate this third principle as: Each value must have its own cell (i.e. no grouping two variables together, e.g. time/date in one cell). Where a cell is where any specific row and column meet; a single data point in a tibble is a cell for example. The Grolemund and Wickham (2017) book is a very useful read and it is free, but browsing the chapter on Tidy Data will help you visualise how you want to arrange data. Try to keep the principles in mind whilst doing so. Explain this - If it isn’t Tidy then what is it? We use Tidy Data because it is really efficient and works well with the tidyverse. However, people used to use data structured in long format or wide format. Long format is where each row is a single observation, typically a single trial in an experiment or a response to a single item on a questionnaire. When you have multiple trials per participant, you will have multiple rows for the same participant. To identify participants, you would need a variable with some kind of participant id, which can be as simple as a distinct integer value for each participant. In addition to the participant identifier, you would have any measurements taken during each observation (e.g., response time) and what experimental condition the observation was taken under. In wide format data, each row corresponds to a single participant, with multiple observations for that participant spread across columns. So for instance, with survey data, you would have a separate column for each survey question. Tidy is a mix of both of these approachs and most functions in the tidyverse assume the tidy format, so typically the first thing you need to do when you get data, particularly wide-format data, is to reshape it through wrangling. Which is why we teach these really important skills. Today’s Lab - Analysing the Autism Specturm Quotient (AQ) To continue building your data wrangling skills we will recap on skills from Level 1 by tidying up data from the Autism Spectrum Quotient (AQ) questionnaire. In that Level 1 lab we used the AQ10; a non-diagnostic short form of the AQ with only 10 questions per participant. It is a discrete scale and the higher a participant scores on the AQ10 the more autistic-like traits they are said to display. Anyone scoring 7 or above is recommended for further diagnosis. You can see an example of the AQ10 through this link: AQ10 Example. Remember you can revisit your Level 1 notes at any time but we will recap here for you. We have 66 participants and your goal in this lab is to find an AQ score for each of them through your data-wrangling skills. We have four data files to work with: responses.csv containing the AQ survey responses to each of the 10 questions for our 66 participants qformats.csv containing information on how a question should be coded - i.e. forward or reverse coding scoring.csv containing information on how many points a specific response should get; depending on whether it is forward or reverse coded pinfo.csv containing participant information such as Age, Sex and importantly ID number. Click here to download the files as a zip file. Now unzip the files into a folder on your M: drive. We will use zip folders a lot so if this is something you struggle with please ask. Portfolio Point - Open Data is best in csv format csv stands for ‘comma separated values’, and is a very basic format for storing data in a plain text file. It really just stores numbers and text separated by commas and nothing else. The great thing about being this basic is that it can be read by many different machines and is non-proprietary, i.e., you don’t need to purchase commercial software to open it. Now set your working directory to the folder where you saved the .csv files. Do this through the dropdown menus at the top toolbar: Session &gt;&gt; Set Working Directory &gt;&gt; Choose Directory and then find your folder with your .csv files. This should be somewhere on your M: drive, but never in a folder titled R. Today we will work in an RScript instead of .Rmd but if you want to turn the lab into an RMarkdown report or to add elements to your Portfolio then please feel free. Group Discussion Point Now would be a good time to make sure that you are all using RStudio effectively and know what each window does. TRUE or FALSE, the Console is best for practice and the Script Window is for saving: TRUE FALSE TRUE or FALSE, the Environment holds all the data and objects you have loaded in and created: TRUE FALSE TRUE or FALSE, clicking the name of a table in the Environment window will open it in the Script window: TRUE FALSE Explain this - I don’t get these answers The answer to all of these are True. The Script window is where you should write code and comments that you are going to save and send to people. The Console is where you should practice stuff - nothing is saved here; it is like a sandbox that just gets wiped away. Any data you load in or create is held in the Environment (or Global Environment) window with the variable name that you gave it. By clicking the name of the table in the Environment window it will open up in the Script window and you can look at it to make sure it is what you expect. This only works for tables but not for other types of data. You will learn the difference as we go along! 2.3.1 Task 1: Open a Script Start a new Rscript and save it to your M: drive in the same folder as your .csv files, calling the Rscript something informative like Lab2_AQ_DataWrangling.R. Make sure your environment is completely empty so we don’t mix up one analysis with the other. You can run the following code line in the console to clear the environment or by clicking the little brush on your environment window. rm(list = ls()) Portfolio point - comments on scripts and running lines Remember that when using a script you can write notes to yourself to remind you what a line of code does. Just put a hashtag at the start of the line and R will ignore this line. This is where you have to be clear on using a Script versus an RMarkdown file. In a Script, # means the line is ignored, in Markdown # sets the line as a header!. To run any line on a script, the simplest way is to click anywhere on that line and either press Run on the top of the script window or press CTRL+Enter on the keyboard (or mac equivalent). 2.3.2 Task 2: Bring in Your Library Add a line to your code that brings the tidyverse package into your working environment and run it. Helpful Hint - on Library vs Install Combine the function library() and the package tidyverse and remember that the solutions are at the end of the chapter. On our lab machines in Psychology all the necessary packages will already be on the machines, they just need called into the library. If however you are using your own machine you will have to install the packages first. Do not install packages on the Psychology machines! Why? They are already installed and can cause the package to stop working if a student tries to install the same package on our machines. They are already installed and it is a bit like using apps on your phone. Install is putting the app onto your phone, library is just opening the app. If you’ve already downloaded the app (package) then you just need to open it (library()) to use it! 2.3.3 Task 3: Load in the Data Now we have to load in the .csv datafiles using the read_csv() function and save them as variables in our environment. For example, to load in the responses we would type: responses &lt;- read_csv(&quot;responses.csv&quot;) Add the following lines of code to your script and complete them to load in all four .csv datafiles. Use the above code as an example and name each variable the same as its original filename (minus the .csv part), again as above, e.g. responses.csv gets saved as responses. Remember to run the lines so that the data is loaded in and stored in your environment. responses &lt;- read_csv() # survey responses qformats &lt;- # question formats scoring &lt;- # scoring info pinfo &lt;- # participant information Portfolio Point - Haven’t I read_csv before As you work with data and functions you will find there are functions with similar names but that give different results. One of these is the read function for csv. Make sure to always use read_csv() as your function to load in csv files. Nothing else. It is part of the readr package automatically brought in with tidyverse. 2.3.4 Task 4: Review Your Data. Group Discussion Point Now that we have the data loaded in it is always best to have a look at the data to get an idea of its layout. We showed you one way before, by clicking on the name in the environment, but you can also use the glimpse() or View() functions in your Console window. Put the name of the data between the brackets to see how it is arranged. Don’t add these to your script though - they are just one-offs for testing. As a small group, have a look at the data in responses to see if you think it is Tidy or not and answer the following question: The data in responses is in Tidy Long Wide format Explain This - I don’t get why? The reponses tibble is far from being tidy; each row represents multiple observations from the same participant, i.e. each row shows responses to multiple questions - wide format. Remember we want the data in tidy format as described above. Eh, what’s a tibble? A tibble is simply a dataframe - or a table of data with columns and rows - that is really handy for working with when using the tidyverse package. When we say tibble, you can think of a dataframe with rows and columns of information and numbers stored in them - like responses, it is a tibble. For more info, see here: Tibbles 2.3.5 Task 5: Gathering Data. We now have all the data we need loaded in, but in order to make it easier for us to get the AQ score for each participant, we need to change the layout of the responses tibble using the gather() function. Copy the below code line to your script and run it. rlong &lt;- gather(responses, Question, Response, Q1:Q10) The first argument given to the gather() function is the dataset which holds the data we want to wrangle, responses. The second and third arguments are the names we want to give the columns we are creating; the first will store the question numbers, Question the second will store the responses, Response. Note that these names could have been anything but by using these names the code makes more sense. Finally, the fourth argument is the names of specific columns in the original tibble that we want to gather together. In case you are wondering, if we wanted to go back the way and ungather the data we just gathered, we would use the spread() function: e.g. rwide &lt;- spread(rlong, Questions, Response). But we do not want to do that here so let’s not add this to the code. Quickfire Questions Let’s see if you understand gather(). Say I wanted to gather the first three columns of responses (Q1, Q2, Q3), put the question numbers in a column called Jam, the responses in a column called Strawberry, and store everything in a tibble called sandwich. Fill in the box with what you would write: Explain this - I dont get the right answer! sandwich &lt;- gather(responses, Jam, Strawberry, Q1:Q3) gather wants the data first, then the name of the new column to store the gathered column names, then the name of the new column to store the data, and then finally which columns to gather. 2.3.6 Task 6: Combining Data. So now our responses data is in tidy format, we are closer to getting an AQ score for each person. However, we still need to add some information together to: Show if the question is reverse or forward scored - found in qformats Show the number of points to give a specific response - found in scoring. This is a typical analysis situation where different information is in different tables and you need to join them altogether. Both these pieces of information are contained in qformats and scoring respectively, but we want to join it to responses to create one informative tidy table with all the info. We can do this through the function inner_join(); a function to combine information in two tibbles using a column common to both tibbles. Copy the below line into your code and run it. # combine rows in the tibble rlong with rows in the tibble `qformats`, based on the common column &quot;Question&quot; rlong2 &lt;- inner_join(rlong, qformats, &quot;Question&quot;) Now have a look in rlong2. We have matched each question with its scoring format, forward or reverse. Portfolio Point - Reverse and Forward A lot of questionnaires have some questions that are Forward scored and some questions that are Reverse scored. What does this mean? Imagine a situation where your options in replying to a question are: 1 - extremely agree, 2 - agree, 3 - neutral, 4 - disagree, 5 - extremely disagree. In a forward-scoring question you would get 1 point for extremely agree, 2 for agree, 3 for neutral, etc. In a reverse scoring question you would get 5 for extremely agree, 4 for agree, 3 for neutral, etc. The reasoning behind this shift is that sometimes agreeing or disagreeing might be more favourable depending on how the question is worded. Secondly, sometimes these questions are used just to catch people out - imagine if you had two similar questions where one has the reverse meaning of the other. In this scenario, people should respond opposites. If they respond the same then they might not be paying attention. Now we need to combine the information in our table, rlong2, with the scoring table so we know how many points to attribute each question based on the answer the participant gave, and whether the question was forward or reverse coded. Again, we use the inner_join() function, but this time the common columns found in rlong2 and scoring are QFormat and Response. To combine by two columns you just write them in sequence as shown below. Note: when there is more than one common column between two tibbles you are joining, it is best to combine by all the columns to avoid repeat columns names in the new tibble. Copy the below line into your code and run it. # combine rows in rlong2 and scoring based on QFormat and Response rscores &lt;- inner_join(rlong2, scoring, c(&quot;QFormat&quot;, &quot;Response&quot;)) 2.3.7 Task 7: Calculating the AQ Scores. We have now created rscores which has information on how each participant responded to each question and how each question should be coded and scored, all within the one tibble. All we need now is to sum the scores for each participant to get their AQ score. Based on your Preclass knowledge, copy the below line into your code and complete it to obtain individual aq_scores for each participant. Save your script and run it all again from the start to make sure it works! aq_scores &lt;- rscores %&gt;% group_by() %&gt;% # how will you group individual participants? summarise(AQ = sum()) # which column will you sum to obtain AQ scores? Helpful Hint Each participant could be grouped by their Id. If we summed up the value for each Score we might get a full AQ Score for each particpipant. Portfolio Points - Hang on isn’t that a Pipe? We saw Pipes in Level 1 and then again in the Preclass for this lab. Pipes are your friend. Think of them as saying ‘and then’ or ‘goes into’. So in the example above we take rscores and then group it by something and then summarise it into AQ scores based on… In most cases, the pipe serves the purpose of putting the input into the function or taking the output of one function and treating it as the input of another function. In the example above the first pipe takes rscores as the input for group_by, and the second pipe takes the output of group_by and puts it as the input to summarise. See how you can almost read it as a chain of actions or steps. Quickfire Questions The whole purpose of this lab was to calculate AQ scores for individual participants. As a small group, try to answer the following questions. Try to do it using code where possible to help you based on your knowledge from the preclass and inclass activity. Remember the cheatsheets as well. Look for the dplyr one! From the options, choose the correct citation for the AQ 10 question questionnaire: Allison, Auyeung, and Baron-Cohen, (2011) Allison, Auyeung, and Baron-Cohen, (2012) Allison and Baron-Cohen, (2012) Auyeung, Allison, and Baron-Cohen, (2012) Complete the sentence, the higher the AQ score… the less autistic-like traits displayed has no relation to autistic-like traits the more autistic-like traits displayed Type in the AQ score (just the number) of Participant ID No. 87: Type how many participants had an AQ score of 3 (again just the number): The cut-off for the AQ10 is usually said to be around 6 meaning that anyone with a score of more than 6 should be referred for diagnostic assessment. Type in how many participants we should refer from our sample: Explain This - I dont get these answers From the link above you can see that an appropriate citation for the AQ10 would be (Allison, Auyeung, and Baron-Cohen, (2012)) As mentioned, the higher the score on the AQ10 the more autistic-like traits a participant is said to show. You could do this by code with filter(aq_scores, Id == 87), which would give you a tibble of 1x2 showing the ID number and score. If you just wanted the score you could use pull() which we havent shown you that yet: filter(aq_scores, Id == 87) %&gt;% pull(AQ). The answer is an AQ score of 2. Same as above but changing the argument of the filter. filter(aq_scores, AQ == 3) %&gt;% count(). The answer is 13. Remember you can do this by counting but the code makes it reproducible and accurate every time. You might make mistakes. filter(aq_scores, AQ &gt; 6) %&gt;% count() or filter(aq_scores, AQ &gt;= 7) %&gt;% count(). The answer is 6. 2.3.8 Task 8: One Last Thing on Pipes You now have a complete code to load in your data, convert it to Tidy, combine the tables and calculate an AQ score for each participant. But, if you look at it, some of your code could be more efficient by using pipes. Go back through your code and rewrite it using pipes %&gt;% so that it is as efficient as possible. Helpful Hint At any point where the first argument of your function is the name of a variable created before that line, there is a good chance you could have used a pipe! Here are all the bits of this code that could be piped together into one chain: rlong &lt;- gather(responses, Question, Response, Q1:Q10) rlong2 &lt;- inner_join(rlong, qformats, “Question”) rscores &lt;- inner_join(rlong2, scoring, c(“QFormat”, “Response”)) aq_scores &lt;- rscores %&gt;% group_by(Id) %&gt;% summarise(AQ = sum(Score)) Job Done - Activity Complete! You have now recapped one-table and two-table verbs. These are great to know as for example, in the above Activity, it actually only took a handful of reproducible steps to get from messy data to tidy data; could you imagine doing this by hand in Excel through cutting and pasting? Not to mention the mistakes you could make! You should now be ready to complete the Homework Assignment for this lab. The assignment for this Lab is FORMATIVE and is NOT to be submitted and will NOT count towards the overall grade for this module. However you are strongly encouraged to do the assignment as it will continue to boost your data-wrangling skills which you will need in future assignments. If you have any questions, please post them on the slack forum under the channel #level2_2018. Finally, don’t forget to add any useful information to your Portfolio before you leave it too long and forget. Happy wrangling! Excellent work! You are a DataWrangling expert! Now go try the assignment! 2.4 Assignment This is a formative assignment but we strongly encourage you to do the assignment as the knowledge gained from the practical activities in this lab will be super important to your future-self! Lab 2: Formative Data Wrangling Assignment In order to complete this assignment you will need to download the data .csv files, as well as the assignment .Rmd file, which you need to edit, titled GUID_Level2_Lab2.Rmd. These can be downloaded within a zip file from the below link. Once downloaded and unzipped, you should create a new folder that you will use as your working directory; put the data files and the .Rmd file in that folder and set your working directory to that folder through the drop-down menus at the top. Download the Assignment .zip file from here. Now open the assignment .Rmd file within RStudio. You will see there is a code chunk for each task. Much as you did in the first assignment, follow the instructions on what to edit in each code chunk. This will often be entering code based on the lab you have just done as opposed to always just entering a value. In the lab we recapped on data-wrangling using the Wickham 6 verbs, looked at additional functions such as gather() and inner_join(), and at piping chains of code for efficiency using %&gt;%. You will need these skills to complete the following assignment so please make sure you have carried out the PreClass and InClass activities before attempting this formative assignment. Remember to follow the instructions and if you get stuck at any point to post questions on rguppies.slack.com channel #level2_2018. Also, two useful online resources are: Hadley Wickham’s R for Data Science book @ http://r4ds.had.co.nz RStudio’s dplyr cheatsheet @ Rstudio.com 2.4.1 Today’s Topic - The Ageing Brain A key topic in current psychologial research, and one which forms a main focus for some of the research in our School, is that of human ageing. In this research we use brain imaging techniques to understand how changes in brain function and structure relate to changes in perception and behaviour. A typical ‘ageing’ experiment will compare a measure (or a number of measures) such as performance on a cognitive or perceptual task between younger and older adults (i.e. a between-subjects design experiment). However, in order to make sure we are studying ‘healhty’ ageing, we first have to ‘screen’ our older participants for symptoms of age-related dementia (Alzheimer’s Disease), where cognitive function can be significantly impaired. We do this using a range of cognitive tests. Some studies will also test participants’ sensory acuity (ability to perceive something), as a function of age (particularly eyesight and hearing). The data you have downloaded for this lab is example screening data taken from research investigating how the ageing brain processes different types of sounds. The tests used in this study are detailed below. Please note that the links are there to provide you with further information and examples of the tests once you have completed the assignment if you so wish; you do not have to read them to complete the assignment. Montreal Cognitive Assessment (MoCA) : a test specifically devised as a stand-alone screening tool for mild cognitive impairment. Assesses visuospatial skills, memory, language, attention, orientation, and abstraction skills. Example here Working Memory Digit Span Test (D-SPAN): measures the capacity of participants’ short-term (working) memory. Example here D2 Test of Attention: measures participants’ selective and sustained concentration and visual scanning speed. Example here Better Hearing Institute Quick Hearing Check: a self-report questionnaire which measures participants’ subjective experience of their own hearing abilities. Paper version and scoring on pages 8-9. Example here and Main Test page here The Data Files You have just downloaded the three .csv files containing all the data you need. Below is a list of the .csv file names and a description of the variables each contains: p_screen.csv contains particpants demographic information including: ID Participant Id number - for confidentiality (no names or other identifying info) AGE in years SEX M for male, F for female HANDEDNESS L for left-handed, R for right-handed EDUCATION in years MUSICAL whether they have any musical abilties/experience (YES or NO) FLANG speak any foreign languages (YES or NO) MOCA Montreal Cognitive Assessment score D-SPAN Working Memory Digit Span test score D2 D2 Test of Attention score QHC_responses.csv contains participants’ responses to each question on the Better Hearing Institute Quick Hearing Check (QHC) questionnaire. Column 1 represents participants’ ID (matching up to that in p_screen.csv). Each column thereafter represents the 15 questions from the questionnaire. Each row represents a participant and their response to each question. QHC_scoring.csv contains the scoring key for each question of the QHC, with the columns: RESPONSE the types of responses participants could give (STRONGLY DISAGREE, SLIGHTLY DISAGREE, NEUTRAL, SLIGHTLY AGREE, STRONGLY AGREE) SCORE the points awarded for each response type (from 0 to 4). A score for each participant can be calculated by converting their categorical responses to values and summing the values. Before starting lets check: The .csv files are saved into a folder on your computer and you have manually set this folder as your working directory. The .Rmd file is saved in the same folder as the .csv files. For assessments we ask that you save it with the format GUID_Level2_Lab2.Rmd where GUID is replaced with your GUID. Though this is a formative assessment, it may be good practice to do the same here. 2.4.2 Load in the data You will see a code chunk called libraries, similar to the one below, at the top of your .Rmd assignment file. It is all set-up to load in the data for you and to call tidyverse to the library(). Run this code chunk now to bring in the data and tidyverse. You can do this in the console, in a script, or even through the code chunk by clicking the small green play symbol in the top right of the code chunk. library(&quot;tidyverse&quot;) screening &lt;- read_csv(&quot;p_screen.csv&quot;) responses &lt;- read_csv(&quot;QHC_responses.csv&quot;) scoring &lt;- read_csv(&quot;QHC_scoring.csv&quot;) View the data It is always a good idea to familiarise yourself with the layout of the data that you have just loaded in. You can do this through using glimpse() or View() in the Console window, but you must never put these functions in your assignment file. The Tasks: Now that we have the data loaded, tidyverse attached, and have viewed our data, you should now try to complete the following 9 tasks. You may want to practice them first to get the correct code and format, and to make sure they work. You can do this in the console or a script, but remember, once you have the correct code, edit the necessary parts of the assignment .Rmd file to produce a reproducible Rmd file. This is what you will do from now on for all other assessment files so practicing this now will really help. In short, go through the tasks and change only the NULL with what the question asks for and then make sure that the file knits at the end so that you have a fully reproducible code. 2.4.3 Task 1 - Oldest Participant Replace the NULL in the T1 code chunk with the Participant ID of the oldest participant. Store this single value in oldest_participant (e.g. oldest_participant &lt;- 999. # hint: look at your data, who is oldest? oldest_participant &lt;- NULL 2.4.4 Task 2 - Arranging D-SPAN Replace the NULL in the T2 code chunk with code that arranges participants’ D-SPAN performance from highest to lowest using the appropriate one-table dplyr (i.e., Wickham) verb. Store the output in cogtest_sort. (e.g. cogtest_sort &lt;- verb(data, argument)) # hint: arrange your screening data cogtest_sort &lt;- NULL 2.4.5 Task 3 - Foreign Language Speakers Replace the NULL in each of the two lines of code chunk T3, so that descriptives has a column called n that shows the number of participants that speak a foreign language and number of participants that do not speak a foreign language, and another column called median_age that shows the median age for those two groups. If you have done this correctly, descriptives should have 3 columns and 2 rows of data, not including the header row. # hint: First need to group_by() foreign language screen_groups &lt;- NULL # hint: second need to summarise(). Pay attention to specific column names given. descriptives &lt;- NULL 2.4.6 Task 4 - Creating Percentage MOCA scores Replace the NULL in the T4 code chunk with code using one of the dplyr verbs to add a new column called MOCA_Perc to the dataframe screening In this new column should be the MOCA scores converted to percentages. The maximum achievable score on MOCA is 30 and percentages are calculated as (participant score / max score) * 100. Store this output in screening. # hint: mutate() something using MOCA and the percentage formula screening &lt;- NULL 2.4.7 Task 5 - Remove the MOCA column Now that we have our MoCA score expressed as a percentage MOCA_Perc we no longer need the raw scores held in MOCA. Replace the NULL in the T5 code chunk using a one-table dplyr verb to keep all the columns of screening, with the same order, but without the MOCA column. Store this output in screening. # hint: select your columns screening &lt;- NULL Halfway There! The remaining tasks focus on merging two tables. You suspect that the older adults with musical experience might report more finely-tuned hearing abilities than those without musical experience. You therefore decide to check whether this trend exists in your data. You measured participants’ self reported hearing abilties using the Better Hearing Institute Quick Hearing Check Questionnaire. In this questionnaire, participants rated the extent to which they agree or disagree with a list of statements (e.g. ‘I have a problem hearing over the telephone’) using a 5 point Likert scale (Strongly Disagree, Slightly Disagree, Neutral, Slightly Agree, Strongly Agree). Each participant’s response to each question is contained in the responses dataframe in your environment. Each response type is worth a certain number of points (e.g. Strongly Disagree = 0, Strongly Agree = 5) and the scoring key is contained in the scoring dataframe. A score for each participant is calculated by totalling up the number of points across all the questions to derive an overall score. The lower the overall score, the better the participants’ self-reported hearing ability. In order to score the questionnaire we first need to perform a couple of steps. 2.4.8 Task 6 - Gather the Responses together Replace the NULL in the T6 code chunk using code to gather the responses to all the questions of the QHC from wide format to tidy/long format. Name the first column Question and the second column RESPONSE. Store this output in responses_long. # hint: gather the question columns (Q1:Q15) in responses responses_long &lt;- NULL 2.4.9 Task 7 - Joining the data Now we need to join the number of points for each response in scoring to the participants’ responses in responses_long. Replace the NULL in the T7 code chunk using inner_join() to combine responses_long and scoring into a new variable called responses_points. # hint: join them by the column common to both scoring and responses_long responses_points &lt;- NULL 2.4.10 Task 8 - Working the Pipes Below we have given you a code chunk with 5 lines of code. The code takes the data in its current long format and then creates a QHC score for each participant, before calculating a mean QHC score for the two groups of participants - those that play musical intruments and those that don’t - and stores it in a variable called musical_means. participant_groups &lt;- group_by(responses_points, ID) participant_scores &lt;- summarise(participant_groups, Total_QHC = sum(SCORE)) participant_screening &lt;- inner_join(participant_scores, screening, &quot;ID&quot;) screening_groups_new &lt;- group_by(participant_screening, MUSICAL) musical_means &lt;- summarise(screening_groups_new, mean_score = mean(Total_QHC)) Replace the NULL in the T8 code chunk with the following code converted into a functioning pipeline using pipes. Put each function on a new line one under the other. This pipeline should result in the mean QHC values of musical and non-musical people stored in musical_means which should be made of two rows by two columns. # hint: in pipes, the output of the previous function is the input of the subsequent function. # hint: function1 %&gt;% function2 musical_means &lt;- NULL 2.4.11 Task 9 - Difference in Musical Means Finally, replace the NULL in the T9 code chunk with a single value, to two decimal places, that is the value of how much higher the QHC score of people who play music is compared to people who don’t play music (e.g. 2.93) # hint: look in musical means and enter the difference between the two means. QHC_diff &lt;- NULL Job Done - Activity Complete! Well done, you are finished! Now you should go check your answers against the solution file which can be found on Moodle. You are looking to check that the answers you have submitted are exactly the same as the ones in the solution - for example, remember that Mycolumn is different to mycolumn and only one is correct. If you have any questions, please post them on the moodle forum or on the rguppies.slack.com forum #level2_2018. 2.5 Solutions to Questions Below you will find the solutions to the questions for the Activities for this chapter. Only look at them after giving the questions a good try and speaking to the tutor about any issues. 2.5.1 PreClass Activities 2.5.1.1 PreClass Task 1 # To include variables: select(pong_data, Participant, PaddleLength, TrialNumber, BackgroundColor, HitOrMiss) # To exclude variables: select(pong_data, -JudgedSpeed, -BallSpeed, -BlockNumber) # To select in order: select(pong_data, BallSpeed, HitOrMiss, JudgedSpeed, Participant, TrialNumber) Return to Task 2.5.1.2 PreClass Task 2 arrange(pong_data, desc(HitOrMiss), desc(JudgedSpeed)) Return to Task 2.5.1.3 PreClass Task 3 filter(pong_data, JudgedSpeed == 1, BallSpeed %in% c(&quot;2&quot;, &quot;4&quot;, &quot;5&quot;, &quot;7&quot;), HitOrMiss == 0) Return to Task 2.5.1.4 PreClass Task 4 pong_data_filt &lt;- filter(pong_data, TrialNumber &gt;= 2) # remember you can call this variable anything, as long as it makes sense to yourself and others pong_data2 &lt;- mutate(pong_data_filt, TrialNumber = TrialNumber - 1) Return to Task 2.5.1.5 PreClass Task 5 group_by(pong_data2, BlockNumber, BackgroundColor) Return to Task 2.5.1.6 PreClass Task 6 pong_data2_group &lt;- group_by(pong_data2, BackgroundColor, PaddleLength) pong_data2_hits &lt;- summarise(pong_data2_group, total_hits = sum(HitOrMiss)) You should find that the number of hits made with the small paddle (50) and the red color background is 516 Return to Task 2.5.2 InClass Actitivies 2.5.2.1 InClass Task 2 library(tidyverse) # or library(&quot;tidyverse&quot;) # both do the same thing. Return to Task 2.5.2.2 InClass Task 3 responses &lt;- read_csv(&quot;responses.csv&quot;) qformats &lt;- read_csv(&quot;qformats.csv&quot;) scoring &lt;- read_csv(&quot;scoring.csv&quot;) pinfo &lt;- read_csv(&quot;pinfo.csv&quot;) Return to Task 2.5.2.3 InClass Task 7 aq_scores &lt;- rscores %&gt;% group_by(Id) %&gt;% # group by the ID number in column Id summarise(AQ = sum(Score)) # sum column Score to obtain AQ scores. Return to Task 2.5.2.4 InClass Task 8 aq_scores2 &lt;- responses %&gt;% gather(Question, Response, Q1:Q10) %&gt;% inner_join(qformats, &quot;Question&quot;) %&gt;% inner_join(scoring, c(&quot;QFormat&quot;, &quot;Response&quot;)) %&gt;% group_by(Id) %&gt;% summarise(AQ = sum(Score)) Return to Task 2.5.3 Assignment Solution 2.5.3.1 Assignment Task 1 - Oldest Participant Whether you coded this answer or just read from the data, the Participant with ID Number 3 is the oldest. oldest_participant &lt;- 3 # This could also be answered with code. We haven&#39;t quite shown you how yet but it would look like this: oldest_participant_code &lt;- arrange(screening, desc(AGE)) %&gt;% slice(1) %&gt;% pull(ID) Return to Task 2.5.3.2 Assignment Task 2 - Arranging D-SPAN arrange() is the main function here You also needed to use desc() to sort from high to low cogtest_sort &lt;- arrange(screening, desc(DSPAN)) Return to Task 2.5.3.3 Assignment Task 3 - Foreign Language Speakers First group the screening data by FLANG using group_by() Next, summarise, paying attention to use the variable names as instructed screen_groups &lt;- group_by(screening, FLANG) descriptives &lt;- summarise(screen_groups, n = n(), median_age = median(AGE)) Return to Task 2.5.3.4 Assignment Task 4 - Creating Percentage MOCA scores mutate() is the function to add a new column to data Here we are mutating/adding on a column called MOCA_Perc which shows a participant’s MOCA score divided by 30 and multiplied by 100. screening &lt;- mutate(screening, MOCA_Perc = (MOCA / 30) * 100) Return to Task 2.5.3.5 Assignment Task 5 - Remove the MOCA column select() is the key function to keep and remove certain columns. Two options here; both would give the same dataframe. The first option shows how to deselect a column and keep everything else. The second option shows how to select all the columns you want. Remember that order is very important and you should select the columns in the order you want. # First Option screening &lt;- select(screening, -MOCA) # Second Option screening &lt;- select(screening, ID, AGE, SEX, HANDEDNESS, EDUCATION, MUSICAL, FLANG, DSPAN, D2, MOCA_Perc) Return to Task 2.5.3.6 Assignment Task 6 - Gather the Responses together gather() is the function to use here. People take a while to understand this function but spend some time looking at the example below and it will start to make some sense. The first argument is the data. In this case responses. The second argument is the name of a new column which will hold all the question numbers (e.g. Q1, Q2, Q3, etc). This could be called anything but here we are calling it Question. The third argument is the name of a new column which will hold the answers to the questions stored in Question - e.g. the actual answer for that participant to Q1 will appear in this column. Again this could be called anything but here we are calling it RESPONSE. The last argument is the name of the columns you want to gather. Here we are gathering all columns between the Q1 column and the Q15 column. responses_long &lt;- gather(responses, Question, RESPONSE, Q1:Q15) Return to Task 2.5.3.7 Assignment Task 7 - Joining the data inner_join() will combine all common information in two sets of data by a common column or columns. Here we are joining the data in responses_long with the data in scoring by the common column RESPONSE. Keep in mind that inner_join() keeps only the rows that have data in both datasets. It will remove rows that only have data in one dataset. When joining two datasets, join by ALL common columns when there is more than one column in common. responses_points &lt;- inner_join(responses_long, scoring, &quot;RESPONSE&quot;) Return to Task 2.5.3.8 Assignment Task 8 - Working the Pipes This is the code we started with. participant_groups &lt;- group_by(responses_points, ID) participant_scores &lt;- summarise(participant_groups, Total_QHC = sum(SCORE)) participant_screening &lt;- inner_join(participant_scores, screening, &quot;ID&quot;) screening_groups_new &lt;- group_by(participant_screening, MUSICAL) musical_means &lt;- summarise(screening_groups_new, mean_score = mean(Total_QHC)) Below is how to transcribe the above series of functions into a pipeline. Remember, when using pipes, the output of the previous function is the input of the subsequent function musical_means &lt;- group_by(responses_points, ID) %&gt;% summarise(Total_QHC = sum(SCORE)) %&gt;% inner_join(screening, &quot;ID&quot;) %&gt;% group_by(MUSICAL) %&gt;% summarise(mean_score = mean(Total_QHC)) Return to Task 2.5.3.9 Assignment Task 9 - Difference in Musical Means People who play music have a QHC score that is 1.53 units higher than people who don’t play music. You can do this by looking in musical_means, reading the values, and doing some quick maths. A second option is through code. Code is always better as it can reduce error and is reproducible! # Option 1 QHC_diff &lt;- 1.53 # Option 2 # You will soon learn the functions to do this by code but here is how you could do it. QHC_diff_code &lt;- spread(musical_means, MUSICAL, mean_score) %&gt;% mutate(diff = YES - NO) %&gt;% pull(diff) %&gt;% round(2) Return to Task Chapter Complete! 2.6 Additional Material Below is some additional material that might help your wrangling. More on read_csv() In the preclass activity we used the following code to load in our data: pong_data &lt;- read_csv(&quot;PongBlueRedBack 1-16 Codebook.csv&quot;) This is a totally acceptable approach and it is the one we will use 99% of the time. Now one thing to note that the read_csv() function by default always loads any number as double, meaning that it can take a decimal. As shown here by the at the start of each variable. The one column that is not a double is the BackgroundColor column, which is of course “characters” . Note: we can use glimpse() from dplyr to check our data types. glimpse(pong_data) ## Observations: 4,608 ## Variables: 8 ## $ Participant &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1... ## $ JudgedSpeed &lt;dbl&gt; 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1... ## $ PaddleLength &lt;dbl&gt; 50, 250, 50, 250, 250, 50, 250, 50, 250, 50, 5... ## $ BallSpeed &lt;dbl&gt; 5, 3, 4, 3, 7, 5, 6, 2, 4, 4, 7, 7, 3, 6, 5, 7... ## $ TrialNumber &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,... ## $ BackgroundColor &lt;chr&gt; &quot;red&quot;, &quot;blue&quot;, &quot;red&quot;, &quot;red&quot;, &quot;blue&quot;, &quot;blue&quot;, &quot;... ## $ HitOrMiss &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0... ## $ BlockNumber &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1... However you might not always want this default and might want to stipulate the data-type to load the data in as. Particularly if you already know the data-type you should have. Let’s look at this first and then talk about it. pong_data3 &lt;- read_csv(&quot;PongBlueRedBack 1-16 Codebook.csv&quot;, col_types = &quot;iiiiicii&quot;) Which if we look at again, we now see that the numerical data are integers instead of double . glimpse(pong_data3) ## Observations: 4,608 ## Variables: 8 ## $ Participant &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1... ## $ JudgedSpeed &lt;int&gt; 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1... ## $ PaddleLength &lt;int&gt; 50, 250, 50, 250, 250, 50, 250, 50, 250, 50, 5... ## $ BallSpeed &lt;int&gt; 5, 3, 4, 3, 7, 5, 6, 2, 4, 4, 7, 7, 3, 6, 5, 7... ## $ TrialNumber &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,... ## $ BackgroundColor &lt;chr&gt; &quot;red&quot;, &quot;blue&quot;, &quot;red&quot;, &quot;red&quot;, &quot;blue&quot;, &quot;blue&quot;, &quot;... ## $ HitOrMiss &lt;int&gt; 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0... ## $ BlockNumber &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1... So what is the difference? Note that in the new code we have specified the col_types argument as &quot;iiiiicii&quot;. The col_types argument allows you to control the data type for each variable. If you don’t specify this argument, the default is for read_csv() to guess, and when it sees numbers in a column, it will default to treating it as type double. What does the &quot;iiiiicii&quot; string do? Well, we know that there are 8 different columns in the csv file, and we have 8 characters in the string &quot;iiiiicii&quot;; each letter in this string tells read_csv() the data type for each of these columns. The string has five “i”s followed by one “c” followed by two more “i”s, which tells read_csv() to treat the first five columns as type integer (i), the sixth column as type character (c), and the last two columns as type integer. (If we wanted a column to be read in as a double, we would use &quot;d&quot;.) Whilst this is a very useful approach if you are already familiar with the type and structure of the data you are working with, it can cause issues if you don’t know that. For instance, you need to know exactly how many columns there are, what order, and what type they are in. So it can get tricky. For this series of lab activities, we will just stick to using the basic read_csv() defaults and not state column types. More on Code Layout One issue we see a lot is people not being able to debug their code quickly (i.e. find issues) because of the way the code is laid out. Pipes (%&gt;%) helps with that, but so does taking new lines for different parts of your code. After a comma (,) or a pipe (%&gt;%), you can take a new line to continue your code to make it easier to read and follow. For example, both of the following will work, but the second is easiest to read. musical_means &lt;- group_by(responses_points, ID) %&gt;% summarise(Total_QHC = sum(SCORE)) %&gt;% inner_join(screening, &quot;ID&quot;) %&gt;% group_by(MUSICAL) %&gt;% summarise(mean_score = mean(Total_QHC)) musical_means &lt;- group_by(responses_points, ID) %&gt;% summarise(Total_QHC = sum(SCORE)) %&gt;% inner_join(screening, &quot;ID&quot;) %&gt;% group_by(MUSICAL) %&gt;% summarise(mean_score = mean(Total_QHC)) And you could even expand this second option further to make it clearer on the group_by() and inner_join() what are the different inputs: musical_means &lt;- group_by(responses_points, ID) %&gt;% summarise(Total_QHC = sum(SCORE)) %&gt;% inner_join(screening, &quot;ID&quot;) %&gt;% group_by(MUSICAL) %&gt;% summarise(mean_score = mean(Total_QHC)) Remember, Tidy Code and Tidy Data Make Wrangling Fun! OK, that is not catchy, but true! "],
["visualisation-through-ggplot2.html", "Lab 3 Visualisation Through ggplot2 3.1 Overview 3.2 PreClass Activity 3.3 InClass Activity 3.4 Assignment 3.5 Solutions to Questions 3.6 Additional Material", " Lab 3 Visualisation Through ggplot2 3.1 Overview In the last lab we encouraged you to always be looking at your data, making sure you are understanding your data and paying attention to how it is made up in regards to data types. A second way of looking at your data is through visualisation - figures and plots - to help understand patterns and effects in your data. For example, when we asked you about the relationship between age and the Ponzo illusion in Lab 1. Visualisation is very important for understanding your data, for example in regards to seeing differences between groups, but also for seeing where things don’t quite match up with what you think is happening. A great example of this is Anscombe’s Quartet, which you can read up about at a later date if you like - see here. The key point is that it is always good to visualise your data and visualisation should be a common step in your practical skill set. Last year you learnt a bit about data visualisation using ggplot2, the main visualisation package of tidyverse, and you can find more info here if you like: ggplot2. Today we will revisit plotting data and expand your skills in order to make effective and informative figures. This will become really beneficial to you as your progress through University and is a skill that applies to multiple careers, not just Psychology. In this lab you will: Recap on visualisation Expand your skills to produce new figures Learn about Mental Rotation 3.2 PreClass Activity Testing Mental Rotation Ability The data we will use today comes from a recent replication of a classic experiment merging the fields of Perception and Cognition. Shepard and Metzler (1971) demonstrated that when participants are shown two similar three-dimensional shapes, one just a rotated version of the other (see the figure below - top panel), and asked participants whether they were the same shape or not, the reaction time and error rates of responses were a function of rotation; i.e. the larger the difference in rotation between the two shapes, the longer it took participants to say “same” or “different”, and the more errors they made. Figure 3.1: The Mental Rotation Task as shown in Ganis and Kievit (2016) Figure 1 The image shown in Figure 3.1 actually comes from the recent replication, Ganis and Kievit (2016). In the top panel the two shapes are the same but the shape on the right is rotated vertically at 150 degrees from the original (the left shape) and so participants should respond “same”. In the bottom panel however the two shapes are different; the one on the right is again rotated at 150 degrees but in this trial it takes longer for participants to realise that they are different shapes. You can read more about Ganis and Kievit (2016) in your own time but the basic methods are that they ran 54 participants on a series of these images using 4 angles of rotation (0, 50, 100, 150 degrees) and asked people to respond same or different on each trial. The data can be downloaded from here. You should use this data to follow along below and try to answer the questions. Visualising Data Download the data folder, unzip it, and save it to a folder you have access to (e.g. your M: drive if using the lab machines). Set your working directory to that folder Session &gt;&gt; Set Working Directory &gt;&gt; Choose Directory Open a new Rscript and save it within the folder that contains the data, giving the script a sensible name, e.g. Lab3_preclass_visualisations.R Copy the three code lines below into your script and run them to bring tidyverse into the library and to read in the two datafiles. library(&quot;tidyverse&quot;) menrot &lt;-read_csv(&quot;MentalRotationBehavioralData.csv&quot;) demog &lt;- read_csv(&quot;demographics.csv&quot;) Portfolio Point - Why load tidyverse and not just ggplot2? This is a really great question as we always seem to be saying to use dplyr or readr or ggplot, but we never actually call them in. Remember however that tidyverse is actually a collection of packages, the most common packages in fact, and we use it to bring in these common packages (including ggplot2) because you will probably need the other packages along with it for the codes to run smoothly. We will try to tell you when you need to call other packages alongside tidyverse but do keep in mind that most of your codes will at least start with the tidyverse package. Small point, if looking for help on ggplot, the package is actually called ggplot2. This is the newer version of the package, so search ggplot2 if you need help. Let’s start by having a look at the data we have brought in. You can do this whichever way you choose; we mentioned three ways in the previous labs - check your notes. First, demog - short for demographics. It has three columns: Participant - the ID of the participant Age - the age of the participant Sex - the sex of the participant Secondly, menrot - short for mental rotation. It has 8 columns: Participant - the ID of the participant; matches to demog Trial - the trial number in the experiment for each participant Condition - the name of the image shown; R indicates the rotated image was different Time - the reaction time to respond on each trial in milliseconds DesiredResponse - what participants should have responded on each trial; Different or Same ActualResponse - what participants did respond on each trial; Different or Same. Angle - the angle that the shape on the right was rotated compared to the shape on the left (0, 50, 100, 150) CorrectResponse - whether the participant was correct or incorrect on a given trial Portfolio Point - A nice procedure Ganis and Kievit (2016) is a very short paper that is really to introduce the stimuli set rather than give an extensive background on the topic of mental rotation - we call this a ‘methods paper’. That said, the writing of the paper is very clear and the procedure is well detailed as to how they ran the actual experiment. When writing a procedure, remember to give as much information as needed to allow someone to exactly replicate your study. Have a read at this procedure when you have time and think about what information is there, but also what information is not there, to help you develop your writing and your reports. For example, which fingers did the participants use to respond and why would that be important? Creating Some Plots We now have our data and we want to create some plots to visualise it. We will show you the code to create four types of plots and then get you to practice more yourself in the labs, but you will remember some of this from Level 1 and you should edit/change the codes we give you and see what changes you can create - this is a great way of working. Be sure to bring any questions with you to the lab. Portfolio Point - A note on how ggplot works The two main things to know about working with ggplot are that: the usual format is: ggplot(data, aes(x = x_axis, y = y_axis)) + geom_type_of_plot() it works on a concept of layers Looking at point a: The first thing you enter is your dataframe, your data. Then within the aes() you say what is my x_axis and y_axis, using the column names from within your dataframe. aes stands for aesthetics and maps data into visual features. Finally you tell the code what type of plot you want. On point b: Layers are a common way for graphics to work. Think about it as creating your first layer and then adding more layers on top to create the figure you want. The first layer is always your data and the x and y axes. The second layer, added by using the plus symbol ‘+’, is the type of plot. We will look at adding more layers as we progress. ggplot() is an incredibly powerful package that is used by a whole range of industries, including newspapers and mainstream media outlets, as it can make quite sophisticated images. 3.2.1 Scatterplots - geom_point() Scatterplots are a great way of visualising continuous data - data that can take any value on the scale it is measured. For example, you can use scatterplots to explore the relationship between two continuous scales such as Age and Reaction Time: Do both variables increase at the same rate? Does one variable increase and the other decrease? Or maybe there is no overall relationship? In our data, say we want to test if the overall average time to respond in the mental rotation task is affected by age of participant. We could show this in a scatterplot. The code below: Wrangles the data to create an average response time for each participant, Mean_Time, and then joins this information to the demographic data, by Participant. All this is stored in menrot_time_age. It then plots a scatterplot (geom_point()) where age is plotted on the x axis, and Mean_Time is on the y axis Finally, it uses an additional aes call to color by Sex which will color each point based on whether it was a male or female participant responding: menrot_time_age &lt;- group_by(menrot, Participant) %&gt;% summarise(Mean_Time = mean(Time, na.rm = TRUE)) %&gt;% inner_join(demog, &quot;Participant&quot;) ggplot(data = menrot_time_age, aes(x = Age, y = Mean_Time, color = Sex)) + geom_point() Figure 3.2: A scatterplot of Mean Time as a function of Age Quickfire Questions Looking at the scatterplot in Figure 3.2, what can you say about the relationship between age and overall response time? as age increases, overall response time increases as age increases, overall response time decreases there is no overall relationship Looking at the scatterplot, what can you say about difference between male and female participants? males show more of an increase in overall response time with age than females females show more of an increase in overall response time with age than males there is no real difference between males and females in terms of overall response time and age Explain This - I don’t get these answers You will have covered correlations briefly before but that is essentially what the first question is asking. If you look at the figure, does it appear that as age increases (x axis) so does overall resposne time (y axis)? Or as age decreases so does overall response time? Or maybe even as age increases, overall response time decreases? Etc etc. Well, actually, looking at the figure there appears to be no relationship between the two variables at all and it is not the case that as one either increases or decreases so does the other. The relationship appears flat. When comparing sex, based on the color of the dots, again there appears to be no major differences here as the relationship looks flat for both sex. Note: It will often be the case that to visualise data you first have to wrangle it into a format. When we do this we will be using the functions we saw in Chapter 2, so make sure you have been through those tasks and have understood what the wrangle verbs are doing and how pipes work. Keep in mind that most functions use the format, function(data, argument) 3.2.2 Histograms - geom_histogram() Histograms are a great way of showing the overall distribution of your data. Is it skewed? Does it look normal? Is it peaky? Is it flat? These are terms that will be familiar to you through the statistics lectures so try to think about them when visualising your data. Looking at our data, say we wanted to test if the overall distribution of mean response times for correct trials was normally distributed. The code below: Wrangles the data to create an average response time for each participant, Mean_Time, and then filters this information for correct trials only. This is then stored in menrot_hist_correct Plots a histogram (geom_histogram()) where Mean_Time is plotted on the x axis, and the count of each value in Mean_Time is plotted on the y axis. The code creates the y axis automatically and we don’t have to state it: menrot_hist_correct &lt;- group_by(menrot, Participant, CorrectResponse) %&gt;% summarise(Mean_Time = mean(Time, na.rm = TRUE)) %&gt;% filter(CorrectResponse == &quot;Correct&quot;) ggplot(data = menrot_hist_correct, aes(x = Mean_Time)) + geom_histogram() Figure 3.3: A histogram of distribution of Mean Time counts Quickfire Questions Looking at the histogram in Figure 3.3, what can you say about the overall shape of the distribution? the data looks reasonably normally distributed the data looks positively skewed the data looks negatively skewed Looking at the histogram, what is the most common average overall response time for correct trials? approximately 2000 milliseconds approximately 2500 milliseconds approximately 3000 milliseconds Explain This - I don’t get these answers Keep in mind that real data will never give that beautiful textbook shape that you see in classic diagrams when looking for normal or skewed data. Your decisions regarding your distributions will often requre a degree of judgement. From your lectures you will remember that positive skewed data means that most of the data is shifted to the left (low numbers) with a tail stretching to the right (high numbers). Negative skew is where most of the data is shifted to the right (high numbers) with a tail stretching to the left (low numbers). Normal data has most of the data in the middle with even tails on either side. Although not perfect, the data shown in our histogram is a good representation of normal data in the real world. As the y axis is the count of the values on the x axis, the most common overall response time can be found by reading the highest column of the data. For this distribution, this looks to be around 2500 milliseconds or 2.5 seconds. 3.2.3 Boxplots - geom_boxplot() Boxplots are a great means for visualising the spread of your data and highlighting outliers. When looking at boxplots, you should consider: Whether the box is skewed or not? Whether the median is in the middle or to one side? Are the box whiskers a similar length on both sides? Are there any outliers - usually highlighted as a star or a dot beyond the whiskers? Looking at our data, let’s look at and compare the distributions of mean reaction times for correct and incorrect responses. The code below: Repeats the first two wrangle steps as when we created a scatterplot, but additionally groups by CorrectResponse, and stores the data in menrot_box_correct Plots a boxplot (geom_boxplot()) of the overall average response times on the y-axis, Mean_Time, split by the condition CorrectResponse on the x-axis Uses an additional aes call to fill the colour of the boxplots, of the two categories, based on whether CorrectResponse was correct or incorrect. Turns off the legend using the guides() call as it isn’t needed because the x-axis tells you which group is which. More on that later though. menrot_box_correct &lt;- group_by(menrot, Participant, CorrectResponse) %&gt;% summarise(Mean_Time = mean(Time, na.rm = TRUE)) %&gt;% inner_join(demog, &quot;Participant&quot;) ggplot(data = menrot_box_correct, aes(x = CorrectResponse, y = Mean_Time, fill = CorrectResponse)) + geom_boxplot() + guides(fill = FALSE) Figure 3.4: A boxplot of the spreads of Mean Time for Correct and Incorrect Responses Quickfire Questions Looking at the boxplots, how many outliers were there? 1 2 3 0 Looking at the boxplots in Figure 3.4,, which condition had the longer median overall average response time to the mental rotation task? Median response time was longer for the Correct responses Median response time was longer for the Incorrect responses Both medians are the same approximately Explain This - I don’t get these answers There are a number of ways of determining outliers which will be discussed in more detail in your lectures. You can do it through standard deviations (usually 2.5 or 3 SD are used as cut-offs) or do it with boxplots where an outlier is determined as 1.5*IQR (inter-quartile range) above or below the top and bottom of the box. Outliers are shown as dots (unless changed) above or below the whiskers of the boxplot. As you can see in the figure, there are no outliers to see in this data. The median is one of the 5 values required to make a boxplot and is shown as the thick black line within the box itself. Looking at the two conditions and comparing the position on the y axis (response time) we can see that the median response time for incorrect trials was higher than correct trials, meaning that people take longer to think on the trials that they get wrong - makes sense if you think about it, uncertainty takes longer and leads to more errors. 3.2.4 Barplots - geom_bar() or geom_col() Barplots typically show the average value of a condition, e.g. the mean, and the average spread of values via error bars, e.g. standard error. We don’t show error bars here below but you will learn about them at a later point. When looking at barplots, the main considerations are whether or not there appears to be a difference between the conditions you are interested in or are all conditions about the same? It is worth knowing that barplots are now used less frequently than they were as they actually do not show a lot of information, as discussed in this blog, One simple step to improving statistical inference. However, you will still see them in the field so it is good to be able to interpret them. Looking at our data, let’s say we are interested in whether there is a difference in the average percentage of correct and incorrect responses across male and female participants. The code below: Wrangles the data through a series of steps to establish the overall percent average for correct and incorrect responses for both sex, stored in menrot_resp_sex. Plots a barplot (geom_bar()) with the condition Sex on the x axis, the Avg_Percent on the y axis, created through the wrangle, and fill the bars based on CorrectResponse Finally, within the geom_bar it says to treat the data as final values and not to average them, stat = &quot;identity&quot;, and makes both columns visible by moving them apart position = position_dodge(.9)) - without this last step the bars would overlap and you wouldn’t see everything. Try changing the .9 total_n_trials &lt;- 96 menrot_resp_sex &lt;- count(menrot, Participant, CorrectResponse) %&gt;% inner_join(demog, &quot;Participant&quot;) %&gt;% mutate(PercentPerParticipant = (n/total_n_trials)*100) %&gt;% group_by(Sex, CorrectResponse) %&gt;% summarise(Avg_Percent = mean(PercentPerParticipant)) ggplot(data = menrot_resp_sex, aes(x = Sex, y = Avg_Percent, fill = CorrectResponse)) + geom_bar(stat = &quot;identity&quot;, position = position_dodge(.9)) Figure 3.5: A barplot of the average percent Correct and Incorrect responses for Female and Male participants - using geom_bar() geom_col() - short for column - is an alternative to geom_bar() that does not require the part of the code where you say to not do anything to the data, i.e. stat=&quot;identity&quot;. This is shown below. Notice the difference in codes but that they produce the same figure! ggplot(data = menrot_resp_sex, aes(x = Sex, y = Avg_Percent, fill = CorrectResponse)) + geom_col(position = position_dodge(.9)) Figure 3.6: A barplot of the average percent Correct and Incorrect responses for Female and Male participants - using geom_col() Quickfire Questions Looking at the barplot and data, on average, which sex had the most correct responses? female male both the same can’t tell Looking at the barplot and data, on average, which sex had the most incorrect responses? female male both the same can’t tell Looking at the code, what happens if you decrease the position.dodge() value? the bars get further apart the bars start to overlap nothing changes in the figure Looking at the code, what happens if you change the aes call of fill to color? the bars will stay a different color the bars become grey and the outlines become different colors nothing changes in the figure Explain This - I don’t get these answers Remember that in barplots the top of the column is the average value of that condition. This is actually why people do not like barplots; though commonly used, they really only show you one value for your data, the average, and they disregard all other information unless some indication of spread is given. With that in mind, comparing the two Correct columns we can see that females had on average more correct responses than males. Doing the same for the Incorrect columns, we can see that males had more incorrect responses than females. This actually makes sense as the response option in the experiment was either correct or incorrect, so when you add all the correct and incorrect percentage responses for one sex together you should get 100%. If females gave more correct reponses then they must have given less incorrect responses. The last two questions are about playing with the code. Remember we said that plots work through a concept of layers. If you set position.dodge() to 0, you will find that one of the columns disappears because they completely overlap now. So we need to set position.dodge() to a reasonable value to have the columns separate. Why not set it at 1? In barplots you often find that the different levels (or categories) of the the same variable are touching. Note however that the value of the dodge, in this case 1, is relative to the size of the x axis - if the scale of your x-axis ran from 0 to 100 then a dodge of 1 will have very little effect. The final point shows that you can add a lot more calls than just x and y axis to change the presentation of your figures. fill changes the color of the columns, color changes the outline color of the columns. We will see more of these as we progress and we will look at the difference between putting them inside the aes() and outside of it. Have a play about with these on other figures and see what happens. 3.2.5 Themes, Labels, Guides, and facet_wraps() Before we finish, we want to mention a couple of other layers you can add to your ggplot calls to make your figures look more professional. We will show you the code here but we want you to run them and teach yourself how they work by changing the code, removing parts within ggplot, and by adding them to the other figures we have shown above. Bring any questions you have about these functions to the upcoming lab. themes - changing the overall presentation of your figure. Try running the below code and comparing the figure to the barplot above. Remember, ?theme_bw() will give some information or look at the cheatsheets for different themes. labels - putting appropriate labels on your figures so readers understand what is being displayed. Try changing the text within the quotes. facet_wraps - splitting data into separate figures for clarity. This will only work when one of your conditions is categorical but it can be a really effective means of displaying information. guides - remove it and see what happens. Do you understand why we use fill in this situation but perhaps not others? Try running and editing this code. total_n_trials &lt;- 96 menrot_better_plot &lt;- count(menrot, Participant, CorrectResponse) %&gt;% inner_join(demog, &quot;Participant&quot;) %&gt;% mutate(PercentPerParticipant = n/total_n_trials) %&gt;% group_by(Sex, CorrectResponse) %&gt;% summarise(Avg_Percent = mean(PercentPerParticipant)) ggplot(data = menrot_better_plot, aes(x = Sex, y = Avg_Percent, fill = CorrectResponse)) + geom_col(position = position_dodge(.9)) + labs(x = &quot;Sex of Participant&quot;, y = &quot;Percent Average (%)&quot;) + guides(fill = FALSE) + facet_wrap(~CorrectResponse) + theme_bw() A figure for all occasions As you progress through Psychology you will come across a variety of different figures and plots, each looking slightly different and giving different information. When looking at these figures, and indeed when choosing one for your own analyses, you have to think about which figure is the most appropriate for your data. For example, scatterplots are great when both variables are continuous; boxplots and histograms are great for viewing spreads of data; barplots are commonly used where one variable is categorical - but as above note that barplots can be misleading and lots of new approaches to display categorical information are being created. Always keep asking yourself, does my plot display my data correctly. We will look more at this in upcoming labs and lectures where we will also work on improving our interpretation of figures. Job Done - Activity Complete! If you have any questions please post them on the Moodle forum or the slack forum rguppies.slack.com under the channel #level2_2018. Finally, don’t forget to add any useful information to your Portfolio before you leave it too long and forget. 3.3 InClass Activity Mental Rotation and Visualisations Visualisation is a key part of data analysis in exploring data, checking assumptions, and displaying results. Also, it is a really important skill for when you want to share your findings with others. As you develop your skills in Psychology, you will start to analyse your own datasets, and learning how best to plot in different ways will benefit you in the long run. Keep in mind that this is a skill that you can carry into your future career even if that is outside of science. For visualisaion we use ggplot2 and below we have listed some great online resources that you might want to consult if you want a fuller understanding. R Graphics Cookbook ggplot2 book ggplot2 cheatsheet ggplot2 Reference Guide It is highly recommended that you consult the ggplot2 cheat sheet when you work with the package to get familiar with it. 3.3.1 Mental Rotation: Angle and Reaction Time We will use the same dataset for this class as we did in the preclass activity. To recap, Ganis and Kievit (2016) created a modern stimuli set to test the classic concept of mental rotation first proposed by Shepard and Metzler (1971). The idea is that the more rotated a test image is from an original image, the longer it will take for participants to determine if they are the same image or if they are completely different images. Results show that this the case and suggest that, when doing mental rotation tasks, people do a form of internal linear interpolation, or internal mental rotation, meaning that they rotate the image in their mind back to the original angle and then compare the two images. This would be opposed to a more rapid contrast and compare approach not involving any rotation. As a result, the more rotation an image needs, the longer the task takes. Ganis and Kievit ran 54 participants on a series of these rotated images using 4 angles of rotation (0, 50, 100, 150 degrees rotated compared to the original) and asked people to respond ‘same’ or ‘different’ on each trial. Today, to further your understanding of this task and to develop your skills in visualisation and interpretation, we will look at mean reaction time for correct trials, as a function of the angle of rotation and sex. The data can be downloaded from here. Portfolio Point - as a function means? You will come across this phrase ‘as a function of’ quite a bit when dealing with visualisations. It means something like ‘compared by’ or ‘across’. So you could say we are going to look at Mean Reaction Time across the four different Angles of Rotation which would be written as Mean Reaction Time as a function of Angle of Rotation. Usually it would be plotted as y axis as a function of x axis. It is a similar idea to the functions we use in our codes in that you want to see what happens when you put y in the function x. It is good to become familiar with the terms and language that is used in reports so that a) you understand what you are reading and b) you can use the same language to give a professional feel. Remember to be developing your own notes as you go! 3.3.2 Task 1: Loading and Viewing the Data Download the data, unzip it, and save it to a folder you have access to - e.g. somewhere on your M: drive Set your working directory to the folder with your data in it. Start a new script, save it to the folder with your data in it. In your script, load in the tidyverse library. Load in both datasets exactly as we did in the preclass, storing the experiment data in menrot and the demographic data in demog. Helpful Hint library() menrot &lt;- read_csv() demog &lt;- read_csv() Remember to always be looking at your data as good practice and to make sure it is as you expect it to be. Use either View() or glimpse() but do this in the console, never in your script or Rmd file. Other useful functions that you can use as checks are: str() - This shows what type your data is. Look for words like table, dataframe, character, integer, numeric. head(), tail(), and names() - These show the top six and bottom six rows with column names, or just the column names with names(). dim() - This shows the dimensions of the data. Refer to the preclass for a list of what all the columns refer to if you are not sure. And keep in mind that from now on you need to be careful with the spelling and punctuation of the variable names at all times - e.g. Female is not the same as female. Quickfire Questions Take a couple of minutes to try the above functions and to answer the following questions. From the options, what type of data is the variable Angle, found in the dataframe menrot? character integer double Type in the box the name of the dataframe that contains information regarding the sex of the participants: From the options, which of these is a column within menrot? Correctresponse CorretcResponse CorrectResponse correctReponse From the options, according to the dim() call, how many rows are there in demog? 3 8 54 5184 Explain This - I don’t get these answers This is about making sure you are loading in your data correctly, using the instructed names - so that you are being reproducible - and that you understand the data you are looking at. If you completed Task 1 successfully, loading in the data to the correct dataframes, then the following answers should work in the above questions. calling str(menrot) and looking through the information that comes out you should see that Angle is an integer. demog should have been where you loaded information regarding the demographics including sex of participant. names(menrot) will give you column names and this is about making sure of the correct spelling: CorrectResponse. dim(demog) shows you the number of rows (54) by the number of columns (3). 3.3.3 Task 2: Recreating the Figure Let’s start by making a representation of the top part of Figure 2 in Ganis and Kievit (2016) - Mean Reaction Time as a function of Angle of Rotation. Copy the lines of code below into your script. Replace the NULLs in order to recreate the figure below similar to that of Ganis and Kievit (2016) Figure 2 (top). Note that this figure shows information for only correct responses just as in Ganis and Kievit (2016). Remember ggplot is a case of layers. The first layer says where is my data and what do I want on each axis. Every subsequent layer says how I want the data displayed - points (geom_point()) with a connecting line (geom_line()). You can see if you can figure out what d_cartesian does by changing the numbers another time. menrot_angle &lt;- filter(menrot, CorrectResponse == NULL) %&gt;% inner_join(demog, NULL) %&gt;% group_by(NULL) %&gt;% summarise(mean_Resp = mean(NULL)) ggplot(data = NULL, aes(x = NULL, y = NULL)) + geom_point() + geom_line() + coord_cartesian(ylim = c(0, 3500), expand = TRUE) Helpful Hint The first four lines, to create menrot_angle, are all functions from the Wickham Six verbs so refer back to your portfolio to see how they work. What in CorrectResponse would allow you to keep just the correct answers? What variable will allow you to join this information to demog? You only really care about the four levels of rotation, so what will you group_by? Which column do you want the mean of for a mean response time? For the ggplot line, think of the format, data, then the x axis name, then the y axis name. See your portfolio for examples. Figure 3.7: Basic Scatterplot of Response Time by Angle of Rotation Group Discussion Point Great, you have replicated the figure! However, do you know what it means? Spend a couple of minutes discussing with your partner or group - what does the figure tell you about mean reaction time and angle of rotation, and how does it fit with the overall theory we introduced above? Answering this question may help: From the options, the figure would suggest that as angle of rotation increases: mean reaction time decreases mean reaction time stays the same mean reaction time increases Portfolio Point - Reaction Time as a function of Rotation As you can see from the figure, consistent with Shepard and Metzler (1971), the participants from Ganis and Kievit (2016) showed an increase in reaction time as the angle of rotation increased. Therefore, Ganis and Kievit (2016) have replicated the findings of Shepard and Metzler (1971). A quick note though is that, yes, mean reaction time does increase with angle of rotation but it is not a consistent increase. You will see that the difference between mean reaction times for 150 and 100 degrees is smaller than between 0 and 50. Reaction times start to plateau after a certain angle of rotation. 3.3.4 Task 3: Examining Additional Variable Effects In the preclass, we looked at sex of participant a lot and that was quite interesting. It is not covered in Ganis and Kievit (2016) so let us take a look at it for them. To your pipeline of Task 2, add the variable Sex to the group_by() function to group the data by Angle and Sex. Running the code again creates the below figure. Helpful Hint group_by(Angle, Sex) Remember, to add more grouping variables just separate them by a comma. Everything else stays the same. Figure 3.8: Separating points by Sex Hmmm, that figure does not look that informative. It looks similar to the one we created but the dots have doubled - we now have 8 instead of 4 - but we do not know which is male or female, and the connecting line is confusing. We need to tell the code how to separate the data based on sex. 3.3.5 Task 4: Grouping the Figure Data In the preclass we used fill and color inside the aes() to change basic information about the figure. There is also one called group. Add a group to your figure code to separate the data by Sex. Run the code again and see what your figure looks like Helpful Hint aes(x = , y = , group = ???) Well at least we now have different lines but we still can’t tell which sex is which line, can we? It just looks like two black parallel lines, one slightly higher than the other. What would be ideal would be changing the color of the points based on whether they are from male or female participants! Fortunately, the geoms can also take information as well. 3.3.6 Task 5: Identifying Groups Using aes() Add an aes() call inside your geom_point() function to color the dots by Sex. Helpful Hint geom_point(aes(??? = ???) You should now have something like this: Figure 3.9: Separate lines for each Sex Great so we can now see that the female line is on top and the male line is on the bottom. But before we start interpreting this figure let’s finish tidying it up with a few more tasks. For example the dots for each data point are perhaps a little small for our old eyes so we could increase them in size. Also, color is great if you can print in color but we could also change the shape of the dots to help people distinguish between the Sex. To do this we will use the additional calls of shape and size within our geom_point(). 3.3.7 Task 6: Changing the Shape and Size of Data Points We want each Sex to have different shaped points, so add a call to shape within the aes() call of our geom_point() function, just like you did for color. However, we want each Sex to have the same size of point, so add a call to size within the geom_point() function, but not inside the aes() call. This time choose an appropriate number for the size instead of naming a variable. Maybe size = 3? Helpful Hint geom_point(aes(color = Sex, Shape = ???), size = ???) This should give you a figure like this: Figure 3.10: Changing Shape and Size of Data Points Portfolio Point - in, out, what’s the aes about? Hopefully you are beginning to spot the difference between setting a call within aes() (which stands for aesthetics) and setting outside the aes(). Outside aes() means that all conditions take the one value or color or type. Inside means that each condition takes a different value or color or type. Let’s look at what we have done above to help us compare. size is called outside of the aes() and we assign it a specific value. As you can see from the Task 6 figure, each condition now has the same size of points. We could set this size to what we want but keep it smallish: 3 or 5 are ok; 50 would be artistic but not that informative. In contrast, we called shape inside the aes() and we set it based on a variable, Sex. Doing it this way ensures that each level of the Sex variable, male or female, get a different shape. Had we instead set shape outside the aes(), something like geom_point(shape = 3, size = 3) then all conditions would have the same shape and the same size. Different numbers relate to different shapes and different sizes. For example compare shape = 3 to shape = 13 Likewise, had we set the size within aes(), something like geom_point(aes(shape = Sex, size = Sex)) then both male and female would have different shapes AND different sizes. You can play around with this in your code to see how things work. And of course there are other arguments you could use. For example if you wanted all points to have the same color, say red for example, then you could do geom_point(color = “red”). Remember to put the quotes around the color. So hopefully this is starting to make sense and you can think about implementing it in your own figures. Note that arguments are separated by a comma. e.g. geom_point(color = “red”, size = 3, shape = 2) 3.3.8 Task 7: Adding Labels and Changing the Background This figure is looking really nice now. Let’s finish it off by making it look a little more professional with appropriate labels and by editing the background. We introduced these to you in the preclass so hopefully you had a play with them to see how they work. To change a label, we use the function labs() and it works like labs(x = &quot;Name&quot;, y = &quot;Name&quot;, title = &quot;Name&quot;). Add this function to your code so that the y axis indicates Mean Reaction Time (ms) and the x axis indicates Anlge of Rotation (degrees). Don’t worry about a title, we tend to not use these in Psychology; we use figure legends underneath instead. Set the figure as theme_bw() - this looks nice but there are other options you might want to try which you can explore through ?theme or the cheatsheet. Helpful Hint labs(x = “…”, y = “…”) + theme_bw() The key thing is to remember to + the layer into the ggplot chain. And don’t get this confused with pipes (%&gt;%). Note: You add (+) layers, and you pipe (%&gt;%) functions. 3.3.9 Task 8: Separating a Variable and Removing Legends Finally in the preclass we showed you two other functions that you could use to tidy up figures: facet_wrap() and guides(). facet_wrap() is really effective for splitting up figures into panels based on a variable; it works like facet_wrap(~variable) where ~ can be read as by. So “split up the figure by variable”, for example Sex. And if you had two variables to split the figure by, then: facet_wrap(~variable1 + variable2). guides() is handy for turning on and off legends which might be taking up space. For instance, if you use a facet_wrap() to split the panels into Female and Male, do you really need the legend on the right saying Female and Male? You will normally have a guide for each color, shape, etc, call within an aes(). It works like guide(call = FALSE). Add a facet_wrap() to have separate panels in your figure based on Sex. Turn off all guides so that you have no legend in your figure. Helpful Hint facet_wrap(~variable) guides(group = FALSE, ???? = False, ….) Group Discussion Point If you have followed all the tasks correctly then you should have the following figure: Figure 3.11: The finished figure! Take a few minutes with your partner or group to look at the figure and try to intepret it in terms of reaction time as a function of rotation and sex. Try answering the following questions to help your discussion: In both sexes, mean reaction time decreases with increases with is unaffected by angle of rotation. Angle of Rotation influences female participants male participants more than female participants male participants Portfolio Point - Interpreting the results By looking at the figure, as angle of rotation increases (moving to the right of the x axis), the mean reaction time increases (getting higher on the y axis), indicating that participants take longer to respond the further the target image is rotated from the original. Also, as the male mean reaction times are quicker overall than the female mean reaction times, and the differences in reaction times between 0 degrees and 150 degrees is smaller in males, then you could perhaps say that males are affected less than females, or males perform the task quicker. Keep in mind that we are only looking at correct responses which would rule out male participants just responding quicker overall, they are in fact responding correctly quicker overall. This is a topic that has received much attention over the years and you should refer to the reference sections of the two main papers of this activity should you wish to follow the topic further and wish to add this to your Portfolio. 3.3.10 Additional Considerations (briefly!) Many of the options we have seen in terms of geom_point() could have been applied to geom_line() to make alterations to the line. Try playing with these options. For example, the code line below would result in both sexes having a red line of equal size but the style of line being different. Give it a shot! geom_line(aes(linetype = Sex), size = .5, color = &quot;red&quot;) Finally, if you look closely at the figure, you will see that the line between points actually goes in front of the points. It looks a bit messy. How could you make it tidier by having the line run behind the data points? Remember that it is a series of layers. It draws one layer and then the next, so ggplot() + geom_line() + geom_point() would draw the lines first and then the points on top. Give it a go! Job Done - Activity Complete! Understanding figures through ggplot can seem like trial and error until you have a lot of experience. That is fine but just remember to keep adding notes to your Portfolio to help out the future you. In this class we have looked at working with layers and a variety of calls to shape, color, fills, etc, to create professional looking figures. And the beauty of it all is that once you have a figure you really like, you run the code and you get exactly the same figure again! Amazing or what??? You should now be ready to complete the Homework Assignment for this lab. The assignment for this Lab is summative and should be submitted through the Moodle Level 2 Assignment Submission Page no later than 1 minute before your next lab. If you have any questions, please post them on the slack forum under the channel #level2_2018. Finally, don’t forget to add any useful information to your Portfolio before you leave it too long and forget. 3.4 Assignment This is a summative assignment. Instructions as to how to access and submit your assignment will be made available during the course. 3.5 Solutions to Questions Below you will find the solutions to the questions for the Activities for this chapter. Only look at them after giving the questions a good try and speaking to the tutor about any issues. 3.5.1 InClass Activities 3.5.1.1 InClass Task 1 library(&quot;tidyverse&quot;) menrot &lt;- read_csv(&quot;MentalRotationBehavioralData.csv&quot;) demog &lt;- read_csv(&quot;demographics.csv&quot;) Return to Task 3.5.1.2 InClass Task 2 menrot_angle &lt;- filter(menrot, CorrectResponse == &quot;Correct&quot;) %&gt;% inner_join(demog, &quot;Participant&quot;) %&gt;% group_by(Angle) %&gt;% summarise(mean_Resp = mean(Time)) ggplot(data = menrot_angle, aes(x = Angle, y = mean_Resp)) + geom_point() + geom_line() + coord_cartesian(ylim = c(0, 3500), expand = TRUE) Return to Task 3.5.1.3 InClass Task 3 menrot_angle_sex &lt;- filter(menrot, CorrectResponse == &quot;Correct&quot;) %&gt;% inner_join(demog, &quot;Participant&quot;) %&gt;% group_by(Sex, Angle) %&gt;% summarise(mean_Resp = mean(Time)) ggplot(data = menrot_angle_sex, aes(x = Angle, y = mean_Resp)) + geom_point() + geom_line() + coord_cartesian(ylim = c(0, 3500), expand = TRUE) Return to Task 3.5.1.4 InClass Task 4 menrot_grouped &lt;- filter(menrot, CorrectResponse == &quot;Correct&quot;) %&gt;% inner_join(demog, &quot;Participant&quot;) %&gt;% group_by(Sex, Angle) %&gt;% summarise(mean_Resp = mean(Time)) ggplot(data = menrot_grouped, aes(x = Angle, y = mean_Resp, group = Sex)) + geom_point() + geom_line() + coord_cartesian(ylim = c(0, 3500), expand = TRUE) Return to Task 3.5.1.5 InClass Task 5 menrot_grouped_color &lt;- filter(menrot, CorrectResponse == &quot;Correct&quot;) %&gt;% inner_join(demog, &quot;Participant&quot;) %&gt;% group_by(Sex, Angle) %&gt;% summarise(mean_Resp = mean(Time)) ggplot(data = menrot_grouped_color, aes(x = Angle, y = mean_Resp, group = Sex)) + geom_point(aes(color = Sex)) + geom_line() + coord_cartesian(ylim = c(0, 3500), expand = TRUE) Return to Task 3.5.1.6 InClass Task 6 menrot_shape_size &lt;- filter(menrot, CorrectResponse == &quot;Correct&quot;) %&gt;% inner_join(demog, &quot;Participant&quot;) %&gt;% group_by(Sex, Angle) %&gt;% summarise(mean_Resp = mean(Time)) ggplot(data = menrot_shape_size, aes(x = Angle, y = mean_Resp, group = Sex)) + geom_point(aes(color = Sex, shape = Sex), size = 3) + geom_line() + coord_cartesian(ylim = c(0, 3500), expand = TRUE) Return to Task 3.5.1.7 InClass Task 7 menrot_lab_theme &lt;- filter(menrot, CorrectResponse == &quot;Correct&quot;) %&gt;% inner_join(demog, &quot;Participant&quot;) %&gt;% group_by(Sex, Angle) %&gt;% summarise(mean_Resp = mean(Time)) ggplot(data = menrot_lab_theme, aes(x = Angle, y = mean_Resp, group = Sex)) + geom_point(aes(color = Sex, shape = Sex), size = 3) + geom_line() + coord_cartesian(ylim = c(0, 3500), expand = TRUE) + labs(x = &quot;Angel of Rotation (degrees)&quot;, y = &quot;Mean Reaction Time (ms)&quot;) + theme_bw() Return to Task 3.5.1.8 InClass Task 8 manrot_facet_guide &lt;- filter(menrot, CorrectResponse == &quot;Correct&quot;) %&gt;% inner_join(demog, &quot;Participant&quot;) %&gt;% group_by(Sex, Angle) %&gt;% summarise(mean_Resp = mean(Time)) ggplot(data = manrot_facet_guide, aes(x = Angle, y = mean_Resp, group = Sex)) + geom_point(aes(color = Sex, shape = Sex), size = 3) + geom_line() + coord_cartesian(ylim = c(0, 3500), expand = TRUE) + labs(x = &quot;Angel of Rotation (degrees)&quot;, y = &quot;Mean Reaction Time (ms)&quot;) + facet_wrap(~Sex) + guides(group = FALSE, color = FALSE, shape = FALSE) + theme_bw() Return to Task Chapter Complete! 3.6 Additional Material Below is some additional material that might help your figures in reports 3.6.1 Combining Figures Space within a report is a commodity. Figures can be incredibly useful in getting information across in a very efficient manner, but when you have a strict word count, having multiple figures can really chew into the limit, given that each figure needs a legend and each legend counts. One way to get around this is to merge figures together into one big figure, that perhaps convey similar or related information. We are going to show you how to do that using a package called patchwork. Thinking back to the preclass, figures like boxplots and histograms, when combined, can be incredibly useful in understanding the overall shape of your data and whether or not it fits the assumptions of inferential tests, something we will come on to later. If we were to create two separate plots, we might get something like this: menrot_hist_correct &lt;- group_by(menrot, Participant, CorrectResponse) %&gt;% summarise(Mean_Time = mean(Time, na.rm = TRUE)) %&gt;% filter(CorrectResponse == &quot;Correct&quot;) %&gt;% inner_join(demog, &quot;Participant&quot;) ggplot(data = menrot_hist_correct, aes(x = Mean_Time, fill = Sex)) + geom_histogram() + theme_bw() Figure 3.12: A histogram of distribution of Mean Time counts by Sex And: menrot_box_correct &lt;- group_by(menrot, Participant, CorrectResponse) %&gt;% summarise(Mean_Time = mean(Time, na.rm = TRUE)) %&gt;% filter(CorrectResponse == &quot;Correct&quot;) %&gt;% inner_join(demog, &quot;Participant&quot;) ggplot(data = menrot_box_correct, aes(x = CorrectResponse, y = Mean_Time, fill = Sex)) + geom_boxplot() + theme_bw() Figure 3.13: A boxplot of the spreads of Mean Time for Correct Responses by Sex Now given that they both divide the data by sex, you can start to see how the figure legends become a bit repetitive, and how combining them into one figure would potentially make things easier. There are a number of packages to do this, but patchwork is very straightforward and flexible. DO NOT install packages in the Boyd Orr labs; they are already there and just need called in through library(). If it is not there, speak to a member of the team. However, If you are using a computer at home and you haven’t previously installed the patchwork package on your own machine before, you will have to install it first, e.g. install.packages(“patchwork”). Let’s now call in patchwork library(patchwork) The first thing you need to do when patchwork is save your figures in an object (just like you would with the output of any function). Using the code above, this might look like below for the boxplot: p_box &lt;- ggplot(data = menrot_box_correct, aes(x = CorrectResponse, y = Mean_Time, fill = Sex)) + geom_boxplot() + labs(title = &quot;A&quot;) + theme_bw() And below for the histogram: p_hist &lt;- ggplot(data = menrot_hist_correct, aes(x = Mean_Time, fill = Sex)) + geom_histogram() + labs(title = &quot;B&quot;) + theme_bw() Note: The reason for the inclusion of a title on each figure will become clear in a second. Now when you run these codes no figures will be generated as you are saving them as objects. It is important to know this as if someone asks you to make sure the figure is generated when your code knits, and you have saved it as an object, then your figure might not show. If you have save a figure as an object, you can generate the figure by just calling the name of the object. If you look at the ggplot cheatsheet you will see this approach a lot. p_box Figure 3.14: A boxplot of the spreads of Mean Time for Correct Responses by Sex p_hist ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 3.15: A histogram of distribution of Mean Time counts by Sex Great, but we want the figures in a single figure right? Well to do that in patchwork, we simply “add” the figures together using a plus sign (+), as such: p_box + p_hist ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 3.16: A boxplot (A - left) of the spreads of Mean Time for Correct Responses, and histogram (B - right) of distribution of Mean Time counts, both separated by Sex (female - red, male - cyan) And we can use the titles that we put on the figures above to tell readers which figure within the combined figure we are referring to, A or B, left or right, as shown in the Figure legend. Awesome, but we can also change the layout. Say we wanted the figures on top of each other - portrait rather than landscape - well in that instance we divide the figures using the divide sign (/), as such: p_box / p_hist ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 3.17: A boxplot (A - top) of the spreads of Mean Time for Correct Responses, and histogram (B - bottom) of distribution of Mean Time counts, both separated by Sex (female - red, male - cyan) And now we refer to top and bottom, rather than left and right. In fact, patchwork is really flexible and can work with multiple figures and arrangements. Hypothetically, say you had three figures and wanted two on top of one, then you would use the approach of combining “+” and “/” as such: (figure1 + figure2)/figure3 Remember the trick to using patchwork is to save your figures as objects first (p1 &lt;- ggplot(....)) and the rest is easy. But be sure to always know if your figure is shown when knitted or not; more often than not, seeing the figure is more important than seeing the code. Happy Visualising! "],
["revisiting-probability-distributions.html", "Lab 4 Revisiting Probability Distributions 4.1 Overview 4.2 PreClass Activity 1 4.3 PreClass Activity 2 4.4 InClass Activity 1 4.5 InClass Activity 2 (Additional) 4.6 InClass Activity 3 (Additional) 4.7 Assignment 4.8 Solutions to Questions", " Lab 4 Revisiting Probability Distributions 4.1 Overview In this lab you will: Revise probability concepts that we discussed in Level 1 Calculate probabilities Create probability distributions Make estimations from probability distributions. Probability is to some degree the cornerstone of any Psychological theory that is based on quantitative analysis. We establish an outcome (e.g. a difference between two events), then establish the probability of that outcome against some model or standard. Probability is important for quantifying the uncertainty in our conclusions. You will have already seen this in your lectures and in the journal articles that you are reading. We will try to help you gain a deeper understanding of probability through the course of the next few labs and in how we use it to make an inference about a population. We will start by looking at some of the general ideas behind probability. We won’t be using a lot of Psychology data or concepts here as it can be easier to understand in very concrete examples but be sure to try things out, ask questions, and think about how it might relate to Psychology examples. This preclass is a bit of a read so take your time and try to understand it fully. There are no cheatsheets for this lab as we will not be using a specific package. However you can make full use of the R help function (e.g. ?sample) when you are not clear on what a function does. Also, do not be shy to do what we do and run a Google Search for finding out more about some of the stats concepts covered here. There are loads of videos and help pages out there with clear examples to explain difficult concepts. Portfolio Point - Samples, Populations and Inference The population is the whole group that you want to know something about - everyone or everything in that group. The sample is the part of the population that you are testing. The sample is always smaller than the population as it is unlikely that you would ever be able to test everyone in a population, but the sample should be representative of the population based on random sampling. This means that even though you are not using the whole population, the sample you are using represents the whole population because you randomly sampled people into it. If this is true, that the sample is representative of the population, then testing on the sample allows you to make some inference about the population; you infer a characteristic of the population from testing on the sample. 4.1.1 Discrete or Continuous Datasets This is a short tutorial with just a couple of quick recap questions on how the level of measurement can alter the way you tackle probability - i.e. whether the data is discrete or continuous. Quickfire Questions Discrete data can only take integer values (whole numbers). For example, the number of participants in an experiment would be discrete - we can’t have half a participant! Discrete variables can also be further broken down into nominal/categorical and ordinal variables. Fill in the blanks in the below sentences using the words: ordinal, nominal, categorical. Nominal Ordinal data is based on a set of categories that have no natural ordering (e.g. left or right handed). For example, you could separate participants according to left or right handedness or by course of study (e.g., psychology, biology, history, etc.). Nominal Ordinal data is a set of categories that have a natural ordering; you know which is the top/best and which is the worst/lowest, but the difference between categories may not be constant. For example, you could ask participants to rate the attractiveness of different faces based on a 5-item Likert scale (very unattractive, unattractive, neutral, attractive, very attractive). Continuous data on the other hand can take any value. For example, we can measure age on a continuous scale (e.g. we can have an age of 26.55 years), also reaction time or the distance you travel to university every day. Fill in the blanks in the below sentences using the two remaining levels of measurement not offered above Continuous data can be broken into or data - more of this another time. When you read journal articles or when you are working with data in the lab, it is really good practice to take a minute or two to figure out the type of variables you are reading about and/or working with. Explain This - I don’t get these answers The four level of measurements are nominal (also called categorical), ordinal, interval and ratio. Discrete data only uses categories or whole numbers and is therefore either nominal or ordinal data. Continuous data can take any value, e.g. 9.00 or 9.999999999, and so is either interval or ratio data. You should have a good understanding of discrete vs continuous data. This will help you complete the activities of this chapter and in your understanding of probability and inference. 4.2 PreClass Activity 1 4.2.1 General Probability Calculations Today we will being by recapping the concepts of probability calculations from Level 1, looking at discrete distributions - where values are whole numbers (e.g. 1, 2 3 and not 1, 1.1, 1.2 etc). At the end of this section, there are three Quickfire Questions to test your understanding. Bring any that you aren’t clear on to class next week for discussion. When talking about probability, we often are interested in the probability of an event occurring. The probability of an event is represented by a number between 0 and 1, and the letter p. For example, the probability of flipping a coin and it landing on ‘tails’, most people would say, is estimated at p = .5, i.e. the likelihood of getting tails is \\(p = \\frac {1}{2} \\\\) as there is one desired outcome (tails) and two possibilities (heads or tails). Calculating the probability of any discrete event occuring can be formulated as: \\[p = \\frac{number \\ of \\ ways \\ the \\ event \\ could \\ arise}{number \\ of \\ possible \\ outcomes}\\] For example: 1. The probability of drawing the ten of clubs from a standard pack of cards would be 1 in 52: \\(p = \\frac {1}{52} \\ = .019\\). One outcome (ten of clubs) with 52 possible outcomes (all the cards) 2. Likewise, the probability of drawing either a ten of clubs or a seven of diamonds as the the first card that you draw would be 2 in 52: \\(p = \\frac {2}{52} \\ = .038\\). In this case you are adding to the chance of an event occurring by giving two possible outcomes, so it becomes more likely to happen than when you only had one outcome. 3. Now say you have two standard packs of cards mixed together. The probability of drawing the 10 of clubs from this mixed pack would be 2 in 104: \\(p = \\frac{2}{104}= .019\\). Two possible outcomes but more alternatives than above, 104 this time, meaning it is less probable than example 2. 4. Let’s instead say you have two separate packs of cards. The probability of drawing the 10 of clubs from both packs would be: \\(p = \\frac{1}{52} \\times \\frac{1}{52}= .0004\\). The probability has gone down again because you have created an event that is even more unlikely to happen - this is called the joint probability - multiplying the two individual probabilities of separate events (often stated as independent, mutually exclusive events) to find the joint probability. 5. What about the probability of drawing the 10 of clubs from a pack of 52, putting it back (i.e. replacement), and subsequently drawing the 7 of diamonds? Again, this would be represented by multiplying together the probability of each of these events happening: \\(p = \\frac{1}{52} \\times \\frac{1}{52}= .0004\\). 6. Finally, say you draw the 10 of clubs from a pack of 52 but this time don’t replace it. What is the probability that you will draw the 7 of diamonds in your next draw (again without replacing it) and the 3 of hearts in a third draw? This time the number of cards in the pack is fewer for the second (51 cards) and third draws (50 cards) so you take that into account in your multiplication: \\(p = \\frac{1}{52} \\times \\frac{1}{51}\\times \\frac{1}{50}= .000008\\). Portfolio Point - Presenting probabilities So the probability of an event is the number of all the possible ways an event could happen, divided by all the possible outcomes. When you combine probabilities of two separate events you multiple them together to obtain the cumulative probability. But you may have noticed that sometimes we write p = .008, for example, and sometimes p = 0.008. What is the difference? Well nothing really. However, there is a convention that as probability can never go above 1, then the 0 before the decimal place is pointless. Meaning that most people will write p = .008 instead of p = 0.008. We have allowed either version in the answers to this point, but try to get in the habit of writing it without the 0 before the decimal place. Quickfire Questions What is the probability of randomly drawing your name out of a hat of 12 names where one name is definitely your name? Enter your answer to 3 decimal places: What is the probability of randomly drawing your name out of a hat of 12 names, putting it back, and drawing your name again? Enter your answer to 3 decimal places: Tricky: In a stimuli set of 120 faces, where 10 are inverted and 110 are the right way up, what is the probability of randomly removing one inverted face on your first trial, not replacing it, and then removing another inverted face on the second trial? Enter your answer to three decimal places: Helpful Hint Out of 12 possible outcomes you are looking for one possible event. There are two separate scenarios here: in both scenarios there are 12 possible outcomes in which you are looking for one possible event. Since there are two separate scenarios, does this make it more or less likely that you will draw your name twice? Think about the first trial: there are 120 possible outcomes (faces) in which you are looking for 10 possible events (inverted faces). In the second trial you have removed the first inverted face from the stimuli set so there are now only 119 trials in total and 9 inverted faces. Remember you need to combine the probabilities of the first trial and second trial results together! Explain This - I don’t get these answers p = 0.083 or p = .083. One outcome (your name) out of 12 possibilities, i.e. 1/12 p = 0.007 or p = .007. Because you replace the name on both draws it is 1/12. So 1/12*1/12 and then rounded to three decimal places p = 0.006 or p = .006. It starts off as 10 out of 120, but as you remove one inverted face the second round is 9 out of 119. So the formula is (10/120)*(9/119) 4.2.2 Creating a Simple Probability Distribution We will now recap plotting probability distributions by looking at the probability distributions of a simulated coin toss. Work through this example and then apply the logic to the quickfire questions at the end of the section. Imagine we want to know the probability of getting X number of heads in 10 coin flips. To simulate 10 coin flips we used the sample() function where we randomly sample (with replacement) from all possible events: i.e. either heads or tails. Open a new script and copy in the code lines below. The last argument in the code provides the instruction to sample with or without replacement by setting it to TRUE or FALSE respectively. Note1: Because our event labels are strings (text), we enter them into the function as a vector; i.e. in “quotes” Note2: Below the lines of code, you will see the output that we got when we ran our code. Don’t worry if your sequence of heads and tails is different from this output; this is to be expected as we are generating a random sample. library(&quot;tidyverse&quot;) sample(c(&quot;HEADS&quot;, &quot;TAILS&quot;), 10, TRUE) ## [1] &quot;TAILS&quot; &quot;HEADS&quot; &quot;TAILS&quot; &quot;TAILS&quot; &quot;HEADS&quot; &quot;TAILS&quot; &quot;HEADS&quot; &quot;HEADS&quot; ## [9] &quot;HEADS&quot; &quot;TAILS&quot; Portfolio Point - Sampling and Replacement Sampling is simply choosing or selecting something - here we are randomly choosing one of the possible options. Other psychology experiment ‘sampling’ could include randomly selecting participants, randomly choosing which stimuli to present on a given trial, or randomly assigning participants to a condition e.g.drug or placebo…etc. Replacement is putting the sampled option back into the ‘pot’ of possible options. For example, on the first turn you randomly sample HEADS from the options of HEADS and TAILS with replacement, meaning that on the next turn you have the same two options again; HEADS or TAILS. Sampling without replacement means that you remove the option from subsequent turns. So say on the first turn you randomly sample HEADS from the options HEADS and TAILS but without replacement. Now on the second turn you only have the option of TAILS to ‘randomly’ sample from. On the third turn without replacement you would have no options. So replacement means putting the option back for the next turn. Why would you or why wouldn’t you want to use sampling with replacement in our coin toss scenario? If you aren’t sure then set replacement as FALSE and run the code again. The code will stop working after 2 coin flips. We want to sample with replacement here because we want both options available at each sampling - and if we didn’t then we would run out of options very quickly since we’re doing 10 flips. So far our code returns the outcomes from the 10 flips; either heads or tails. If we want to count how many ‘heads’ we have we can simply sum up the heads. However, heads isn’t a number, so to make life easier we can re-label our events as 0 for tails and 1 for heads. Now if we run the code again we can pipe the sample into a sum() function to total up all the 1s (heads) from the 10 flips. Run this line of code a number of times, what do you notice about the output? Note1: As our event labels are now numeric, we don’t need the vector. Note2: 0:1 means all numbers from 0 to 1 in steps of 1. So basically, 0 and 1. sample(0:1, 10, TRUE) %&gt;% sum() ## [1] 8 The ouptut of this line changes every time we run the code as we are randomly sampling 10 coin flips each time. And to be clear, if you get an answer of 6 for example, this means 6 heads, and in turn, 4 tails. By running this code over and over again we are basically demonstrating how a sampling distribution is created. Portfolio Point - What’s a sampling distribution? A sampling distribution shows you the probability of drawing a sample with certain characteristics from the population; e.g. the probability of 5 heads in 10 flips, or the probability of 4 heads in 10 flips, or the probability of X heads in 10 flips of the coin. But in order to create a full and accurate sampling distribution we need to replicate these 10 flips a number of times. The more replications we do the more reliable the estimates. Let’s do 10000 replications of our 10 coin flips. This means we flip the coin 10 times, count how many heads, save that number, and then repeat it 10000 times. We could do it the slow way we demonstrated above, just running the same line over and over and over again and noting the outcome each time. Or we could use the replicate function. Copy this line of code into your script and run it. Here we are doing exactly as we said and saving the 10000 outputs (counts of heads) in the variable called heads10k (k is shorthand for thousand). heads10k &lt;- replicate(10000, sample(0:1, 10, TRUE) %&gt;% sum()) ## int [1:10000] 4 3 4 6 5 6 3 7 3 5 ... The results of these 10000 replications are now stored in a vector called heads10k. If you have a look at heads10k, as shown in the bottom box, it is a series of 10000 numbers between 0 and 10 each indicating the number of heads, or more specifically 1s, that you got in a set of 10 flips. Now in order to complete our distribution we need to: convert the vector (list of numbers for the heads counts) into a data frame (a table) so we can work on it. Then group the results by the number of possible heads; i.e. group all the times we got 5 heads together, all the times we got 4 heads together, etc. Finally, we work out the probability of a heads result, (e.g. probability of 5 heads), by totaling the number of observations for each possible result (e.g. 5 heads) and submitting it to our probability formula above (number of outcomes of event divided by all possible outcomes) so the number of times we got a specific number of heads (e.g. 5 heads) divided by the total number of outcomes (i.e. the number of replications - 10000). We can carry out these steps using the following code: Copy the below code into your script and run it. data10k &lt;- data_frame(heads = heads10k) %&gt;% # convert to a data frame group_by(heads) %&gt;% # group by number of possibilities summarise(n = n(), p=n/10000) # count occurances of possibility, ## Warning: `data_frame()` is deprecated, use `tibble()`. ## This warning is displayed once per session. # &amp; calculate probability (p) of # each We now have a discrete probability distribution of the number of heads in 10 coin flips. Use the View() function to have look at your data10k variable. You should now see for each heads outcome, the total number of occurrences in 10000 replications (n) plus the probability of that outcome (p). It might be useful to visualize the distribution as a histogram: ggplot(data10k, aes(heads,p)) + geom_col(fill = &quot;skyblue&quot;) + labs(x = &quot;Number of Heads&quot;, y = &quot;Probability of Heads in 10 flips (p)&quot;) + theme_bw() + scale_x_discrete(limits=0:10) Figure 4.1: Probability Distribution of Number of Heads in 10 Flips So in our analysis, the probability of getting 5 heads in 10 flips is 0.2486. But remember, do not be surprised if you get a slightly different value. Ten thousand replications is a lot but not a huge amount compared to infinity. If you run the analysis with more replications your numbers would become more stable, e.g. 100K. Note that as the possible number of heads in 10 flips are all related to one another, then summing up all the probabilities of the different number of heads will give you a total of 1. You can use this informaiton to start asking questions such as what would be the probability of obtaining 2 or less Heads in 10 flips? Well, if the probability of no heads in this distribution is 0.001, and the probability of 1 heads is 0.0097, and the probability of 2 heads is 0.0462, then the probability of 2 or less Heads in this distribution is simply the sum of these values: 0.0569 Quickfire Questions Look at the probability values corresponding to the number of coin flips you created in the data10k sample distribution (use View() to see this): Choose from the following options, if you wanted to calculate the probability of getting 4, 5 or 6 heads in 10 coin flips you would: multiply individual probabilities together sum individual probabilities together Choose from the following options, if you wanted to calculate the probability of getting 6 or more heads in 10 coin flips you would: multiply individual probabilities together sum individual probabilities together Choose from the following options, the distribution we have created is: continuous discrete Explain This - I don’t understand the answers! If you think about it, we can’t get 5.5 heads or 2.3 heads, we can only get whole numbers, 2 heads or 5 heads. This means that the data and the distribution is discrete. (Don’t be confused by one of the functions saying continuous) To find the probability of getting say 4, 5, or 6 heads in 10 coin flips, you are combining related scenarios together, therefore you need to find the individual probabilities of getting 4, 5 or 6 heads in 10 coin flips, then sum the probabilities together to get the appropriate probability of obtaining 4, 5 or 6 heads. It is the same with 6 or more heads, just sum the probabilities of 6, 7, 8, 9 and 10 heads to get the probability of 6 or more heads. Not sure if you should be summing or multiplying probabilities? A good way to remember, from both the coin flip examples and from the pack of cards examples earlier, is that if the scenarios are related you are summing probabilities, if scenarios are separate you are multiplying probabilities. Related scenarios are usually asking you about the probability of ‘either / or’ scenarios occuring, whereas separate scenarios usually ask about the probability of one scenario ‘and’ another scenario both occuring. Your sample distribution data10k has already completed the first part of this calculation for you (finding individual probabilities of n heads in 10 coin flips), so all you need to to is sum the required probabilities together! 4.2.3 The Binomial Distribution - Creating a Discrete Distribution Great, so we are now learning how probabilities and distributions work. However, if we had wanted to calculate the probability of 8 heads from 10 coin flips we don’t have to go through this entire procedure each time. Instead, because we have a dichotomous outcome, “heads or tails”, we can calculate probabilities using the binomial distribution - effectively what you just created. You can look up the R help page on the binomial distribution (type ?dbinom directly into the console) to understand how to use it but we will walk through some essentials here. We’ll use 3 functions to work with the binomial distribution: dbinom - the density function: gives you the probability of x successes (heads) given the size (trials) and probability of success prob on a single trial (here it’s 0.5, because we assume we’re flipping a fair coin) pbinom - the distribution function: gives you the probability of getting a number of successes below a certain cut-off point (e.g. probability of getting 0 to 5 heads out of 10 flips), given the size and the prob. qbinom - the quantile function: is the inverse ofpbinom in that it gives you the x axis value below (and including the value) which the summation of probabilities is greater than or equal to a given probability p, given the size and prob. 4.2.4 dbinom - The Density Function Using the dbinom function we can create probabilities for any possible outcomes of heads (e.g. 3) in 10 flips.: dbinom(3, 10, 0.5) or all possible outcomes of heads (0:10) in 10 flips: dbinom(0:10, 10, 0.5) And if we plot the probability of all possible outcomes in 10 flips it would look like this: Figure 4.2: Probability Distribution of Number of Heads in 10 Flips The dbinom (density binom) function takes the format of dbinom(x, size, prob), where the arguments we give are: x the number of ‘heads’ we want to know the probability of. Either a single one, 3 or a series 0:10. size the number of trials (flips) we are doing; in this case, 10 flips. prob the probability of ‘heads’ on one trial. Here chance is 50-50 which as a probability we state as 0.5 or .5 Now say if we wanted to know the probability of 6 heads out of 10 flips, we would write: dbinom(6, 10, 0.5) # we only have to change the first (x) argument ## [1] 0.2050781 As such the probability of 6 heads, using dbinom is p = 0.2050781. If you compare this value to the data10k value for 6 you will see they are similar but not quite the same because dbinom uses a lot more replications than the 10000 we used. In terms of visualising what we have just calculated. That number represents the height of the green bar in the plot below. Figure 4.3: Probability Distribution of Number of Heads in 10 Flips with the probability of 6 out of 10 Heads highlighted in green Quickfire Questions To three decimal places, what is the probability of 2 heads out of 10 flips? Explain This - I can’t get the right answer You want to know the probability of 2 heads in 10 flips. X is therefore 2; Size is therefore 10; and the probability stays the same at .5, meaning that the code would look like this: dbinom(2, 10, 0.5) = .04394531 or rounded = .044 4.2.5 pbinom - The Cumulative Probability Function What if we wanted to know the probability of up to and including 3 heads out of 10 flips? We can either use dbinom for each outcome up to 3 heads and sum the results: dbinom(0:3, 10, 0.5) %&gt;% sum() ## [1] 0.171875 Or we can use the pbinom function; known as the cumulative probability distribution function or the cumulative density function. The first argument we give is the cut-off value up to and including which we want to know the probability of (here it’s up to 3 heads). Then, as before, we tell it how many flips we want to do and the probability of heads on a single trial. Copy this line into your script and run it: pbinom(3, 10, 0.5, lower.tail = TRUE) ## [1] 0.171875 So the probability of up to and including 3 heads (as lower.tail = TRUE) out of 10 flips is 0.172. For visualization, what we have done is calculated the cumulative probability of the lower tail of the distribution up to our cut-off of 3 (in green below): Figure 4.4: Probability Distribution of Number of Heads in 10 Flips with the probability of 0 to 3 Heads highlighted in green - lower.tail = TRUE So the pbinom function gives us the cumulative probability of outcomes up to and including the cut-off. But what if we wanted to know the probability of outcomes including and above a certain value? Say we want to know the probability of 7 heads or more out of 10 coin flips. The code would be this: pbinom(6, 10, 0.5, lower.tail = FALSE) ## [1] 0.171875 We still specify our cut-off as 6 heads, even though we want 7 and above, but we switch the lower.tail call from TRUE to FALSE to tell pbinom we don’t want the lower tail this time (to the left of and including the cut-off), we want the upper tail, to the right of the cut-off. This results in the cumulative probability for the upper tail of the distribution down to our cut-off value (shown in green below). We set the cut-off as ‘6’ because in the discrete distribution, only the lower.tail = TRUE includes the cut-off (6 and below) whereas the lower.tail = FALSE would be everything above the cut-off but not including the cut-off (7 and above). Figure 4.5: Probability Distribution of Number of Heads in 10 Flips with the probability of 7 or more Heads highlighted in green - lower.tail = FALSE Portfolio Point - Am all in a Tail Spin! Lower TRUE or FALSE The most confusing part for people we find is the concept of lower.tail. If you look at a distribution, say the binomial, you find a lot of the high bars are in the middle of the distribution and the smaller bars are at the far left and right of the distribution. Well the far left and right of the distribution is called the tail of the distribution - they tend to be an extremity of the distribution that taper off like a…..well like a tail. A lot of the time we will talk of left and right tails but here in the pbinom function it only ever considers the data in relation to the left side of the data - this is what it calls the lower.tail. Let’s consider lower.tail = TRUE, this is the default, so if you dont state lower.tail then this is what is considered to be what you want. lower.tail = TRUE means all the values to the left of your value including the value you state. So on the binomial distribution if you say give me the probability of 5 heads or less, then you would set lower.tail = TRUE and you would be counting and summing the probability of 0, 1, 2, 3, 4 and 5 heads. You can check this with dbinom(0:5, 10, .5) %&gt;% sum(). However, if you say give me the probability of 7 or more heads, then you need to do lower.tail = FALSE, to consider the right-hand side tail, but also, you need to set the code as pbinom(6, 10, .5, lower.tail = FALSE). Why 6 and not 7? Because the pbinom function, when lower.tail = FALSE, starts at the value plus one to the value you state; it always considers the value you state as being part of the lower.tail so if you say 6, it includes 6 in the lower.tail and then gives you 7, 8, 9 and 10 as the upper tail. If you said 7 with lower.tail = FALSE, then it would only give you 8, 9 and 10. This is tricky but worth keeping in mind when you are using the pbinom function. And remember, you can always check it by using dbinom(7:10, 10, .5) %&gt;% sum() and seeing whether it matches pbinom(6, 10, 0.5, lower.tail=FALSE) or pbinom(7, 10, 0.5, lower.tail=FALSE) Quickfire Questions Using the format of the above pbinom function, enter the code that would determine the probability of up to and including 5 heads out of 20 flips, assuming a probability of 0.5: To two decimal places, what is the probability of obtaining more than but not including 50 heads in 100 flips? Helpful Hint You are looking to calcuate the probability of 5 or less heads (x) out of 20 flips (size), with the probability of ‘heads’ in one trial (prob) remaining the same. Do you need the lower.tail call here if you are calculating the cumulative probability of the lower tail of the distribution? You are looking to calculate the probability of 51 or more heads (x), out of 100 flips (size), with the probability of ‘heads’ in one trial (prob) remaining the same. Do you need the lower.tail call here if you are calculating the cumulative probability of the upper tail of the distribution? Remember, because you are not looking at the lower.tail, the value of heads that you enter in pbinom will not be included in the final calculation, e.g. entering pbinom(3, 100, lower.tail = FALSE) will give you the probability for 4 and above heads. If you were instead looking at the lower.tail, entering pbinom(3, 100, lower.tail = TRUE) would give you the probability of 3 and below heads. Explain This - I can’t get these answers The code for the first one would be: pbinom(5, 20, 0.5) or pbinom(5, 20, 0.5, lower.tail = TRUE) The code for the second one would be: pbinom(50, 100, 0.5, lower.tail = FALSE), giving an answer of .46. Remember you can confirm this with: dbinom(51:100, 100, 0.5) %&gt;% sum() 4.2.6 qbinom - The Quantile Function The qbinom() function is the inverse of the pbinom() function. Whereas with pbinom() you supply an outcome value x and get a tail probability, with qbinom() you supply a tail probability and get the outcome value that (approximately) cuts off that tail probability. It is approximate with a discrete distribution because of the “jumps” in probability between the discrete outcomes. Let’s see how these functions are inverses of one another: p1 &lt;- pbinom(49, 100, .5) # probability of 0 to 49 heads in 100 coin flips p1 ## [1] 0.4602054 This tells us that the probability is p = 0.4602054. If we now put that probability (stored in p1) into qbinom we get back to where we started. E.g. qbinom(p1, 100, .5) ## [1] 49 Put the above another way, the qbinom function is useful if you want to know the minimum number of successes (‘heads’) that would be needed to achieve a particular probability. This time the cut-off we specify is not the number of ‘heads’ we want but the probability we want. For example say we want to know the minimum number of ‘heads’ out of 10 flips that would result in a 5% heads success rate (a probability of .05), we would use the following code. qbinom(.05, 10, 0.5) ## [1] 2 Because we are dealing with the lower tail, this is telling us for a lower tail probability of .05, we would expect at most 2 heads in 10 flips. However, it is important to note that for a discrete distribution, this value is not exact. We can see this by: pbinom(2, 10, .5) ## [1] 0.0546875 which is not exactly .05. This is because the data is discrete and so qbinom() gives us the closest category to the boundary. Portfolio Point - I don’t understand qbinom arguments We found a lot of students asking about qbinom and how it works when you are inputting two different probabilities as the arguments for qbinom. Let us try and make things clearer. First, it is useful to remember that it is the inverse of pbinom(): pbinom() gives you a tail probability associated with x, and qbinom() gives you the closest x that cuts off the specified tail probability. The function is set up as qbinom(p, size, prob). First of all, you have used prob in the previous two functions, dbinom and pbinom, and it represents the probability of success on a single trial (here it is the probability of ‘heads’ in one coin flip, .5). Now, prob represents the probability of success in one trial, whereas p represents the tail probability you desire. The function gives you the value of x that would yield that probability. So you give qbinom() a tail probability of .05, in 10 flips, when the probability of a success on any one flip is .5. And it tells you the answer is 2, meaning that getting at most 2 flips on 10 trials has a probability of roughly .05. qbinom also uses the lower.tail argument and it works in a similar fashion to pbinom. Quickfire Questions Type in the box, the maximum number of heads associated with a tail probability of 10% (.1) in 17 flips: Explain This - I can’t get the answer The answer would be 6 because the code would be: qbinom(0.1, 17, 0.5, lower.tail = TRUE) Remember that you want an overall probability of 10% (p = .1), you have 17 flips in each go (size = 17), and the probability of heads on any one flip is .5 (prob = .5). And you want the maximum number for the lower tail, so the lower.tail is TRUE. Job Done - Activity Complete! Excellent! We have now recapped some general probability concepts as well as going over the binomial distribution again. We will focus on the normal distribution in the lab but it would help to read the brief section on it which can be found on Moodle. Keep in mind that what you are trying to get an understanding of is that every value in a distribution has a probability of existing in that distribution. That probability may be very large, meaning that it is from the middle of the distribution, or that probability might be rather low, meaning it is from the tail, but ultimately every value of a distribution has a probability. If you have any questions please post them on the Moodle forum or the slack forum rguppies.slack.com under the channel #level2_2018. Finally, don’t forget to add any useful information to your Portfolio before you leave it too long and forget. That is all for now. See you in the lab! 4.3 PreClass Activity 2 4.3.1 Continuous Data - Brief Recap on The Normal Distribution This a very short recap on the normal distribution which will help you in the lab if you read it before arriving. In the main PreClass activity, we have seen how we can use a distribution to estimate probabilities and determine cut-off values (these will play an important part in hypothesis testing in later labs!), but we have looked only at the discrete binomial distribution. Many of the variables we will encounter will be continuous and tend to show a normal distribution (e.g. height, weight, IQ). Let’s say we’re interested in the height of the population of level 2 psychology students, which we estimate to be between 146cm and 194cm. If we plotted this as a continuous, normal distribution, it will look like: Figure 4.6: The Normal Distribution The figure shows the hypothetical probability density of heights ranging from 146cm to 194cm in the population of level 2 psychology students (black curve). This data is normally distributed - we know this as it has the following properties: 4.3.2 Properties of the Normal distribution: 1. The distribution is defined by its mean and standard deviation: The mean (\\(\\mu\\)) describes the center, and therefore peak density, of the distribution. This is where the largest number of the people in the population will be in terms of height. The standard deviation (\\(\\sigma\\)) describes how much variation there is from the mean of the distribution - on the figure, the standard deviation is the distance from the mean to the inflection point of the curve (the part where the curve changes from a upside-down bowl shape to a right-side-up bowl shape). 2. Distribution is symmetrical around the mean: The mean lies in the middle of the distribution and divides the area under the curve into two equal sections - so we get the typical bell-shaped curve. 3. Total area under the curve is equal to 1: If we were to add up the probabilities (densities) for every possible height, we would end up with a value of 1. 4. The mean, median and mode are all equal: A good way to check if a given dataset is normally distributed is to calculate each measure of central tendency and see if they are approximately the same (normal distribution) or not (skewed distribution). 5. The curve approaches, but never touches, the x axis: You will never have a probability of 0 for a given x axis value. 6. The normal distribution follows the Empirical Rule: The Empirical Rule states that 99.7% of the data within the normal distribution falls within three standard deviations (\\(\\pm3\\sigma\\)) from the mean, 95% falls within two standard deviations (\\(\\pm2\\sigma\\)), and 68% falls within one standard deviation (\\(\\pm\\sigma\\)). Job Done - Activity Complete! We will look at the normal distribution more in the lab. If you have any questions please post them on the Moodle forum or the slack forum rguppies.slack.com under the channel #level2_2018. Finally, don’t forget to add any useful information to your Portfolio before you leave it too long and forget. That is all for now. See you in the lab! 4.4 InClass Activity 1 Today we are going to continue our study of distributions and probability to help gain an understanding of how we can end up with an inference about a population from a sample. In the preclass activity, we focused a lot on basic probability and binomial distributions. In the lab today, we will focus more on the normal distribution; one of the key distributions in Psychology. It will be really beneficial to you when it comes to doing the assignment to have run through both the preclass and inclass activities. The assignment for this lab is formative and should not be submitted. This of course does not mean that you should not do it. As will all the formative labs, completing them and understanding them, will benefit you when it comes to completing the summative labs. You may also want to refer to the Lecture series for help. If you are unclear at any stage please do ask; probability is challenging to grasp at first. Portfolio Point - Always be adding to your knowledge in your own words Remember to add useful information to you Portfoilio! One of the main reasons we do this is because there is a wealth of research that says the more interactive you are with your learning, the deeper your understanding of the topic will be. This relates to the Craik and Lockhart (1972) levels of processing model you will hear about in lectures. Always make notes about what you are learning to really get an understanding of the concepts. And, most importantly, in your own words! Craik, F. I. M., &amp; Lockhart, R. S. (1972). Levels of processing: A framework for memory research. Journal of Verbal Learning and Verbal behavior, 11, 671-684. 4.4.1 Continuous Data and the Normal Distribution Many of the variables we will encounter in Psychology will: be continuous as opposed to discrete. tend to show a normal distribution. look similar to below - the bell-shaped curve - when plotted. But can you name any? Take a couple of minutes as a group to think of variables that we might encounter that are normally distributed. Figure 4.6: The Normal Distribution 4.4.2 Estimating from the Normal Distribution We won’t ask you to create a normal distribution as it is more complicated than the binomial distribution you estimated in the preclass. Unlike coin flips, the outcome in the normal distribution is not just 50/50. Instead, just as with the binomial distribution (and other distributions) there are functions that allow us to estimate the normal distribution and to ask questions about the distribution. These are: dnorm() - the density function pnorm() - the probability or distribution function qnorm() - the quantile function They work in a similar way to their binomial counterparts. If you are unsure about how a function works you can call the help on it by typing in the console, for example, ?dnorm or ?dnorm(), but the brackets aren’t essential for the help. Quickfire Questions Type in the box the binomial counterpart to dnorm()? Type in the box the binomial counterpart to pnorm()? Type in the box the binomial counterpart to qnorm()? Explain This - I don’t get the answers The counterpart functions all start with the same letter, d, p, q, it is just the distribution name that changes, binom, norm, t - though we haven’t quite come across the t-distribution yet. dbinom() is the binomial equivalent to dnorm() pbinom() is the binomial equivalent to pnorm() qbinom() is the binomial equivalent to qnorm() There is also rnorm() and rbinom() but we will look at them another time. 4.4.3 dnorm() - The Density Function for the Normal Distribution Using dnorm, like we did with dbinom, we can plot a normal distribution. This time however we need: x, a vector of quantiles (in other words, values on the x axis) the mean of our data and standard deviation sd of our data. We will use IQ as an example. Many Psychologists are interested in studying IQ, perhaps in terms of heritability, or interested in controlling for IQ in their own studies to rule out any effect (e.g. clinical and autism studies). 4.4.3.1 Task 1: Standard Deviations and IQ Score Distribution Copy the below code into a new script and run it. Remember that you will need to call tidyverse to your library first. e.g. library(&quot;tidyverse&quot;). This code creates the below plot showing a normal distribution of IQ scores (M = 100, SD = 15) ranging from 40 to 160. These are values considered typical for the general population. # First we set up our range of values from 40 to 160 IQ_data &lt;- tibble(IQ_range = c(40, 160)) # Then we plot the distribution of IQ_data, where we have M = 100 and SD = 15 ggplot(IQ_data, aes(IQ_range)) + stat_function(fun = dnorm, args = list(mean = 100, sd = 15)) + labs(x = &quot;IQ Score&quot;, y = &quot;probability&quot;) + theme_classic() Figure 4.7: Distribution of IQ scores Which part of the code do you need to change to alter the SD of your plot? mean = 100 sd = 15 (40, 160) Now copy and edit the above code to plot a distribution with mean = 100 and sd = 10, and compare the first plot to your own plot. Group Discussion Point What does changing the sd do to the shape of the distribution? Spend a few minutes editing the code and discussing with your group to answer the following questions: What happens to the shape of the distribution if you change the sd from 10 to 20? the distribution gets narrower the distribution gets wider What happens to the shape of the distribution if you change the sd from 10 to 5? the distribution gets narrower the distribution gets wider What does a small or large standard deviation in your sample tell you about the data you have collected? Explain This - I don’t get Standard Deviations! Changing the SD from 10 to 20 means a larger standard deviation so you will have a wider distribution. Changing the SD from 10 to 5 means a smaller standard deviation so you will have a narrower distribution. Smaller SD results in a narrower distribution meaning that the data is less spread out; larger SD results in a wider distribution meaning the data is more spread out. A note on the Standard Deviation: You will know from your lectures that you can estimate data in two ways: point-estimates and spread estimates. The mean is a point-estimate and condenses all your data down into one data point - it tells you the average value of all your data but tells you nothing about how spread out the data is. The standard deviation however is a spread estimate and gives you an estimate of how spread out your data is from the mean - it is a measure of the standard deviation from the mean. So imagine we are looking at IQ scores and you test 100 people and get a mean of 100 and an SD of 5. This means that the vast majority of your sample will have an IQ around 100 - probably most will fall within 1 SD of the mean, meaning that most of your participants will have an IQ of between 95 and 105. Now if you test again and find a mean of 100 and an SD of 20, this means your data is much more spread out. If you take the 1 SD approach again then most of your participants will have an IQ of between 80 and 120. So one sample has a very tight range of IQs and the other sample has a very wide range of IQs. All in, from the point-estimate and spread estimate of your data you can tell the shape of your sample distribution. So far so good! But in the above example we told dnorm() the values at the limit of our range and it did the rest; we said give us a range of 40 to 160 IQ scores. However, we could plot it another way by telling dnorm() the sequence, or range, of values we want and how much precision we want between them. 4.4.3.2 Task 2: Changing Range and Step Size of The Normal Distribution Copy the code below in to your script and run it. Here we plot the standard Normal Distribution from -4 to 4 in steps of 0.01. We have also stated a mean of 0 and an sd of 1. ND_data &lt;- tibble(ND_range = seq(-4, 4, 0.01)) ggplot(ND_data, aes(ND_range)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 1)) + labs(x = &quot;SD units&quot;, y = &quot;probability&quot;, title = &quot;The Normal Distribution&quot;) + theme_classic() Figure 4.8: The Normal Distribution with Mean = 0 and SD = 1 Quickfire Questions Fill in the box to show what you would type to get a range and step size of -10 to 10 in steps of .05: ND_data &lt;- Now that you know what to change, plot the normal distribution with the range of -10 to 10, in steps of 0.05, with a mean of 0, and a standard deviation of 1. Compare your new plot the the original one we created. What change is there in the distribution? Distribution widens No change in distribution Distribution narrows Explain This - I don’t understand the answer! To change the distribution you would write: ND_data &lt;- tibble(ND_range = seq(-10, 10, 0.05)) However, when comparing the plots, whilst the plot itself may look thinner, the distribution has not changed. The change in appearance is due to the range of sd values which have been extended from -4 and 4 to -10 and 10. The density of values within those values has not changed however and you will see, more clearly in the second plot, that values beyond -3 and 3 are very unlikely. 4.4.4 pnorm() - The Probability or Distribution Function There is one important difference you should note between continuous and discrete probability distributions. With discrete probability distributions, there are usually a finite number of outcomes over which you take a probability. For instance, with 4 coin flips, there are 5 possible outcomes for the number of heads: 0, 1, 2, 3, 4, 5. We can use dbinom() to get the exact probability for each. However, with a truly continuous variable, the number of possible outcomes are infinite, because you not only have 0.01 but also .0000001 and .00000000001 to arbitrary levels of precision. So rather than asking for the probability of a single value, we ask the probability for a range of values, which is equal to the area under the curve for that range. Just as dnorm() works like dbinom(), pnorm() works just like pbinom(). So, pnorm(), given the mean and sd of our data, returns the cumulative density function (cumulative probability) that a given probability (p) lies at a specified cut-off point and below, unless of course lower.tail = FALSE is specified in which case it is from the cut-off point and above. We will use height to give a concrete example. Say that we test a sample of students (M = 170cm, SD = 7). If we want to calculate the probability that a given student is 150cm or shorter we would do the following: # lower.tail = TRUE means lower than and including the value of X # TRUE is the default so we don&#39;t actually need to declare it pnorm(150, 170, 7, lower.tail = TRUE) This tells us that finding the probability of someone 150cm or shorter in our class is about p = 0.0021374; stated differently, we would expect 0.21% of students would be 150cm or shorter. This is a very small probability and suggests that it is pretty unlikely to find someone shorter than 150cm in our class. This is mainly because of how small the standard deviation of our distribution is. Think back to what we said earlier about narrow standard deviations round the mean! 4.4.4.1 Task 3: Calculating Cumulative Probability of Height Edit the pnorm code above to calculate the probability that a given student is 190cm or taller. To three decimal places, as in Task 3, what is the probability of a student being 190cm or taller in this class? Explain This - I don’t understand the answer! The answer is .002. See the solution code at the bottom of the page to see how to complete Task 3. There is a difference in where you need to specify the cut-off point in the pbinom (discussed in the preclass activity) and pnorm functions for values above x, i.e. when lower.tail = FALSE. If you had discrete data, say the number of coin flips that result in heads, and wanted to calculate the probability above x, you would apply pbinom and have to specify your cut-off point as x-1 to include x in your calculation. For example, to calculate the probability of 4 or more ‘heads’ occuring in 10 coin flips, you would specify pbinom(3, 10, 0.5, lower.tail = FALSE) as lower.tail includes the value you state. For continuous data, however, such as height, you would be applying pnorm and therefore can specify your cut-off point simply as x. In the above example, for the cut-off point of 190, a mean of 170 and standard deviation of 7, you can write pnorm(190, 170, 7, lower.tail = FALSE). The way to think about this is that setting x as 189 on a continuous scale, when you only want all values greater than 190, would also include all the possible values between 189 and 190. Setting x at 190 starts it at 190.0000000…001. This is a tricky difference between pbinom and pnorm to recall easily, so best include this explanation point in your portfolio to help you carry out the correct analyses in the future! 4.4.4.2 Task 4: Using Figures to Calculate Cumulative Probability Have a look at the distribution below: Figure 4.9: The Normal Distribution of Height with the probability of people of 185cm highlighted in purple Using the information in the figure, and the mean and SD as above, calculate the probability associated with the shaded area. Helpful Hint You already have your mean and standard deviations to input in pnorm, look at the shaded area to obtain your cut-off point. What should the lower.tail call be set to according to the shaded area? Quickfire Question To three decimal places, what is the cumulative probability of the shaded area in Task 4? Explain This - I don’t get this answer The answer should be .016. See the solution code at the end of the Chapter for Task 4. lower.tail is set to FALSE as you want the area to the right. 4.4.5 qnorm() - The Quantile Function Using qnorm() we can do the inverse of pnorm(), and instead of finding out the cumulative probability from a given set of probabilities, we can find a cut-off value given a desired probability. For example, what is the maximum IQ a student would have if they were in the bottom 10% of the IQ distribution (M = 100 &amp; SD = 15)? qnorm(0.1, 100, 15) # note that we need to convert 10% to a probability (0.1). So anyone with an IQ of 80.8 or lower would be in the bottom 10%. To recap, we have calculated the inverse cumulative density function (or inverse of the cumulative probability) of the lower tail of the distribution, with a cut-off of a probability of 0.1 (10%), illustrated in purple below: Figure 4.10: The Normal Distribution of Height with the bottom 10% of heights highlighted in purple 4.4.5.1 Task 5: Using pnorm and qnorm to find probability and cut-off Values Calculate the lowest IQ score a student must have to be in the top 5% of the above distribution. More challenging: Using the appropriate normal distribution function, calculate the probability that a given student will have an IQ between 105 and 110, on a normal distribution of mean = 100, sd = 15. Helpful Hint Part 1: Remember to include the lower.tail call if required! If you are unsure, visualise what you are trying to find (i.e. the lowest IQ score you can have to be in top 5%) by sketching it out on a normal distribution curve. It may help to reverse the question to sound more like the previous example. Part 2: For the second part, each function, not necessarily qnorm, gives one value, so you are looking to do separate calculations for each IQ. Then you have to combine these two values, but are you summing or subtracting them? Is it more or less likely for students to have an IQ that falls between this range than above or below a cut-off? Quickfire Questions To one decimal place, enter your answer for Task 5 part 1: What is the lowest IQ score a student must have to be in the top 5% of the distribution? To two decimal places, enter your answer for Task 5 part 2: What is the probability that a student will have an IQ between 105 and 110, on a normal distribution of mean = 100, sd = 15? Explain This - I dont get this answer The question can be rephrased as what value would give you 95% of the distribution - and the answer would be 124.7. See the solution code for Task 5 at the bottom of the page. You could use pnorm() to establish the probability of an IQ of 110. And you could use pnorm() again to establish the probability of an IQ of 105. The answer is the difference between these two probabilities and should be .12. So a 12% probability. See the solution code for Task 5 at the bottom of the page. Job Done - Activity Complete! You have now recapped probability and the binomial and normal distributions. You should now be ready to complete the Homework Assignment for this lab. The assignment for this Lab is FORMATIVE and is NOT to be submitted and will NOT count towards the overall grade for this module. However you are strongly encouraged to do the assignment as it will continue to boost your skills which you will need in future assignments. If you have any questions, please post them on the slack forum under the channel #level2_2018. Finally, don’t forget to add any useful information to your Portfolio before you leave it too long and forget. Excellent work! You are Probably an expert! Now go try the assignment! 4.5 InClass Activity 2 (Additional) In the preclass activity, we focused a lot on basic probability and binomial distributions. If you have followed it, and understood it correctly, you should be able to have a go at the following scenario to help deepen your understanding of the binomial distribution. Have a go and see how you get on. Solutions are at the bottom of the page. 4.5.1 Return of the Binomial You design a psychology experiment where you are interested in how quickly people spot faces. Using a standard task you show images of faces, houses, and noise patterns, and ask participants to respond to each image by saying ‘face’ or ‘not face’. You set the experiment so that, across the whole experiment, the number of images per stimuli type are evenly split (1/3 per type) but they are not necessarily evenly split in any one block. Each block contains 60 trials. To test your experiment you run 1 block of your experiment and get concerned that you only saw 10 face images. As this is quite a low number out of a total of 60 you think something might be wrong. You decide to create a probability distribution for a given block of your experiment to test the likelihood of seeing a face (coded as 1) versus not seeing a face (coded as 0). You start off with the code below but it is incomplete. blocks_5k &lt;- replicate(n = NULL, sample(0:1, size = NULL, replace = NULL, c(2/3,1/3)) %&gt;% sum()) Copy the code to a script and replace the NULLs with the correct value and statement to establish how many different numbers of faces you might see in 1 block of 60 trials, over 5000 replications. Answering the following questions to help you out: Quickfire Questions The n NULL relates to: Number of Monte Carlo replications Number of blocks in the experiment Number of trials in the experiment Number of faces in the experiment The size NULL relates to: Number of faces in a block Number of houses in a block Number of trials in a block Number of noise patterns in a block replace should be set to: TRUE FALSE Explain This - I don’t understand these answers replicate is how many times you want to run the sample, n = 5000. The number of trials is the size, 60. And in order for the code to work, and to not run out of options, you need to set replace as TRUE. You saw this exact same code in the preclass so be sure to have had a look there. The solution code is shown in one of the code chunks below. Portfolio Point - Why the odd weight of probability? Note that the elements in the prob argument set probabilities for the two outcomes specified in the first argument (0, 1). So, we are setting a 2/3 probability of 0 (no face), and a 1/3 probability of 1 (face). Why? Remember that in this instance you have three different stimuli (face, house and noise) and therefore have a greater chance of not seeing a face in the experiment (i.e. you see a house or noise) than seeing a face in the experiment. You need to remember when using sample (?sample) to set the probability appropriately or you will end up with an incorrect calculation. 2. Your blocks_5k code has created 5000 values where each value represents the number of faces you ‘saw’ in each individual block of 60 trials - like the number of heads you see in 10 flips. If your code has worked, then running the chunk below should produce a probability distribution of your data. Run it and see. If not, something has gone wrong; debug or compare to the solution at the bottom of the page. data5k &lt;- tibble(faces_per_block = blocks_5k) %&gt;% count(faces_per_block) %&gt;% mutate(p = n / 5000) ggplot(data5k, aes(faces_per_block, p)) + geom_col() + theme_light() If you view data5k, you will now see three columns: faces_per_block tells you all the number of face trials seen in a block of 60 trials. Note that it does not run from 0 to 60. n tells you the number of blocks in which the corresponding number of face trials appeared, e.g. in 216 blocks there were 15 face trials. (Remember this is sampled data so your values may be different!). p tells you the probability of the corresponding number of faces_per_block. From your data5k tibble, what was the probability of seeing only 10 faces in a block? Ours was 0.002 but remember that we are using a limited number of replications here so our numbers will vary from yours. To find this value you can use View() or glimpse() on data5k. Group Discussion Point Discuss in your groups the following questions: If the probability of seeing only 10 faces was p = 0.002, what does that say about whether our experiment is coded correctly or not? We only have data for 26 potential different number of face trials within a block. Why does this value not stretch from 0 to 60, or at least have more potential values than 26? Portfolio Point - What does it say about your experimental design? On the first point, it says that the probability of having a block with only 10 faces is pretty unlikely - it has only a small probability. However, it is still possible. Does that say there is an error in your code? Not necessarily as it is possible you just happened to get a block with only 10 faces. Despite it being a low probability it is not impossible. What it might say, however, is that if this is something that concerns you, this potentially low number of faces in a block, then you need to think about the overall design of your experiment and consider having an equal weighting of stimuli in each block. On the second point, you need to think about the width of the distribution. The probability of a face is 1/3 or p = .33. If there are 60 trials per block then that works out, on average, at approx 20 faces per block . The further you move away from that average, 20, the less probable it is to see that number of faces. E.g. 10 faces had a very low probability as will 30 faces. 40 faces will have almost no probability whatsoever. In only 5000 trials not all values will be represented. Quickfire Questions Using the appropriate binom() function, to three decimal places, type in the actual probability of exactly 10 faces in 60 trials: Using the appropriate binom() function, to three decimal places, type in the actual probability of seeing more than, but not including, 25 faces in a block of 60 trials? Using the appropriate binom() function, enter the number of faces needed in a block of 60 trials that would cut off a lower tail probability of .05. Helpful Hint You are looking for the probability of one specific value of faces appearing, i.e. 10 faces in a total of 60 trials. You are not looking for the cumulative probability or a range of values. What does this tell you about the apporpriate binom() function you require? Also, in the sample function, we created a vector of probability dependent on the appearance of both faces and other stimuli. For the binom() function, you need to include only the probability of faces appearing. You are looking for the cumulative probability for seeing more than but not including 25 faces in a block of 60 trials. What does this tell you about the apporpriate binom() function you require? Remember to set an appropriate call for lower.tail! The probability level has been provided for you (5% 0r .05). With this you are looking to find the cutoff value for this tail. What does this tell you about the appropriate binom() function you require? If you are looking for the probability of values at or below this cutoff, is lower.tail set to TRUE or FALSE? Explain This - I don’t get these answers The answer would be .002 as you require the density, dbinom(), of 10 faces out of a possible 60 given a probability of 1/3. See the Binomial Quickfire Questions solution below for the code. Code: dbinom(10, 60, 1/3) The answer would be .068 as you require the cumulative probability, pbinom(), of more than 25 faces out of 60 given a probability of 1/3. As it is more than 25, setting 25 with lower.tail set as FALSE will give you 26 and above (remember this is a binomial distribution, so where you set your cut-off is very important). See the Binomial Quickfire Questions solution below for the code. Code: pbinom(25, 60, 1/3, lower.tail = FALSE) The answer would be 14 using the qbinom() function based on wanting to maintain 5% probability of faces in 60 trials. See the Binomial Quickfire Questions solution below for the code. Code: qbinom(.05, 60, 1/3) Job Done - Activity Complete! Well done on completing this activty! Once you have completed the main body inclass assignment on the Normal Distribution, you should be ready to complete the assignment for this lab. Good luck and remember to refer back to the codes and questions in this lab and the preclass if you get stuck!. 4.6 InClass Activity 3 (Additional) One thing we looked at in the lecture and in the preclass activity were some rather “basic” ideas about probability using cards. Here is a couple of very quick questions using dice (or die if you prefer) to make sure you understand this. Have a go and see how you get on. If you don’t get the right answers, be sure to ask in the lab or post on the slack forum rguppies.slack.com under the channel #level2_2018. Finally, don’t forget to add any useful information to your Portfolio before you leave it too long and forget. 4.6.1 Back to Basics Show your answer either as a fraction (e.g. 1/52) or to 3 decimal places (e.g. 0.019). What is the probability of rolling a 4 on a single roll of a six-sided dice? What is the probability of rolling a 4, 5 or 6 on a single roll? What is the probability of rolling a 4 and then a 6 straight after? Explain this - I dont get these answers Remember: Probability is the number of times or ways an event can occur divided by the total number of all possible outcomes. When you have two separate events, the probability of them both happening is the joint probability which is calculated by mutliplying all the individual probabilities together. .167 as you have 1 outcome over 6 possibilities .5 as you have 3 outcomes over 6 possibilities .028 as you have 1 out of 6 on both occasions so the joint probability is used. Job Done - Activity Complete! Well done on completing this activity! Once you have completed the main body inclass assignment on the Normal Distribution, you should be ready to complete the assignment for this lab. Good luck and remember to refer back to the codes and questions in this lab and the preclass if you get stuck!. 4.7 Assignment This is a formative assignment but we strongly encourage you to do the assignment as the knowledge gained from the practical activities in this lab will be super important to your future-self! Lab 4: Formative Probability Assignment! In order to complete this assignment, you first have to download the assignment .Rmd file which you need to edit - titled GUID_Level2_Lab4.Rmd. This can be downloaded within a zip file from the link below. Once downloaded and unzipped you should create a new folder that you will use as your working directory; put the .Rmd file in that folder and set your working directory to that folder through the drop-down menus at the top. Download the Assignment .zip file from here. Now open the assignment .Rmd file within RStudio. You will see there is a code chunk for each of the 10 tasks. Much as you did in the previous assignments, follow the instructions on what to edit in each code chunk. This will often be entering code or a single value and will be based on the skills learnt in the current lab as well as all previous labs. 4.7.1 Today’s Topic - Probability In the lab you recapped and expanded on your understanding of probability, including a number of binom and norm functions as well as some more basic ideas on probability. You will need these skills to complete the following assignment so please make sure you have carried out the PreClass and InClass activities before attempting this formative assignment. Remember to follow the instructions and if you get stuck at any point to questions on the forums. Before starting let’s check The .Rmd file is saved into a folder on your computer that you have access to and you have manually set this folder as your working directory. For assessments we ask that you save it with the format GUID_Level2_Lab4.Rmd where GUID is replaced with your GUID. Though this is a formative assessment it may be good practice to do the same here. Note: You should complete the code chunks below by replacing the NULL (except the library chunk where the appropriate code should just be entered). Not all answers require coding. On those that do not require code, you can enter the answer as either code, mathematical notation, or an actual single value. Pay attention to the number of decimal places required. 4.7.2 Load in the Library There is a good chance that you will need the tidyverse library at some point in this exercise so load it in the code chunk below: # hint: something to do with library() and tidyverse Basic probability and the binomial distribution questions Background Information: You are conducting an auditory discrimination experiment in which participants have to listen to a series of sounds and determine whether the sound was human or not. On each trial participants hear one brief sound (100 ms) and must report whether the sound was from a human (coded as 1) or not (coded as 0). The sounds were either: a person, an animal, a vehicle, or a tone, with each type of sound equally likely to appear. 4.7.3 Assignment Task 1 On a single trial, what is the probability that the sound will not be a person? Replace the NULL in the t1 code chunk with either mathematical notation or a single value. If entering a single value, give your answer to two decimal places. t1 &lt;- NULL 4.7.4 Assignment Task 2 Over a sequence of 4 trials, with one trial for each sound, and sampled with replacement, what is the probability of the following sequence of sounds: animal, animal, vehicle, tone? Replace the NULL in the t2 code chunk with either mathematical notation or a single value. If entering a single value, give your answer to three decimal places. t2 &lt;- NULL 4.7.5 Assignment Task 3 Over a sequence of four trials, with one trial for each sound, without replacment, what is the probability of the following sequence of sounds: person, tone, animal, person? Replace the NULL in the t3 code chunk with either mathematical notation or a single value. t3 &lt;- NULL 4.7.6 Assignment Task 4 Replace the NULL below with code, using the appropriate binomial distribution function, to determine the probability of hearing exactly 17 ‘tone’ trials in a sequence of 100 trials. Assume the probability of a tone on any single trial is 1 in 4. Store the output in t4. t4 &lt;- NULL 4.7.7 Assignment Task 5 Replace the NULL below with code using the appropriate binomial distribution function to determine what is the probability of hearing 30 ‘vehicle’ trials or more in a sequence of 100 trials. Assume the probability of a vehicle trial on any one trial is 1 in 4. Store the output in t5. Hint: do we want the upper or lower tails of the distribution? t5 &lt;- NULL 4.7.8 Assignment Task 6 If a block of our experiment contained 100 trials, enter a line of code to run 10000 replications of one block, summing how many living sounds were heard in each replication. Code 1 for living sounds (person/animal) and 0 for non living sounds (vehicle/tone) and assume the probability of a living sound on any given trial is \\(p = .5\\). t6 &lt;- NULL Normal Distribution Questions Previously, in Lab 2, we looked at an ageing research project investigating differences in visual processing speed between younger (M = 22 years) and older adults (M = 71 years). One check in this experiment, prior to further analysis, is to make sure that older participants do not show signs of mild cognitive impairment (early symptoms of Alzheimer’s disease). To do this, we carry out a battery of cognitive tests to screen for such symptoms. One of the tests is the D2 test of attention which is a target cancellation task (i.e. participants cross out all letter d’s with two dashes from a line of letters - see Lab 2 for an example). It is designed to test peoples’ selective and sustained attention and visual scanning speed. The results of the test give a single score of Concentration Performance for each participant. The key piece of information for this analysis is that the distributions of D2 test scores are typically normally distributed (M = 100, SD = 10). 4.7.9 Assignment Task 7 Replace the NULL below with code using the appropriate function to determine the probability that a given participant will have a D2 score of 90 or lower? Store the output in t7 t7 &lt;- NULL 4.7.10 Assignment Task 8 Replace the NULL below with code using the appropriate function to determine the probability that a given participant will have a D2 score of 120 or more? Store the output in t8 t8 &lt;- NULL 4.7.11 Assignment Task 9 Replace the NULL below with code using the appropriate function(s) to determine what is the difference in scores that cut off the top 5% and bottom 5% of the distribution? Store the output in t9. t9 &lt;- NULL 4.7.12 Assignment Task 10 Finally, if a participant says to you that they are worried because they have heard that their Concentration Performance was in the bottom 2% of scores on the distribution, what is the maximum D2 score that they can have? Replace the NULL below with a single value to two decimal places. Do not enter code. Store this in t10 t10 &lt;- NULL Job Done - Activity Complete! Well done, you are finshed! Now you should go check your answers against the solution file which can be found on Moodle. You are looking to check that the resulting output from the answers that you have submitted are exactly the same as the output in the solution - for example, remember that a single value is not the same as a coded answer. Where there are alternative answers, it means that you could have submitted any one of the options as they should all return the same answer. If you have any questions, please post them on the forums! 4.8 Solutions to Questions Below you will find the solutions to the questions for the Activities for this chapter. Only look at them after giving the questions a good try and speaking to the tutor about any issues. 4.8.1 InClass Activities 4.8.1.1 InClass Task 1 ggplot(IQ_data, aes(IQ_data)) + stat_function(fun = dnorm, args = list(mean = 100, sd = 10)) + labs(x = &quot;IQ Score&quot;, y = &quot;probability&quot;, title = &quot;Distribution of IQ Scores&quot;) + theme_classic() Return to Task 4.8.1.2 InClass Task 2 ND_data &lt;- tibble(ND_range = seq(-10,10,0.05)) ggplot(ND_data, aes(ND_range)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 1)) + labs(x = &quot;SD units&quot;, y = &quot;probability&quot;, title = &quot;The Normal Distribution&quot;) + theme_classic() Return to Task 4.8.1.3 InClass Task 3 # Key thing is to set lower.tail to FALSE to calculate the above area. pnorm(190, 170, 7, lower.tail = FALSE) Return to Task 4.8.1.4 InClass Task 4 # the area is 185cm and above. # Key thing is to set lower.tail to FALSE to calculate the above area. pnorm(185, 170, 7, lower.tail = FALSE) Return to Task 4.8.1.5 InClass Task 5 # Question 1 qnorm(0.95, 100, 15, lower.tail = TRUE) # Question 2 pnorm(105, 100, 15, lower.tail = FALSE) - pnorm(110, 100, 15, lower.tail = FALSE) Return to Task 4.8.2 Assignment Solution 4.8.2.1 Load in the Library library(tidyverse) Return to Task 4.8.2.2 Assignment Task 1 t1 &lt;- 3/4 t1 &lt;- 0.75 The probability that the sound will not be a person is 0.75 Return to Task 4.8.2.3 Assignment Task 2 t2 &lt;- (1/4) * (1/4) * (1/4) * (1/4) t2 &lt;- .004 The probability of that sequence of sounds is 0.004 Return to Task 4.8.2.4 Assignment Task 3 t3 &lt;- (1/4) * (1/3) * (1/2) * (0/1) t3 &lt;- 0 The probability of that sequence of sounds is 0 Return to Task 4.8.2.5 Assignment Task 4 t4 &lt;- dbinom(17, 100, 1/4) Assuming a probability of a tone on a given trial is 1 in 4, the probability of hearing 17 ‘tone’ trials in a sequence of 100 trials is 0.0165156 Return to Task 4.8.2.6 Assignment Task 5 t5 &lt;- pbinom(29, 100, 1/4, lower.tail = FALSE) t5 &lt;- dbinom(30:100, 100, 1/4) %&gt;% sum() In this scenario, the probability of hearing 30 ‘vehicle’ trials or more in a sequence of 100 trials is 0.149541 Return to Task 4.8.2.7 Assignment Task 6 The appropriate code would be: t6 &lt;- replicate(10000, sample(0:1, 100, TRUE, c(.5,.5)) %&gt;% sum()) If you were to look at your output you would see something like the following. Remember your numbers will vary from ours due to random sampling. Here we are only showing the first 10 values of 10000 ## int [1:10000] 55 49 44 50 49 51 52 39 48 48 ... Return to Task 4.8.2.8 Assignment Task 7 t7 &lt;- pnorm(90, 100, 10) The probability of a given participant having a D2 score of 90 or lower is 0.1586553 Return to Task 4.8.2.9 Assignment Task 8 t8 &lt;- pnorm(120, 100, 10, lower.tail = FALSE) The probability that a given participant will have a D2 score of 120 or more is 0.0227501 Return to Task 4.8.2.10 Assignment Task 9 t9 &lt;- qnorm(.95, 100, 10) - qnorm(.05, 100, 10) The difference in scores that cut off the top and bottom 5% of the distribution is 32.8970725 Return to Task 4.8.2.11 Assignment Task 10 t10 &lt;- 79.46 The maximum D2 score that they can have in this situation is 79.46 Return to Task Chapter Complete! "],
["permutation-tests-a-skill-set.html", "Lab 5 Permutation Tests - A Skill Set 5.1 Overview 5.2 PreClass Activity 5.3 InClass Activity 5.4 Assignment 5.5 Solutions to Questions", " Lab 5 Permutation Tests - A Skill Set 5.1 Overview In this week’s lab, you will perform your first hypothesis test using a procedure known as a permutation test. We will help you learn how to do this through building and running data simulation procedures. In order to complete this lab, you will require the following skills which we will teach you today: Skill 1: Generating random numbers with base::rnorm() Skill 2: Permuting values with base::sample() Skill 3: Creating a “tibble” (a type of data table) using tibble::tibble() Skill 4: Computing and extracting a difference in group means using dplyr::pull() and purrr::pluck() Skill 5: Creating your own custom functions using base::function() Skill 6: Repeating operations using base::replicate() To many, a lot of statistics must seem a bit like blind faith as it deals with estimating quantities we haven’t observed (or can’t observe), e.g. the mean of a whole population. As such, we have to know if we can trust our procedures for making estimations and inferences because we rarely get a chance to compare the estimated values to the true values to see if they match up. One way to test a procedure, and in turn learn about statistics, is through data simulation. In simulations, we create a population and then draw samples and run tests on the data, i.e. on this known population. By running lots of simulations we can test our procedures and make sure they are acting as we expect them to. This approach is known as a Monte Carlo simulation, named after the city famous for the many games of chance that are played there. Portfolio Point - Monte Carlo or Bust You can go read up on the Monte Carlo approach if you like. It can, however, get quite indepth, as having a brief glance at the wikipedia entry on it highlights. The main thing to keep in mind is that the method involves creating a population and continually taking samples from that population in order to make an inference. This is what we will show you in the lab. Data simulation and “creating” your own datasets, to see how tests work, is a great way to understand statistics. When doing this lab, keep in mind how easy it really is to find a significant result if even randomly created data can give a significant result. This may help dispell any notion that there is something inherently important about a significant result, in itself. 5.2 PreClass Activity We will now take each skill in turn. Be sure to try them all out. It looks a lot of reading but it is mainly just showing you the output of the functions so you can see you are doing it correctly. The key thing is to try them yourselves and don’t be scared to change things to see what might happen if you do it slightly differently. We will also ask a couple of questions along the way to make sure you understand the skills. 5.2.1 Skill 1: Generating Random Numbers The base::rnorm() function generates values from a normal distribution and takes the following arguments: n: the number of observations to generate mean: the mean of the distribution (default 0) sd : the standard deviation of the distribution (default 1) To generate 10 or even 50 random numbers from a standard normal distribution (M = 0, SD = 1), you would use rnorm(10) or rnorm(50) respectively. Type rnorm(50) into your console and see what happens. Use the below example for rnorm(10) to help you. Try increasing n to 1000. rnorm(10) ## [1] 0.52864250 -0.57773351 -0.75134776 -0.02441903 -1.34086011 ## [6] -1.74859921 1.44629918 -0.03592111 -1.26066925 -1.43302823 Quickfire Questions If you enter rnorm(50) again you will get different numbers. Why? I have made a mistake The numbers are random R has made a mistake Phil has made a mistake If you want to change the mean or sd, you would need to pass additional arguments to the function as shown below. rnorm(n = 10, mean = 1, sd = .5) Try changing the mean and sd values a couple of times and see what happens. You get different numbers again that will be around the mean you set! Set a mean of 10, then a mean of 100, to test this. Finally, for this Skill, you can concatenate (i.e. link) numbers together into a single vector using the c() function from base R. For instance, say you wanted to create a vector with two sets of 50 random numbers from two separate samples: one set of 50 with a mean of 75 and the other with a mean of 90, you would use: random_numbers &lt;- c(rnorm(50, 75), rnorm(50, 90)) Quickfire Questions In the above example code, what is the standard deviation of the two samples you have created? 50 75 90 1 Explain This - I don’t get this answer! What is the default sd of the function? Both populations would have an sd of 1, because that is the default, although you could easily change that. Try it out! It is always good to check that your new vector has the right number of data points in it - i.e. the total of the two samples; a sanity check if you will. The new vector random_numbers should have 100 elements. You could verify this using the length() function: length(random_numbers) ## [1] 100 Skill 1 out of 6 Complete! 5.2.2 Skill 2: Permuting Values Another thing that is useful to be able to do is to generate permutations of values. Portfolio Point - What are Permutations? A permutation is just a different ordering of the same values. For example, the numbers 1, 2, 3 can be permuted into the following 6 sequences: 1, 2, 3 1, 3, 2 2, 1, 3 2, 3, 1 3, 1, 2 3, 2, 1 The more values you have, the more permutations of the order you have. The number of permutations can be calculated by, for example, 321, where 3 is the number of values you have. Or through code: factorial(3) = 6. This assumes that each value is used once in the sequence and that each value never changes, i.e. 1234 cannot suddenly become 1235. We can create random permutations of a vector using the sample() function. Let’s use one of R’s built in vectors: letters. Type letters into the console, as below, and press RETURN/ENTER. You will see it contains all the lowercase letters of the English alphabet. Now, I bet you are wondering what LETTERS does, right? letters ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; ## [18] &quot;r&quot; &quot;s&quot; &quot;t&quot; &quot;u&quot; &quot;v&quot; &quot;w&quot; &quot;x&quot; &quot;y&quot; &quot;z&quot; We can combine base::sample() with letters to put the letters into a random order: Run the below line. Run it again. And again. What do you notice? And why is our output different from yours? (The answer is below) sample(letters) ## [1] &quot;j&quot; &quot;s&quot; &quot;h&quot; &quot;p&quot; &quot;e&quot; &quot;z&quot; &quot;t&quot; &quot;m&quot; &quot;f&quot; &quot;v&quot; &quot;n&quot; &quot;a&quot; &quot;d&quot; &quot;x&quot; &quot;o&quot; &quot;q&quot; &quot;w&quot; ## [18] &quot;g&quot; &quot;c&quot; &quot;u&quot; &quot;k&quot; &quot;r&quot; &quot;i&quot; &quot;y&quot; &quot;l&quot; &quot;b&quot; Quickfire Questions If month.name contains the names of the twelve months of the year, how many possible permutations are there of sample(month.name)? 1 12 144 479001600 Portfolio Point - Different samples with sample() Each time you run sample(letters) it will give you another random permutation of the sequence. That is what sample() does - creates a random permutation of the values you give it. Try repeating this command many times in the console. Because there are so many possible sequences, it is very unlikely that you will ever see the same sequence twice! An interesting thing about sample() is that sample(c(1,2,3,4)) is the same as sample(4). And to recap, there would be 24 different permutations based on factorial(4), meaning that each time you type sample(4) you are getting one of those 24 different orders. So what would factorial(12) be? Top Tip: Remember that you can scroll up through your command history in the console using the up arrow on your keyboard; this way, you don’t ever have to retype a command you’ve already entered. Skill 2 out of 6 Complete! 5.2.3 Skill 3: Creating Tibbles Tables are important because most of the data we want to analyze comes in a table, i.e. tabular form. There are different ways to get tabular data into R for analysis. One common way is to load existing data in from a data file (for example, using readr::read_csv() which you have seen before). But other times, you might want to just type in data directly. You can do this using the tibble::tibble() function. Being able to create a tibble is a useful data analysis skill because sometimes you will want to create some data on the fly just to try certain codes or functions. 5.2.3.1 Entering Data into a Tibble The tibble() function takes named arguments - this means that the name you give each argument within the tibble function, e.g. Y = rnorm(10) will be the name of the column that appears in the table, i.e. Y. It’s best to see how it works through an example. tibble(Y = rnorm(10)) ## # A tibble: 10 x 1 ## Y ## &lt;dbl&gt; ## 1 -0.266 ## 2 -0.526 ## 3 -2.38 ## 4 -0.291 ## 5 1.20 ## 6 0.650 ## 7 0.446 ## 8 1.79 ## 9 0.955 ## 10 -0.695 The above command creates a new table with one column named Y, and the values in that column are the result of a call to rnorm(10): 10 randomly sampled values from a standard normal distribution (mean = 0, sd = 1) - See Skill 1. If, however, we wanted to sample from two different populations for Y, we could combine two calls to rnorm() within the c() function. Again this was in Skill 1, here we are now just storing it in a tibble. See below: tibble(Y = c(rnorm(5, mean = -10), rnorm(5, mean = 20))) ## # A tibble: 10 x 1 ## Y ## &lt;dbl&gt; ## 1 -9.33 ## 2 -10.7 ## 3 -13.0 ## 4 -10.4 ## 5 -10.2 ## 6 20.7 ## 7 19.9 ## 8 20.1 ## 9 19.9 ## 10 21.9 Now we have sampled a total of 10 observations - the first 5 come from a group with a mean of -10, and the second 5 come from a group with a mean of 20. Try changing the values in the above example to get an idea of how this works. Maybe even add a third group! But, of course, it would be good to know which population each data point refers to and so we should add some group names. We can do this with some additional trickery using the rep() function. 5.2.3.2 Repeating Values to Save Typing Before finalising our table, let’s learn a little about the base R function, rep(). This is most useful for automatically repeating values in order to save typing. For instance, if we wanted 20 letter “A”s in a row, we would type: rep(&quot;A&quot;, 20) ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; ## [18] &quot;A&quot; &quot;A&quot; &quot;A&quot; The first argument to rep() is the vector containing the information you want repeated, A, and the second argument, times, is the number of times to repeat it; in this case 20. If you wanted to add more information, e.g. if the first argument has more than one element, say “A” and “B”, it will repeat the entire vector that number of times; A B, A B, A B, … . Note that we enclose “A” and “B” in the c() function so that it is seen as a single argument. rep(c(&quot;A&quot;, &quot;B&quot;), 20) ## [1] &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; ## [18] &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; ## [35] &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; But sometimes we want a specific number of As followed by a specific number of Bs; A A A B B B. If the times argument has the same number of elements as the vector given for the first argument, it will repeat each element of the first vector as many times as given by the corresponding element in the times vector. In other words, for example, times = c(2, 4) for vector c(&quot;A&quot;, &quot;B&quot;) will give you 2 As followed by 4 Bs. rep(c(&quot;A&quot;, &quot;B&quot;), c(2, 4)) ## [1] &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; Quickfire Questions The best way to learn about this function is to play around with it in the console and see what happens. From the dropdown menus, the correct output of the following function would be: rep(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;),(2, 3, 1)) - A A A B B C A A B B B C A A B B C C A B C A B C rep(1:5, 5:1) - 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 5 5 5 5 5 4 4 4 4 3 3 3 2 2 1 1 1 1 1 1 2 2 2 2 3 3 3 4 4 5 1 1 1 1 1 1 1 1 5 5 5 5 5 5 5 5.2.3.3 Bringing it Together in a Tibble Now we know rep(), we can complete our table of simulated data by combining what we’ve learned about generating random numbers and repeating values. We want our table to look like this: ## # A tibble: 10 x 2 ## group Y ## &lt;chr&gt; &lt;dbl&gt; ## 1 A -7.93 ## 2 A -10.7 ## 3 A -10.00 ## 4 A -10.4 ## 5 A -9.46 ## 6 B 21.1 ## 7 B 19.9 ## 8 B 20.9 ## 9 B 21.0 ## 10 B 20.2 You now know how to create this table. Have a look at the code below and make sure you understand it. We have one column called group where we create As and Bs through rep(), and one column called Y, our data, all in our tibble(): tibble(group = rep(c(&quot;A&quot;, &quot;B&quot;), c(5, 5)), Y = c(rnorm(5, mean = -10), rnorm(5, mean = 20))) Be sure to play around with the code chunk to get used to it. Try adding a third group or even a third column? Perhaps you want to give every participant a random age with a mean of 18, and an sd of 1; or even a participant number. Helpful Hint Try row_number() to create participant numbers. Don’t forget, if you wanted to store your tibble, you would just assign it to a name, such as my_data: my_data &lt;- tibble(ID = row_number(1:10), group = rep(c(&quot;A&quot;, &quot;B&quot;), c(5, 5)), Y = c(rnorm(5, mean = -10), rnorm(5, mean = 20)), Age = rnorm(10, 18, 1)) Skill 3 out of 6 Complete! 5.2.4 Skill 4: Computing Differences in Group Means You have already learned how to calculate group means using group_by() and summarise(). For example, you might want to calculate sample means for a randomly generated dataset like so: my_data &lt;- tibble(group = rep(c(&quot;A&quot;, &quot;B&quot;), c(20, 20)), Y = c(rnorm(20, 20, 5), rnorm(20, -20, 5))) my_data_means &lt;- my_data %&gt;% group_by(group) %&gt;% summarise(m = mean(Y)) my_data_means ## # A tibble: 2 x 2 ## group m ## &lt;chr&gt; &lt;dbl&gt; ## 1 A 20.9 ## 2 B -20.1 Sometimes what we want though is to calculate the differences between means rather than just the means; so we’d like to subtract the second group mean -20.1 from the first group mean of 20.9, to get a single value, the difference: 41.1. We can do this using the dplyr::pull() and purrr::pluck() functions. pull() will extract a single column from a dataframe and turn it into a vector. pluck() then allows you to pull out an element (i.e. a value or values) from within that vector. vec &lt;- my_data_means %&gt;% pull(m) vec ## [1] 20.92535 -20.13976 We have now created vec which is a vector containing only the group means; the rest of the information in the table has been discarded. Now that we have vec, we can calculate the mean difference as below, where vec is our vector of the two means and [1] and [2] refer to the two means: vec[1] - vec[2] ## [1] 41.06511 But pluck() is also useful, and can be written as so: pluck(vec, 1) - pluck(vec, 2) ## [1] 41.06511 It can also be incorporated into a pipeline, as below, where we still pull() the means column, m, and then pluck() each value in turn and subtract them from each other. ## whole thing in a pipeline my_data_means %&gt;% pull(m) %&gt;% pluck(1) - my_data_means %&gt;% pull(m) %&gt;% pluck(2) ## [1] 41.06511 However, there is an alternative way to extract the difference between means which may make more intuitive sense. You already know how to calculate a difference between values in the same row of a table using dplyr::mutate(), e.g. mutate(new_column = column1 minus column2). So if you can get the observations in my_data_means into the same row, different columns, you could then use mutate() to calculate the difference. Previously you learned gather() to bring columns together. Well the opposite of gather is the tidyr::spread() function to split columns apart - as below. my_data_means %&gt;% spread(group, m) ## # A tibble: 1 x 2 ## A B ## &lt;dbl&gt; &lt;dbl&gt; ## 1 20.9 -20.1 The spread function (?spread) splits the data in column m by the information, i.e. labels, in column group and puts the data into separate columns. A call to spread() followed by a mutate() can be used to calculate the difference in means - see below: my_data_means %&gt;% spread(group, m) %&gt;% mutate(diff = A - B) ## # A tibble: 1 x 3 ## A B diff ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 20.9 -20.1 41.1 Quickfire Questions What is the name of the column containing the differences between the means of A and B? means group m diff Finally, if you then wanted to just get diff and throw away everything else in the table: my_data_means %&gt;% spread(group, m) %&gt;% mutate(diff = A - B) %&gt;% pull(diff) ## [1] 41.06511 Portfolio Point - Reading pipes and verbalising tasks Keep in mind that a very useful technique for establishing what you want to do to a dataframe is to verbalise what you need, or to write it down in words, or to say it out loud. Take this last code chunk. What we wanted to do was to spread() the data in m into the groups A and B. Then we wanted to mutate() a new column that is the difference, diff, of A minus B. And finally we wanted to pull() out the value in diff. Often step 1 of writing code or understanding code is knowing what it is you want to do in the first place. After that, you just need the correct functions. Fortunately for us, tidyverse names its functions based on what they specifically do! Skill 4 out of 6 Complete! 5.2.5 Skill 5: Creating Your Own Functions In Skills 1 to 4, we have looked at creating and sampling data, storing it in a tibble, and extracting information from that tibble. Now, say we wanted to do this over and over again. For instance, we might want to generate 100 random datasets just like the one in Skill 4. It would be a pain to have to type out the tibble() function 100 times or even to copy and paste it 100 times. We’d likely make an error somewhere and it would be hard to read. To help us, we can create a custom function that performs the action you want; in our case, creating a tibble of random data. Remember, a function is just a procedure that takes an input and gives you the same output each time - like a toaster! A function has the following format: name_of_function &lt;- function(arg1, arg2, arg3) { ## body of function goes between these curly brackets; i.e. what the function does for you. ## Note that the last value calculated will be returned if you call the function. } First, you define your own function name (e.g. name_of_function) and define the names of the arguments it will take (arg1, arg2, …) - an argument is the information that you feed into your function, e.g. data. Finally, you state the calculations or actions of the function in the body of the function (the portion that appears between the curly braces). One of the most basic possible functions is one that takes no arguments and just prints a message. Here is an example: hello &lt;- function() { print(&quot;Hello World!&quot;) } So this function is called hello. It can be run by typing hello() in your console and it will give the output of Hello World! every single time you run it; it has no other actions or information. Test this in the console now by typing: hello() ## [1] &quot;Hello World!&quot; Awesome right? Ok, so not very exciting. Let’s make it better by adding an argument, name, and have it say Hello to name. hello &lt;- function(name = &quot;World!&quot;) { paste(&quot;Hello&quot;, name) } This new function is again called hello() and replaces the one you previously created. This time however you are supplying what is called a default argument, `name = “World!”, but it still has the same action as the previous function of putting “Hello” and “World!” together. So if you run it you get “Hello World!”. Try it yourself! hello() ## [1] &quot;Hello World!&quot; The difference this time however is that because you have added an argument to the input, you can change the information you give the argument and therefore change the output of the function. More flexible. More exciting. Quickfire Questions Test your understanding by answering these questions: Typing hello(&quot;Phil&quot;) in the console with this new function will give: Hello Heather Hello Phil Hello Niamh Hello Kevin Typing the argument as &quot;is it me you are looking for&quot; will give: Hello is it me you are looking for I just called to say Hello You had me at Hello Hello seems to be the hardest word What argument would you type to get “Hello Dolly!” as the output: Dolly Molly Holly Dolly! Most of the time however we want to create a function that computes a value or constructs a table. For instance, let’s create a function that returns randomly generated data from two samples, as we learned in the previous skills - see below. All we are doing is taking the tibble we created in Skill 4 and putting it in the body (between the curly brackets) of the function. gen_data &lt;- function() { tibble(group = rep(c(&quot;A&quot;, &quot;B&quot;), c(20, 20)), Y = c(rnorm(20, 20, 5), rnorm(20, -20, 5))) } This function is called gen_data() and when we run it we get a randomly generated table of two groups, each with 20 people, one with M = 20, SD = 5, the other with M = -20, sd = 5. Try running this gen_data() function in the console a few times; remember that as the data is random, the numbers will be different each time you run it. But say we want to modify the function to allow us to get a table with smaller or larger numbers of observations per group. We can add an argument n and modify the code as follows. Create this function and run it a couple of times through gen_data(). The way to think about this is that every place that n appears in the body of the function (between the curly brackets) it will have the value of whatever you gave it in the arguments, i.e. in this case, 20. gen_data &lt;- function(n = 20) { tibble(group = rep(c(&quot;A&quot;, &quot;B&quot;), c(n, n)), Y = c(rnorm(n, 20, 5), rnorm(n, -20, 5))) } Quickfire Questions How many total participants would there be if you ran gen_data(2)? 2 4 20 40 What would you type to get 100 participants per group? gen_data(50) gen_data(10) gen_dota(100) gen_data(100) Challenge Question: Keeping in mind that functions can take numerous arguments, and that each group in your function have separate means, can you modify the function gen_data to allow the user to change the means for the two calls to rnorm? Have a try before revealing the solution below. Solution To Challenge Question gen_data &lt;- function(n = 20, m1 = 20, m2 = -20) { tibble(group = rep(c(&quot;A&quot;, &quot;B&quot;), c(n, n)), Y = c(rnorm(n, m1, 5), rnorm(n, m2, 5))) } # m1 is the mean of group A # m2 is mean of group B # The function would be called by: gen_data(20, 20, -20) # Giving 20 participants in each group, # The first group having a mean of 20, # The second group having a mean of -20. # Likewise, a call of: gen_data(4, 10, 5) # Would give two groups of 4, # The first having a mean of 10, # The second having a mean of 5. Portfolio Point - Two important facts about functions Here are two important things to understand about functions. Functions obey lexical scoping. What does this mean? It’s like what they say about Las Vegas: what happens in the function, stays in the function. Any variables created inside of a function will be discarded after the function executes and will not be accessible to the outside calling process. So if you have a line, say a variable my_var &lt;- 17 inside of a function, and try to print my_var from outside of the function, you will get an error: object ‘my_var’ not found. Although the function can ‘read’ variables from the environment that are not passed to it through an argument, it cannot change them. So you can only write a function to return a value, not change a value. Functions return the last value that was computed. You can compute many things inside of a function but only the last thing that was computed will be returned as part of the calling process. If you want to return my_var, which you computed earlier but not as the final computation, you can do so explicitly using return(my_var) at the end of the function (before the second curly bracket). Skill 5 out of 6 Complete! 5.2.6 Skill 6: Replicating Operations The last skill you will need for the upcoming lab is knowing how to repeat an action (or expression) multiple times. You saw this in Lab 4 so we will only briefly recap here. Here, we use the base function replicate(). For instance, say you wanted to calculate the mean from rnorm(100) ten times, you could write it like this: ## bad way rnorm(100) %&gt;% mean() rnorm(100) %&gt;% mean() rnorm(100) %&gt;% mean() rnorm(100) %&gt;% mean() rnorm(100) %&gt;% mean() rnorm(100) %&gt;% mean() rnorm(100) %&gt;% mean() rnorm(100) %&gt;% mean() rnorm(100) %&gt;% mean() But it’s far easier and more readable to wrap the expression in the replicate() function where the first argument is the number of times you want to repeat the expression stated as the second argument, i.e. replicate(times, expression). Below, we replicate the mean of 100 randomly generated numbers from the normal distribution, and we do this 10 times: replicate(10, rnorm(100) %&gt;% mean()) Also you’ll probably want to store the results in a variable, for example, ten_samples: ten_samples &lt;- replicate(10, rnorm(100) %&gt;% mean()) ten_samples ## [1] 0.064374954 -0.096612994 -0.002066767 -0.027175987 -0.021833557 ## [6] 0.161512540 0.068076541 0.100106527 0.037087794 -0.078623645 Each element (value) of the vector within ten_samples is the result of a single call to rnorm(100) %&gt;% mean(). Quickfire Questions Assuming that your hello() function from Skill 5 still exists, and it takes the argument name = Goodbye, what would happen in the console if you wrote, replicate(1000, hello(&quot;Goodbye&quot;))? Hello World would appear a thousand times hello Goodbye would appear a thousand times Hello Goodbye would appear a thousand times - Try it and see if it works! Solution To Quickfire Question # the function would be: hello &lt;- function(name = &quot;World!&quot;){ paste(&quot;Hello&quot;, name) } # and would be called by: replicate(1000, hello(&quot;Goodbye&quot;)) Skill 6 out of 6 Complete! Job Done - Activity Complete! To recap, we have shown you the following six skills: Skill 1: Generating random numbers with base::rnorm() Skill 2: Permuting values with base::sample() Skill 3: Creating a “tibble” (a type of data table) using tibble::tibble() Skill 4: Computing and extracting a difference in group means using dplyr::pull() and purrr::pluck() Skill 5: Creating your own custom functions using base::function() Skill 6: Repeating operations using base::replicate() You will need these skills in the coming lab to help you perform a real permutation test. Through these skills and the permutation test you will learn about null hypothesis significance testing. If you have any questions please post them on the Moodle forum or the slack forum rguppies.slack.com under the channel #level2_2018. Finally, don’t forget to add any useful information to your Portfolio before you leave it too long and forget. Great work today. That is all for now. See you in the lab! 5.3 InClass Activity 5.3.1 Permutation Tests of Hypotheses A common statistical question when comparing two groups might be, “Is there a real difference between the group means?” From this we can establish two contrasting hypotheses: The null hypothesis which states that the group means are equivalent and is written as: \\(H_0: \\mu_1 = \\mu_2\\) where \\(\\mu_1\\) is the population mean of group 1 and \\(\\mu_2\\) is the population mean of group 2 Or the alternative hypothesis which states that the group means are not equivalent and is written as: \\(H_1: \\mu_1 \\ne \\mu_2\\). Using the techniques you read about in the PreClass and in previous labs, today you will learn how to test the null hypothesis of no difference between two independent groups. We will first do this using a permutation test before looking at other tests in later labs. A permutation test is a basic inferential procedure that involves a reshuffling of group labels or values to create new possible outcomes of the data you collected to see how your original mean difference compares to a distribution of possible outcomes. The test can in fact be applied in many situations, this is just one, and it provides a good starting place for understanding hypothesis testing. The steps for the exercise below, and really the logic of a permutation test for two independent groups, are: Calculate the real difference \\(D_{orig}\\) between the means of two groups (e.g. Mean of A minus Mean of B). Randomly shuffle the group labels (i.e. which group each participant belonged to - A or B) and re-calculate the difference, \\(D&#39;\\). Repeat step 2 \\(N_{r}\\) times, where \\(N_r\\) is a large number (typically greater than 1000), storing each \\(D_i&#39;\\) value to form a null hypothesis distribution. Locate the difference you observed in step 1 (the real difference) on the null hypothesis distribution of possible differences. Decide whether the original difference is sufficiently extreme to reject the null hypothesis of no difference (\\(H_0\\)). This logic works because if the null hypothesis is true (there is no difference between the groups) then the labeling of the observations/participants into groups is arbitrary, and we can rearrange the labels in order to estimate the likelihood of our original difference under the \\(H_0\\). In other words, if you know the original value of the difference between two groups (or the true difference) falls in the middle of your permuted distribution then there is no significant difference between the two groups. If, however, the original difference falls in the tail of the permuted distribution then there might be a significant difference depending on how far into the tail it falls. Let’s get started! 5.3.2 Step 1: Load in Add-on Packages and Data 1.1. Open a new script and call tidyverse into your library. 1.2. Now type the statement set.seed(1011) at the top of your script after your library call and run it. (This ‘seeds’ the random number generator so that you will get the same results as everyone else. The number 1011 is a bit random but if everyone uses it then we all get the same outcome. Different seeds give different outcomes) Note: This lab was written under RVersion 3.6.1. If you use a different RVersion (e.g. R 3.5.1) or set a different seed (e.g. 1012) you may get different results. 1.3. Download the data file from here and read the data in perm_data.csv into a variable called dat. 1.4. Let’s give every participant a participant number by adding a new column to dat. Something like this would work: mutate(subj_id = row_number()) Helpful Hint 1.1 - Something to do with library() 1.2 - set.seed(1011) 1.3 - Something to do with read_csv() 1.4 - pipe (%&gt;%) dat into the mutate line shown Portfolio Point - Different uses of row_number You will see that, in the example here, to put a row number for each of the participants we do not have to state the number of participants we have. In the Preclass however, we did. What is the difference? Well, in the Preclass we were making a tibble and trying to create a column in that tibble using row_numbers. If you want to do that you have to state the number of rows, e.g. 1:20. However, in this example in the lab today the tibble already exists, we are just adding to it. If that is the case then you can just mutate on a column of row numbers without stating the number of participants. In summary: When creating the tibble, state the number of participants in row_numbers(). If tibble already exists, just mutate on row_numbers(). No need for specific numbers. Have a look at the resulting tibble, dat. ## # A tibble: 100 x 3 ## group Y subj_id ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 A 113. 1 ## 2 A 91.0 2 ## 3 A 89.2 3 ## 4 A 110. 4 ## 5 A 118. 5 ## 6 A 104. 6 ## 7 A 100. 7 ## 8 A 94.1 8 ## 9 A 94.8 9 ## 10 A 92.5 10 ## # ... with 90 more rows The column Y is your dependent variable (DV) The column group is your independent variable (IV). The column subj_id is the participant number. 5.3.3 Step 2: Calculate the Original Mean Difference - \\(D_{orig}\\) We now need to write a pipeline of five functions (i.e. commands) that calculates the mean difference between the groups in dat, Group A minus Group B. Broken down into steps this would be: 2.1.1. Use a pipe of two dplyr one-table verbs (e.g. Lab 2) to create a tibble where each row contains the mean of one of the groups. Name the column storing the means as m. 2.1.2. Continue the pipe to spread() your data from long to wide format, based on the columns group and m. 2.1.3. Now add a pipe that creates a new column in this wide dataset called diff which is the value of group A’s mean minus group B’s mean. 2.1.4. Pull out the value in diff (the mean of group A minus the mean of group B) to finish the pipe. Helpful Hint dat %&gt;% group_by(?) %&gt;% summarise(m = ?) %&gt;% spread(group, m) %&gt;% mutate(diff = ? - ?) %&gt;% pull(?) Quickfire Questions Check that your value for d_orig is correct, without using the solution, by typing your d_orig value to two decimal places in the box. Include the sign, e.g. -1.23. The box will go green if you are correct. The above steps have created a pipeline of five functions to get one value. Nice! We now need to turn this into a function because we are going to be permuting the data set (specifically the grouping labels) and re-calculating the difference many, many times. 2.2. Wrap your pipeline in a function called calc_diff but swap dat for x. This function will take a single argument named x, where x is the tibble that you want to calculate group means from. As in the previous step, the function will return a single value which is the difference between the group means. The start will look like this below: calc_diff &lt;- function(x){ x %&gt;%..... } Helpful Hint calc_diff &lt;- function(x) { x %&gt;% group_by(group) %&gt;% the_rest_of_your_pipe... } 2.3. Now call your new function where x = dat as the argument and store the result in a new variable called d_orig. Make sure that your function returns the same value as you got above and that your function returns a single value rather than a tibble. You can test this: is.tibble(d_orig) should give you FALSE and is.numeric(d_orig) should give you TRUE. Helpful Hint d_orig &lt;- function_name(x = data_name) # or d_orig &lt;- function_name(data_name) # Then type the following in the Console and look at the answer: is.tibble(d_orig) # True (is a tibble) or False (is not a tibble) is.numeric(d_orig) # True (is numeric) or False (is not numeric; it is a character or integer instead.) So we now have the original difference between the groups stored in d_orig. Next we need to create a distribution of possible differences to see where our original difference lies in this distribution. But first we need to shuffle the group letters (A or B) in our dataset and find the difference…a few hundred times! 5.3.4 Step 3: Permute the Group Labels 3.1. Create a new function called permute() that takes as input a dataset x and returns the same dataset transformed such that the group labels (the values in the column group) are shuffled: started below for you. This will require using the sample() function within a mutate(). You have used mutate() twice already today and you saw how to sample() letters in the PreClass. permute &lt;- function(x){ x %&gt;%..... } Helpful Hint Might be easier to think of these steps in reverse. Start with a mutate() function that rewrites the column group every time you run it, e.g. dat %&gt;% mutate(variable = sample(variable)) Now put that into your permute() function making the necessary adjustments to the code so it starts x %&gt;%…. Again x should be in the function and not dat. &quot;) 3.2. Try out your new permute() function by calling it on dat (i.e. x = dat) a few times. You should see the group labels in the group column changing randomly. The most common mistake is that people mutate a new column by mispelling group. You want to overwrite/change the information in the group column not make a new one, so be careful with the spelling. Group Discussion Point Now would be an excellent time to spend five minutes as a group recapping what you are doing. You have the original difference between groups. You have a function that calculates and stores this difference. You have a function that reshuffles the labels of the group. Do you understand why? If not, go back to the principles of the permutation test at the start of the lab then read on… 5.3.5 Step 4: Create the Null-Hypothesis Distribution (NHD) for the Difference Now that we have the original difference and our two functions, one to shuffle group labels and one to calculate the difference between two groups, we need to actually create the distribution of possible differences and see where the original difference lies in it. 4.1.1. Write a a single pipeline that takes dat as the input, permutes the group labels with a call to your function permute(), and then calculates the difference in means between these new groups with a call to your function calc_diff(). 4.1.2. Run this line manually a few times and watch the resulting value change as the labels get permuted. Helpful Hint Think about verbalising your pipelines. In a single pipeline: I want to permute the data into two new groups. Then I want to calculate the difference between these two new groups. The functions you have created do these steps. You just have to put them in order and pipe the data through it. 4.2. Now take your pipeline of functions and repeat it 1000 times using the replicate() function. Store the output in a variable called nhd. nhd will contain 1000 values where each value is the mean difference of each of the 1000 random permutations of the data. (Warning: This will probably take a while to run, perhaps 10 seconds.) Helpful Hint # replace expression with the pipeline you created in 4.1.1 nhd &lt;- replicate(times, expression) You now have 1000 possible values of the difference between the permuted groups A and B - your permuted distribution. 4.3 Let’s visualise this distribution through a frequency histogram of the values in nhd. This shows us the likelihood of various mean differences under \\(H_0\\). One thing to note however is that nhd is not a tibble and ggplot needs it to be a tibble. You need to convert it. You might start by do something like: ggplot(data = tibble(x = NULL), aes(x)) + NULL Helpful Hint Remember that ggplot works as: ggplot(data, aes(x)) + geom…. Here you need to convert nhd into a tibble and put that in as your data. Look at the example above and keep in mind that, in this case, the first NULL could be replaced with the data in nhd. Group Discussion Point Looking at the histogram, visually locate where your original value would sit on this distribution. Would it be extreme, in the tail, or does it look rather common, in the middle? is in the middle so looks common is in the tail so looks extreme Before moving on stop to think about what this means - that the difference between the two original groups is rather uncommon in this permuted distribution, i.e. is in the tails! Again, if unsure, go back to the principles of NHST or discuss it with your tutor! 5.3.6 Step 5: Compare the Observed Mean Difference to the NHD If the null hypothesis is false, and there is a real difference between the groups, then the difference in means we observed for the original data (d_orig) should be somewhere in either tail of the null-hypothesis distribution we just estimated; it should be an “extreme” value. How can we test this beyond a visual inspection? First, we have to decide on a false positive (Type I error) rate which is the rate at which we will falsely reject \\(H_0\\) when it is true. This rate is referred to by the Greek letter \\(\\alpha\\) (“alpha”). Let’s just use the conventional level used in Psychology: \\(\\alpha = .05\\). So the question we must ask is, if the null hypothesis was true, what would be the probability of getting a difference in means as extreme as the one we observed in the original data? We will label this probability p. Group Discussion Point Take a few moments as a group to see if you can figure out how you might compute p from the data before we show you how. We will then show you the process in the next few, final, steps. 5.1. Replace the NULLS in the code below to create a logical vector which states TRUE for all values of nhd greater than or equal to d_orig regardless of sign. Note: A logical vector is one that returns TRUE when the expression is true and FALSE when the expression is false. lvec &lt;- abs(NULL) &gt;= abs(NULL) Portfolio Point - abs and the case of one or two tails In the code above, the function abs() says to ignore the sign and use the absolute value. For instance, if d_orig = -7, then abs(d_orig) = 7. Why do we do this here? Can you think why you want to know how extreme your value is in this distribution regardless of whether the value is positive or negative? The answer relates to whether you are testing in one or two tails of your distribution; the positive side, the negative side, or both. You will have heard in your lectures of one or two-tailed tests. Most people would say to run two-tailed tests. This means looking at the negative and positive tails of the distribution to see if our original value is extreme, and the simplest way to do this is to ignore the sign of the values and treat both sides equally. If you wanted to only test one-tail, say that your value is extreme to the negative side of the tail, then you would not use the abs() and set the expression to make sure you only find values less than your original value. To test only on the positive side of the distribution, make sure you only get values higher than the original. But for now we will mostly look at two-tailed tests. 5.2. Replace the NULL in the code below to sum() the lvec vector to get the total number of values equal to or greater than our original difference, d_orig. Fortunately, R is fine with summing TRUEs and FALSEs so you do not have to convert the data at all. n_exceeding_orig &lt;- NULL 5.3. Replace the NULL in the code below to calculate the probability of finding a value of d_orig in our nhd distribution by dividing n_exceeding_orig, the number of values greater than or equal to your original value, by the length() of your whole distribution nhd. Note: the length of nhd is the same as the number of replications we ran. Using code reduces the chance of human error. p &lt;- NULL 5.4. Finally, complete the sentence below determining if the original value was extreme or not in regards to the distribution. Use inline coding, shown in Lab 1, to replace the XXXs. For example, when formatted without the space before the first r, r length(nhd) would present as 1000. &quot; The difference between Group A and Group B (M = XXX) was found to be have a probability of p = XXX. This means that the original mean difference was …… and the null hypothesis is …..&quot; Job Done - Activity Complete! Well done in completing this lab. Let’s recap before finishing. We had two groups, A and B, that we had tested in an experiment. We calculated the mean difference between A and B and wanted to know if this was a significant difference. To test this, we created a distribution of possible differences between A and B using the premise of permutation tests and then found the probability of our original value in that permuted distribution. The more extreme the value in a distribution, the more likely that the difference is significant. And that is exactly what we found; an \\(\\alpha &lt; .05\\). Next time we will look at using functions and inferential tests to perform this analysis but by understanding the above you now know how probability is determined. You should now be ready to complete the Homework Assignment for this lab. The assignment for this Lab is summative and should be submitted through the Moodle Level 2 Assignment Submission Page no later than 1 minute before your next lab. If you have any questions, please post them on the slack forum under the channel #level2_2018. Finally, don’t forget to add any useful information to your Portfolio before you leave it too long and forget. 5.4 Assignment This is a summative assignment. Instructions on how to access and submit your assignment will be made available during the course. 5.5 Solutions to Questions Below you will find the solutions to the questions for the Activities for this chapter. Only look at them after giving the questions a good try and speaking to the tutor about any issues. 5.5.1 InClass Activities 5.5.1.1 Step 1 library(&quot;tidyverse&quot;) ## Note: ## Here&#39;s how we created the data for today&#39;s task: ## dat &lt;- tibble(group = rep(c(&quot;A&quot;, &quot;B&quot;), each = 50), ## Y = c(rnorm(50, 100, 15), ## rnorm(50, 110, 15))) ## ## write_csv(dat, &quot;perm_data.csv&quot;) ## You could create a new dataset yourself and try it again. dat &lt;- read_csv(&quot;perm_data.csv&quot;) %&gt;% mutate(subj_id = row_number()) Return to Task 5.5.1.2 Step 2 Step 2.1.1 - the basic dat pipeline dat %&gt;% group_by(group) %&gt;% summarise(m = mean(Y)) ## # A tibble: 2 x 2 ## group m ## &lt;chr&gt; &lt;dbl&gt; ## 1 A 101. ## 2 B 109. Step 2.1.2 - using spread() to separate the groups dat %&gt;% group_by(group) %&gt;% summarise(m = mean(Y)) %&gt;% spread(group, m) ## # A tibble: 1 x 2 ## A B ## &lt;dbl&gt; &lt;dbl&gt; ## 1 101. 109. Step 2.1.3 - mutate() the column of mean differences dat %&gt;% group_by(group) %&gt;% summarise(m = mean(Y)) %&gt;% spread(group, m) %&gt;% mutate(diff = A - B) ## # A tibble: 1 x 3 ## A B diff ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 101. 109. -7.39 Step 2.1.4 - pull() out the difference dat %&gt;% group_by(group) %&gt;% summarise(m = mean(Y)) %&gt;% spread(group, m) %&gt;% mutate(diff = A - B) %&gt;% pull(diff) ## [1] -7.388401 Step 2.2 - setting up the calc_diff() function calc_diff &lt;- function(x) { x %&gt;% group_by(group) %&gt;% summarise(m = mean(Y)) %&gt;% spread(group, m) %&gt;% mutate(diff = A - B) %&gt;% pull(diff) } Step 2.3 - Calculating d_orig using calc_diff() d_orig &lt;- calc_diff(dat) is.tibble(d_orig) ## Warning: `is.tibble()` is deprecated, use `is_tibble()`. ## This warning is displayed once per session. is.numeric(d_orig) ## [1] FALSE ## [1] TRUE Return to Task 5.5.1.3 Step 3 permute &lt;- function(x) { x %&gt;% mutate(group = sample(group)) } permute(dat) ## # A tibble: 100 x 3 ## group Y subj_id ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 B 113. 1 ## 2 A 91.0 2 ## 3 A 89.2 3 ## 4 B 110. 4 ## 5 A 118. 5 ## 6 B 104. 6 ## 7 B 100. 7 ## 8 A 94.1 8 ## 9 B 94.8 9 ## 10 B 92.5 10 ## # ... with 90 more rows Return to Task 5.5.1.4 Step 4 Step 4.1 - the pipeline dat %&gt;% permute() %&gt;% calc_diff() ## [1] -1.53662 Step 4.2 - creating nhd nhd &lt;- replicate(1000, dat %&gt;% permute() %&gt;% calc_diff()) Step 4.3 - plotting nhd ggplot(tibble(x = nhd), aes(x)) + geom_histogram(binwidth = 1) Figure 5.1: CAPTION THIS FIGURE!! Return to Task 5.5.1.5 Step 5 Step 5.1 - The logical vector This code establishes all the values in nhd that are equal to or greater than the value in d_orig It returns all these values as TRUE and all other values as FALSE abs() tells the code to ignore the sign of the value (i.e. assumes everything is positive) lvec = abs(nhd) &gt;= abs(d_orig) Step 5.2 - Sum up all the TRUE values This gives the total number of values greater or equal to d_orig n_exceeding_orig &lt;- sum(lvec) Step 5.3 - Calculate the probability The probability of obtaining d_orig or greater is calculated by the number of values equal to or greater than d_orig, divided by the full size of nhd (or in other words, its length) p &lt;- n_exceeding_orig / length(nhd) Return to Task Chapter Complete! "],
["nhst-and-one-sample-t-tests.html", "Lab 6 NHST and One-Sample t-tests 6.1 Overview 6.2 PreClass Activity 1 6.3 PreClass Activity 2 (Additional) 6.4 InClass Activity 6.5 Assignment 6.6 Solutions to Questions", " Lab 6 NHST and One-Sample t-tests 6.1 Overview In the previous labs, we have talked a lot about probability, comparing average values across groups, and the inference from a sample to a population. In effect, this is the essence of a lot of quantitative research. You collect a sample, calculate a summary statistic about that sample, and use probability to establish the likelihood of that statistic occurring given certain situations. To demonstrate this further, and to start introducing commonly used statistical tests, we will look at data related to a recent Posner experiment by Woods et al (2009). We will use part of the information shown in the paper to explore hypothesis testing. Through the lab 6 activities you will learn about: Testing a hypothesis through null hypothesis significance testing (NHST) Binomial tests, as well as one-sample and independent t-tests Posner paradigms and attention (preclass), and the recency effect (inclass). Portfolio Point - Additional background on The Posner Paradigm You don’t need this for the actual lab but it might help it make more sense. The Posner paradigm (Posner, 1980), or the Posner Cueing task, is an attentional shift task, often used in a variety of fields to test spatial attention and how this is impacted by disorders or injury. It works by having participants look at a fixation cross in the center of a screen. To either side is an empty box. After a short delay, a cue (e.g. an arrow, an asterisk, or some other attention grabbing image) appears in one of the boxes (e.g. the box to the left of the fixation). This stays on screen for a few hundred milliseconds and is then replaced by a second image called the target (e.g. a different shape or image). Participants then have to respond left or right depending on which side of the fixation the target appeared. The DV is the time taken to respond to the target appearing. The key thing is that most of the time the target will appear on the same side as the cue - the cue facilitates the target - and so participants will be quicker to respond. These are called valid trials. However, on some occasions the target will appear on the other side from the cue - e.g. the cue is on the left but the target appears on the right - and these are called invalid trials; participants will be slower to respond here as the cue misleads the participant. From that, you should be starting to get an idea of how a Posner paradigm can help to measure attention and how it can help determine if people have issues in shifting attention (particularly from the invalid trials). Posner, M. (1980) Orienting of attention. Quarterly Journal of Experimental Psychology, 32(1), 3-25 6.2 PreClass Activity 1 Woods and colleagues were interested in how the attention of people with poor sleep (Primary Insomnia - PI) was directed towards sleep related stimuli compared to people with normal sleep (NS). They hypothesised that participants with poor sleep would be more attentive to images related to a lack of sleep (i.e. an alarm clock showing 2AM) than participants with normal sleep would be. To test this, they used a modified Posner paradigm, shown in Figure 1 of the paper, where images of the alarm clock acted as the cue on both valid and invalid trials, with the symbol ( .. ) being the target. As can be seen in Figure 3 of the paper, Woods and colleagues found that there was no difference between groups on valid trials. However, Primary Insomnia participants were faster in responding to the target suggesting a slight increase in attention to the sleep related cue compared to the Normal Sleepers. In contrast, for invalid trials, there was a significant difference between groups that was consistent with the original hypothesis; poor sleepers (Primary Insomnia participants) were slower to respond to the target on invalid trials, compared to Normal Sleepers, due to the attention of the Primary Insomnia participants being drawn to the misleading cue (the alarm clock) on the invalid trials. This increased attention to the sleep related cue leads to an overall slower reponse to the target. Are These Participants Normal Sleepers (NS)? Psychology results tend to be more reproducible when we increase participant numbers and in turn the power of the study. However, running several hundred participants can be a significant time and financial investment. Fortunately, the idea of a “ManyLabs”&quot; project can solve this problem. In this, the same experiment is run in various locations, all using the same procedure, and then the data is collapsed together and analysed as one. Portfolio Point - The power of what? Power is a rather tricky concept in research that essentially amounts to the probability of your design being able to detect a significant difference when there is actually a significant difference to detect. Power is an interplay between three other aspects of research design: alpha - your critical p-value (normally .05); the sample size (n); the effect size - how big is the difference (measured in a number of ways). If you know any three of these four elements (power, alpha, effect size, n) you can calculate the fourth. We will save further discussion of power until Lab 8 but if you want to read ahead then this blog is highly recommended: The Power Dialogues. In your quest to be a collaborative researcher, you have joined a ManyLabs study replicating the findings of Woods et al. (2009). Each lab is asked to gather data from 22 participants. After gathering your data, the first key question is whether or not your participants are responding in a similar fashion to the original study. As a group you have decided that you need to do this test to assess whether you have been able to draw a sample of normal sleepers similar to the sample drawn by Woods et al. We will only look at valid trials for normal sleeping participants but in effect you could perform this test on all groups and conditions. Below you see the data you have collected from 22 participants. Their mean reaction time for valid trials is shown in the right hand column. Table 6.1: Data for 22 Participants on Posner task participant valid_rt 1 631.2 2 800.8 3 595.4 4 502.6 5 604.5 6 516.9 7 658.0 8 502.0 9 496.7 10 600.3 11 714.6 12 623.7 13 634.5 14 724.9 15 815.7 16 456.9 17 703.4 18 647.5 19 657.9 20 613.2 21 585.4 22 674.1 In Woods et al (2009) Figure 3 you will see that, on valid trials, the mean reaction time for NS participants was 590 ms with a SD = 94 ms. As above, your overall goal is to do further analyses where the new 22 participants are compared with the original 22 participants, but in order to do this you first must show that the new participants are from the same population as the original sample. To state this in terms of a hypothesis, you are testing that there is no significant difference between the new sample and the original sample based on mean reaction times to valid trials on the modified Posner experiment. We will show you how to do this through two tests. In tasks 1-3 we will use a binomial test and in tasks 4-8 we will use a one-sample t-test Portfolio Point - Binomial test and the one-sample t-test The Binomial test is a very simple test that converts all participants to either being above or below a cut-off point, e.g. the mean value, and looking at the probability of finding that number of participants above that cut-off. The one-sample t-test is similar in that it compares participants to a cut-off but it compares the mean and standard deviation of the collected sample to an ideal mean and standard deviation. By comparing the difference in means, divided by the variance of the difference (the standard deviation), we can determine if the sample is similar or not to the ideal mean. 6.2.1 The Binomial Test The Binomial test is one of the “simplest tests”&quot; in null hypothesis testing in that it uses very little information. The binomial test is used when a study has two possible outcomes (success vs failure) and you have an idea about what the probability of success is - much like in Lab 4 and the Binomial distribution. A binomial test tests if an observed result is different from what was expected. In our case, we want to test whether our normal sleepers are the same or different from those measured by Woods et al. The following tasks will take you through the process. 6.2.2 Task 1: Creating a Dataframe First we need to create a dataframe with our data so that we can work with it. Enter the data for the 22 participants displayed above into a tibble and store it in ns_data. Have one column showing the participant number (called participant) and another column showing the mean reaction time, called valid_rt. We saw how to enter data into tibbles in Lab 5 preclass Skill 3. Helpful Hint You can use this code structure and replace the NULL values: ns_data &lt;- tibble(participant = c(NULL,NULL,…), valid_rt = c(NULL,NULL,…)) The values are: 631.2, 800.8, 595.4, 502.6, 604.5, 516.9, 658.0, 502.0, 496.7, 600.3, 714.6, 623.7, 634.5, 724.9, 815.7, 456.9, 703.4, 647.5, 657.9, 613.2, 585.4, 674.1 ### Task 2: Comparing Original and New Sample Reaction Times {#Ch6PreClassQueT2} Our next step is to establish how many participants from the new sample are above the mean in the original paper. In the original paper the mean reaction time for valid trials was 590 ms. Store this value in woods_mean. Now write code to calculate the number of participants in the new sample (ns_data created in Task 1) that had a mean reaction time greater than the original paper’s mean. Store this single value in n_participants. The function nrow() may help here. Helpful Hint woods_mean &lt;- value n_participants &lt;- filter(x ? y) %&gt;% nrow() or dim[] %&gt;% pluck() Quickfire Questions The number of participants that have a mean reaction time for valid trials greater than that of the original paper is: 6 10 16 17 6.2.3 Task 3: Calculating Probability Our final step for the binomial test is to compare our value from Task 2, 16 participants, to our hypothetical cut-off. Let’s assume that the mean reaction time from the original paper, i.e. 590 ms, is a good estimate for the population of good sleepers (NS). If that is the case then each new participant that we have tested should have a .5 chance of being above this mean reaction time. In other words, the expected number of participants above the cut-off would be \\(.5N\\), where \\(N\\) is the number of participants, or \\(.5 * 22\\) = 11 participants. Calculate what would be the probability of observing at least 16 participants out of your 22 participants that had a valid_rt greater than the Woods et al (2009) mean value. Helpful Hint Think back to Lab 4 where we used the binomial distribution. This question can be phrased as, what is the probability of obtaining X or more succeses out of Y trials, given the chance probability of Z. How many Xs? (see question) How many Ys? (see question) What is the probability of being either above or below the mean/cut-off? (see question) You can use a dbinom() %&gt;% sum() for this or maybe a pbinom() Quickfire Questions Do you think these NS participants are responding in just the same fashion as the participants in the original paper? Select the appropriate answer: No Yes Explain This - I don’t get this answer The probability of obtaining 16 participants with a mean reaction time greater than the cut-off of 590 ms is p = .026. This is smaller than the field norm of p = .05. As such we can say that, using the binomial test, the new sample appears to be significantly different from the old sample as there is a significantly larger number of participants above the cut-off (M = 590ms) than would be expected if the new sample and the old sample were the same. 6.2.4 The One-Sample t-test In Task 3 you ran a binomial test of the null hypothesis testing that the mean reaction time for valid trials in good sleepers has remained stable across the two studies. However, this test did not use all the available information because each participant was simply classified as being above or below the mean of the original paper, i.e. yes or no. Information about the magnitude of the discrepancy from the mean was discarded. This information is really interesting and important however and if we wanted to maintain that information then we would need to use a one-sample \\(t\\)-test. In a one-sample \\(t\\)-test, you test the null hypothesis \\(H_0: \\mu = \\mu_0\\) where: \\(H_0\\) is the symbol for the null hypothesis, \\(\\mu\\) is the unobserved population mean, and \\(\\mu_0\\) is some other mean to compare against (which could be an alternative population or sample mean or a constant). For the current problem: \\(\\mu\\) is the unobserved mean of the 22 participants so we will substitute \\(\\bar{X}\\), the mean of the sample. \\(\\mu_0\\) is the mean of the original paper which we observed to be 590. So in other words we are testing the null hypothesis that \\(H_0: \\bar{X} =\\) 590. And we will do this by calculating the test statistic \\(t\\) which comes from the \\(t\\) distribution - more on that distribution below and in the lectures. The formula to calculate the observed test statistic \\(t\\) for the one-sample \\(t\\)-test is: \\(t = \\frac{\\bar{X} - \\mu_0}{s\\ / \\sqrt(n)}\\) Now we just need to fill in the numbers. 6.2.5 Task 4: Calculating the Mean and Standard Deviation Calculate the mean and standard deviation of valid_rt for our 22 participants (i.e., for all participant data at the top of this lab). Store the mean in ns_data_mean and store the standard deviation in ns_data_sd. Make sure to store them both as single values! Helpful Hint Replace NULL with the code that would find the mean, m, of ns_data. ns_data_mean &lt;- summarise(NULL) %&gt;% pull(NULL) Replace NULL with the code that would find the standard deviation, sd, of ns_data. ns_data_sd &lt;- summarise(NULL) %&gt;% pull(NULL) 6.2.6 Task 5: Calculating the Observed Test Statistic From Task 4, you found out that \\(\\bar{X}\\), the sample mean, was 625.464 ms, and \\(s\\), the sample standard deviation, was 94.307 ms. Now, keeping in mind that \\(n\\) is the number of observations in the sample, and \\(\\mu_0\\) is the mean from Woods et al (2009): Substitute the values into the one-sample t-test formula above to compute your observed test statistic. Store the answer in t_obs . Answering this question will help you in this task as you’ll also need these numbers to substitute into the formula: The mean from Woods et al (2009) was 595 590 580 585, and the number of participants in our sample is: (type in numbers) . Helpful Hint Remember BODMAS and/or PEDMAS when given more than one operation to calculate. (i.e. Brackets/Parenthesis, Orders/Exponents, Division, Multiplication, Addition, Subtraction) t_obs &lt;- (sample mean - woods mean) / (sample standard deviation / square root of n) 6.2.7 Task 6: Comparing the Observed Test Statistic to the t-distribution using pt() Now you need to compare t_obs to the t-distribution to determine how likely the observation (i.e. your test statistic) is under the null hypothesis of no difference. To do this you need to use the pt() function. Use the pt() function to get the \\(p\\)-value for a two-tailed test with \\(\\alpha\\) level set to .05. The test has \\(n - 1\\) degrees of freedom, where \\(n\\) is the number of observations contributing to the sample mean \\(\\bar{X}\\). Store the \\(p\\) value in the variable pval. Do you reject the null? Helpful Hint Remember to get help you can enter ?pt in the console. The pt() function works similar to pbinom() and pnorm(): pval &lt;- pt(test statistic, df, lower.tail = FALSE) * 2 Use the absolute value of the test statistic; i.e. ignore minus signs. Remember, df is equal to n-1. Use lower.tail = FALSE because we are wanting to know the probability of obtaining a value higher than the one we got. Because we want the p-value for a two-tailed test, multiply pt() by two. Reject the null at the field standard of p &lt; .05 ### Task 7: Comparing the Observed Test Statistic to the t-distribution using t.test() {#Ch6PreClassQueT7} Now that you have done this by hand, try using the t.test() function to get the same result. Take a moment to read the documentation for this function by typing ?t.test in the console window. No need to store the t-test output in a dataframe but do check that the p-value matches the pval in Task 6. Helpful Hint The function requires a vector, not a table, as the first argument. You can use the pull() function to pull out the valid_rt column from the tibble ns_data with pull(ns_data, valid_rt). You also need to include mu in the t.test(), where mu is equal to the mean you are comparing to. Quickfire Questions To make sure you are understanding the output of the t-test, try to answer the following questions. To three decimal places, type in the p value for the t-test in Task 7 As such this one sample t-test is significant not significant The outcome of the binomial test and the one sample t-test produce the same a different answer 6.2.8 Task 8: Drawing Conclusions about the new data Given these results, what do you conclude about how similar these 22 participants are to the original participants in Woods et al (2009) and whether or not you have managed to recruit sleepers similar to that study? Think about which test used more of the available information? Also, how reliable is the finding if the two tests give different answers? We have given some of our thoughts at the bottom of the solutions. Job Done - Activity Complete! That’s all! There is quite a bit in this lab in terms of theory of Null Hypothesis Significance Testing (NHST) so you might want to go back and add any informative points to your Portfolio. Post any questions on the the Moodle forum or the slack forum rguppies.slack.com under the channel #level2_2018. See you in the lab! 6.3 PreClass Activity 2 (Additional) 6.3.1 A short blog on Barplots The data used in this preclass allows us to show you something that you might find interesting. As we mentioned back in lab 3 on visualisation, the barplot is becoming less frequently used, as summarised in this blog: One simple step to improve statistical inferences. The data we have used today demonstrate the point that a simple barplot can actually be somewhat misleading about the data. Have a look at the figure below. Both bars represent the data from our 22 NS participants. The column on the left, hide_data, is a standard representation (albeit without error bars) whereas the column on the right, show_data, demonstrates the issue. Looking at the column on the left, the assumption is that all the data is around the peak of the column. However, looking at the column on the right, we can see that this is not the case and there are participants both above and below the mean by approximately 100 ms. This misleading perception, when the data is hidden, was tested and shown to exist in participants viewing these figures by Newman and Scholl (2012) which you can read up on if you like. The main thing to keep in mind is that barplots can be misleading and displaying the individual data may be more informative. Figure 6.1: How representative are barplots of the actual spread of the data! That’s all there is to this short and sweet blog on showing more information than a basic barchart. The function we use to do this is called geom_jitter() and it gets added to a visualisation pipeline just like other geom_?()s that we have used. Look out for it in the coming labs. 6.4 InClass Activity Comparing the Means of Two Samples Two-sample designs are very common in Psychology as often we want to know whether there is a difference between groups on a particular variable. There are different types of two-sample designs depending on whether or not the two groups are independent (e.g. different participants on different conditions) or not (e.g. same participants on different conditions). In today’s exercise we will focus on independent samples, which typically means that the observations in the two groups are unrelated - usually meaning different people. In the next lab you will examine cases where the observations in the two groups are from pairs (paired samples) - most often the same people but could also be a matched pairs design. Portfolio Point - All the different names for one thing! Now that we are progressing through the semester, we will start to reduce the pointers for things that you should make a note of. By now you should have a really good idea yourself about what you need to remember. That said, one of the really confusing things about research design is that there are many names for the same type of design. This is definitely something you should be writing out in your own words, to remember it better, if it is something you struggle with. E.g. independent and between-subjects design typically mean the same thing - different participants in different conditions within-subjects, dependent, paired samples, and repeated measures tend to mean the same participants in all conditions matched pairs design means different people in different conditions but you have matched participants across the conditions so that they are effectively the same person (e.g. age, IQ, Social Economic Status, etc) mixed design is when there is a combination of within-subjects and between-subjects designs in the one experiment. For example, say you are looking at attractiveness and dominance of male and female faces. Everyone might see both male and female faces (within) but half of the participants do ratings of attractiveness and half do ratings of trustworthiness (between). The paper we are looking at today technically uses a mixed design at times but we will use the between element to show you how to run independent t-tests. Spend some time when reading articles to really figure out the design they are using. Background For this lab we will be revisiting the data from Schroeder and Epley (2015), which you first encountered as part of the homework for Lab 5. You can take a look at the Psychological Science article here: Schroeder, J. and Epley, N. (2015). The sound of intellect: Speech reveals a thoughtful mind, increasing a job candidate’s appeal. Psychological Science, 26, 277–891. The abstract from this article explains more about the different experiments conducted (we will be specifically looking at the dataset from Experiment 4, courtesy of the Open Stats Lab): “A person’s mental capacities, such as intellect, cannot be observed directly and so are instead inferred from indirect cues. We predicted that a person’s intellect would be conveyed most strongly through a cue closely tied to actual thinking: his or her voice. Hypothetical employers (Experiments 1-3b) and professional recruiters (Experiment 4) watched, listened to, or read job candidates’ pitches about why they should be hired. These evaluators (the employers) rated a candidate as more competent, thoughtful, and intelligent when they heard a pitch rather than read it and, as a result, had a more favorable impression of the candidate and were more interested in hiring the candidate. Adding voice to written pitches, by having trained actors (Experiment 3a) or untrained adults (Experiment 3b) read them, produced the same results. Adding visual cues to audio pitches did not alter evaluations of the candidates. For conveying one’s intellect, it is important that one’s voice, quite literally, be heard.” To recap on Experiment 4, 39 professional recruiters from Fortune 500 companies evaluated job pitches of M.B.A. candidates from the University of Chicago Booth School of Business. The methods and results appear on pages 887–889 of the article if you want to look at them specifically for more details. The original data, in wide format, can be found at the Open Stats Lab website for later self-directed learning. Today however, we will be working with a modfied version in “tidy” format which can be downloaded from here. Likewise, if you are unsure about tidy format, refer back to the inclass activity of Lab 2. Today’s Goal! Our task today is to reproduce a figure and the results from the article (p. 887-888). The two packages you will need are tidyverse, which we have used a lot, and broom, which is new to you but will become your friend. One of the main functions we use in broom is broom::tidy() - this is an incredibly useful function that converts the output of an inferential test in R from a combination of text and lists, that are really hard to work with, into a table that you can then use much more easily. Might that be worth making a note of? We will show you how to do this today and then ask you to use it over the coming assignments. If you are using the Boyd Orr labs, broom is already installed and just needs called to the library(). If you are using your own machine, you will need to install it one time to begin with if you have never installed it before. 6.4.1 Task 1: Evaluators Open a new script and call broom and tidyverse into your library. Note: Order is important when calling multiple libraries - if two libraries have a function named the same thing, R will use the function from the library loaded in last. The file called evaluators.csv contains the demographics of the 39 raters. After downloading and unzipping the data, and of course setting the working directory, read in the information from this file and store it in a variable called evaluators. Now, use a line of code to calculate the overall mean and standard deviation of the age of the evaluators. Next, use a separate line of code to count up how many male and how many female evaluators there were in the study. Note that there are NAs in the data so you will need to include a call to na.rm = TRUE. Helpful Hint Remember to load the libraries you need! Also make sure you’ve downloaded and saved the data in the folder you’re working from. You can use summarise() and count() or a pipeline with group_by() to complete this task. When analysing the number of male and female evaluators it isn’t initially clear that ‘1’ represents males and ‘2’ represents females. We can use recode() to convert the numeric names to indicate something more meaningful. Have a look at?recode to see if you can work out how to use it. It’ll help to use mutate() to create a new variable to recode() the numeric names for evaluators. This website is also incredibly useful and one to save for anytime you need to use recode(): https://debruine.github.io/recode.html. For your own analysis and future reproducible analyses, it’s a good idea to make these representations clearer to others. Quickfire Questions From your results: What was the mean age of the evaluators in the study? Type in your answer to one decimal place: What was the standard deviation of the age of the evaluators in the study? Type in your answer to two decimal places: How many participants were noted as being female: How many participants were noted as being male: Group Discussion Point The paper claims that the mean age of the evaluators was 30.85 years (SD = 6.24) and that there were 9 male and 30 female evaluators. Do you agree? Why might there be differences? Explain This - Why is there a discrepancy? This paper claimed there were 9 males, however looking at your results you can see only 4 males, with 5 NA entries making up the rest of the participant count. It looks like the NA and male entries have been combined! That information might not be clear to a person re-analysing the data. This is why it’s important to have reproducible data analyses for others to examine. Having another pair of eyes examining your data can be very beneficial in spotting any discrepancies - this allows for critical evaluation of analyses and results and improves the quality of research being published. All the more reason to emphasize the importance of conducting replication studies! #ReproducibleScience 6.4.2 Task 2: Ratings We are now going to calculate an overall intellect rating given by each evaluator - how intellectual the evaluators thought candidates were overall depending on whether or not the evaluators read or listened to the candidates’ resume pitches. This is calculated by averaging the ratings of competent, thoughtful and intelligent for each evaluator; held within ratings.csv. Note: we are not looking at ratings to individual candidates; we are looking at overall ratings for each evaluator. This is a bit confusing but makes sense if you stop to think about it a little. We will then combine the overall intellect rating with the overall impression ratings and overall hire ratings for each evaluator, with the end goal of having a tibble called ratings2 - which has the following structure: ## # A tibble: 12 x 4 ## eval_id Category Rating condition ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 hire 6 listened ## 2 1 impression 7 listened ## 3 1 intellect 6 listened ## 4 2 hire 4 listened ## 5 2 impression 4.67 listened ## 6 2 intellect 5.67 listened ## 7 3 hire 5 listened ## 8 3 impression 8.33 listened ## 9 3 intellect 6 listened ## 10 4 hire 4 read ## 11 4 impression 4.67 read ## 12 4 intellect 3.33 read The following steps describe how to create the above tibble but you might want to have a bash yourself without reading them first. The trick when doing data analysis and data wrangling is to first think about what you want to achieve - the end goal - and then what function do I need to use. You know what you want to end up with - the above table - now how do you get there? Steps 1-3 calculate the new intellect rating. Steps 4 and 5 combine this rating to all other information. Load the data found in ratings.csv into a tibble called ratings. filter() only the relevant variables (thoughtful, competent, intelligent) into a new tibble (call it what you like - we use iratings), and calculate a mean Rating for each evaluator. Add on a new column called Category where every entry is the word intellect. This tells us that every number in this tibble is an intellect rating. Now create a new tibble called ratings2 and filter into it just the “impression” and “hire” ratings from the original ratings tibble. Next, bind this tibble with the tibble you created in step 3 to bring together the intellect, impression, and hire ratings, in ratings2. Join ratings2 with the evaluator tibble that we created in Task 1. Keep only the necessary columns as shown above and arrange by Evaluator and Category. Don’t forget to use the hints and the solution at the bottom if you are really stuck. Helpful Hint Make sure you’ve downloaded and saved the data into the folder you’re working from. filter(Category %in% c()) might work and then use group_by() and summarize() to calculate a mean Rating for each evaluator. Use mutate() to create a new column. bind_rows() from Lab 2 will help you to combine these variables from two separate tibbles. Use inner_join() with the common column in both tibbles. select() and arrange() will help you here too. 6.4.3 Task 3: Creating a Figure To recap, we now have ratings2 which contains an overall Rating score for each evaluator on the three Category (within: hire, impression, intellect) depending on which condition that evaluator was in (between: listened or read). Great! Now we have all the information we need to replicate Figure 7 in the article (page 888), shown here: Figure 6.2: Figure 7 from Schroeder and Epley (2015) which you should try to replicate. Replace the NULLs below to create a very basic version of this figure. You did something like this for the Lab 5 assignment and again in the Lab 3 Visualisation tasks. group_means &lt;- group_by(ratings2, NULL, NULL) %&gt;% summarise(Rating = mean(Rating)) ggplot(group_means, aes(NULL, NULL, fill = NULL)) + geom_col(position = &quot;dodge&quot;) Group Discussion Point Improve This Figure: Discuss with others how you could improve this plot. What other geom_() options could you try? Are barcharts that informative or would something else be better? How would you add or change the labels of your plot? Could you change the colours in your figure? Next, have a look at the possible solution below to see a modern way of presenting this information. There are some new functions in this solution that you should play about with to understand what they do. Remember it is a layering system, so remove lines and see what happens. Note how in the solution the Figure shows the raw data points as well as the means in each condition; this gives a better impression of the true data as just showing the means can be misleading. You can continue your further exploration of visualisations by reading this paper later when you have a chance: Weissberger et al., 2015, Beyond Bar and Line Graphs: Time for a New Data Presentation Paradigm Possible Solution group_means &lt;- ratings2 %&gt;% group_by(condition, Category) %&gt;% summarise(Rating = mean(Rating)) ggplot(ratings2, aes(condition, Rating, color = condition)) + geom_jitter(alpha = .2) + geom_violin(aes(fill = condition), alpha = .2) + facet_wrap(~Category) + geom_point(data = group_means, size = 2) + labs(x = &quot;Category&quot;, y = &quot;Recruiters&#39; Evaluation of Candidates&quot;) + coord_cartesian(ylim = c(0, 10), expand = FALSE) + guides(color = &quot;none&quot;, fill = &quot;none&quot;) + theme_bw() Figure 6.3: A possible alternative to Figure 7 6.4.4 Task 4: t-tests Brilliant! So far we have checked the descriptives and the last thing now is to check the inferential tests; the t-tests. You should still have ratings2 stored from Task 2. From this tibble, let’s reproduce the t-test results from the article and at the same time show you how to run a t-test. Refer back to the lectures to understand the maths behind it but essentially it is a measure between the difference in means over the variance about those means. Here is a paragraph from the paper describing the results (p. 887): “The pattern of evaluations by professional recruiters replicated the pattern observed in Experiments 1 through 3b (see Fig. 7). In particular, the recruiters believed that the job candidates had greater intellect—were more competent, thoughtful, and intelligent—when they listened to pitches (M = 5.63, SD = 1.61) than when they read pitches (M = 3.65, SD = 1.91), t(37) = 3.53, p &lt; .01, 95% CI of the difference = [0.85, 3.13], d = 1.16. The recruiters also formed more positive impressions of the candidates—rated them as more likeable and had a more positive and less negative impression of them—when they listened to pitches (M = 5.97, SD = 1.92) than when they read pitches (M = 4.07, SD = 2.23), t(37) = 2.85, p &lt; .01, 95% CI of the difference = [0.55, 3.24], d = 0.94. Finally, they also reported being more likely to hire the candidates when they listened to pitches (M = 4.71, SD = 2.26) than when they read the same pitches (M = 2.89, SD = 2.06), t(37) = 2.62, p &lt; .01, 95% CI of the difference = [0.41, 3.24], d = 0.86.” We are going to run the t-tests for Intellect, Hire and Impression; each time comparing evaluators overall ratings for the listened group versus overall ratings for the read group to see if there was a significant difference between the two conditions: i.e. did the evaluators who listened to pitches give a significant higher or lower rating than evaluators that read pitches. Portfolio Point - A vs B or B vs A in a t-test? Now would be a good time to add to your notes about what is the difference between a positive and negative value as the outcome to a t-test? Remember? It just tells you which group had the bigger mean - the absolute value will be the same. Most commonly, t-tests are reported as a positive value. You will need the function t.test() and you will use broom::tidy() to pull out the results from each t-test into a table. Below, we show you how to create the group means and then run the t-test for intellect. Run these lines and have a look at what they do. First we calculate the group means: group_means &lt;- ratings2 %&gt;% group_by(condition, Category) %&gt;% summarise(m = mean(Rating), sd = sd(Rating)) And we can call them and look at them by typing: group_means Now to just look at intellect ratings we need to filter them into a new tibble: intellect &lt;- filter(ratings2, Category == &quot;intellect&quot;) And then we run the actual t-test and tidy it into a table. t.test() requires two vectors as input pull() will pull out a single column from a tibble, e.g. Rating from intellect tidy() takes information from a test and turns it into a table. Try running the t.test with and without piping into tidy() to see what it does differently. intellect_t &lt;- t.test(intellect %&gt;% filter(condition == &quot;listened&quot;) %&gt;% pull(Rating), intellect %&gt;% filter(condition == &quot;read&quot;) %&gt;% pull(Rating), var.equal = TRUE) %&gt;% tidy() Now lets look at the intellect_ttibble we have created (assuming you piped into tidy()): Table 6.2: The t-test output of those in the intellect condition. estimate1 estimate2 statistic p.value parameter conf.low conf.high method alternative 5.635 3.648 3.526 0.001 37 0.845 3.128 Two Sample t-test two.sided From the resultant tibble, intellect_t, you can see that you ran a Two Sample t-test (meaning between) with a two sided hypothesis test (“two.sided”). The mean for the listened condition, estimate1, was 5.635, whilst the mean for the read condition, estimate2 was 3.648. So an overall difference of 1.987. The degrees of freedom, parameter, was 37. The t-value, statistic, was 3.526, and it was significant as the p-value, p.value, was 0.0011 (lower than alpha (\\(a\\)) = .05 - the Type 1 error rate). The t-test would be written up as: t(37) = 3.526, p = 0.001. Now: Run the remaining t-tests for hire and for impression. Store them in tibbles called hire_t and impress_t respectively. Bind the rows of intellect_t, hire_t and impress_t to create a table of the three t-tests called results. It should look like this: Table 6.3: Output of all three t-tests Category estimate1 estimate2 statistic p.value parameter conf.low conf.high method alternative intellect 5.635 3.648 3.526 0.001 37 0.845 3.128 Two Sample t-test two.sided hire 4.714 2.889 2.620 0.013 37 0.414 3.237 Two Sample t-test two.sided impression 5.968 4.074 2.851 0.007 37 0.548 3.240 Two Sample t-test two.sided Quickfire Questions Check your results for hire. Enter the mean estimates and t-test results (means and t-value to 2 decimal places, p-value to 3 decimal places): Mean estimate1 (listened condition) = Mean estimate2 (read condition) = t() = , p = Looking at this result, True or False, this result is significant at \\(\\alpha = .05\\)? TRUE FALSE Check your results for impression. Enter the mean estimates and t-test results (means and t-value to 2 decimal places, p-value to 3 decimal places): Meanestimate1 (listened condition) = Mean estimate2 (read condition) = t() = , p = Looking at this result, True or False, this result is significant at \\(\\alpha = .05\\)? TRUE FALSE Helpful Hint Your t-test answers should have the following structure: t(degrees of freedom) = t-value, p = p-value, where: degrees of freedom = parameter, t-value = statistic, and p-value = p.value. Remember that if a result has a p-value lower (i.e. smaller) than or equal to the alpha level then it is said to be significant. Job Done - Activity Complete! So to recap, we looked at the data from Schroeder and Epley (2015), both the descriptives and inferentials, we plotted a figure, and we confirmed that, as in the paper, there are significant differences in each of the three rating categories (hire, impression and intellect), with the listened condition receiving a higher rating than the read condition on each rating. All in, our interpretation would be that people rate you hire when they hear you speak your resume as opposed to them just reading your resume! Well done for completing this inclass activity on independent samples t-tests! Are there any useful points in this activity about t-tests or plots that you think could be useful to include in your portfolio? Make sure to include them now! Also, if you have more time, you might want to visit this website that will give you a better understanding of the relationship between the \\(t\\) distribution and the normal distribution: gallery.shinyapps.io/tdist You should now be ready to complete the Homework Assignment for this lab. The assignment for this Lab is FORMATIVE and is NOT to be submitted and will NOT count towards the overall grade for this module. However you are strongly encouraged to do the assignment as it will continue to boost your skills which you will need in future assignments. If you have any questions, please post them on the forums. 6.5 Assignment Lab 6: Independent samples t-test Assignment In order to complete this assignment, you first have to download the assignment .Rmd file which you need to edit for this assignment: titled GUID_Level2_Lab6.Rmd. This can be downloaded within a zip file from the link below. Once downloaded and unzipped you should create a new folder that you will use as your working directory; put the .Rmd file in that folder and set your working directory to that folder through the drop-down menus at the top. Download the Assignment .zip file from here or on Moodle. Background For this assignment we will be using real data from the following paper: Nave, G., Nadler, A., Zava, D., and Camerer, C. (2017). Single-dose testosterone administration impairs cognitive reflection in men. Psychological Science, 28, 1398–1407. The full data for these exercises can be downloaded from the Open Science Framework repository but for this assignment we will just use the .csv file in the zipped folder: CRT_Data.csv. You may also want to read the paper, at least in part, to help fully understand this analysis if at times you are unsure. Here is the article’s abstract: In nonhumans, the sex steroid testosterone regulates reproductive behaviors such as fighting between males and mating. In humans, correlational studies have linked testosterone with aggression and disorders associated with poor impulse control, but the neuropsychological processes at work are poorly understood. Building on a dual-process framework, we propose a mechanism underlying testosterone’s behavioral effects in humans: reduction in cognitive reflection. In the largest study of behavioral effects of testosterone administration to date, 243 men received either testosterone or placebo and took the Cognitive Reflection Test (CRT), which estimates the capacity to override incorrect intuitive judgments with deliberate correct responses. Testosterone administration reduced CRT scores. The effect remained after we controlled for age, mood, math skills, whether participants believed they had received the placebo or testosterone, and the effects of 14 additional hormones, and it held for each of the CRT questions in isolation. Our findings suggest a mechanism underlying testosterone’s diverse effects on humans’ judgments and decision making and provide novel, clear, and testable predictions. The critical findings are presented on p. 1403 of the paper under the heading The influence of testosterone on CRT performance. Your task today is to attempt to try and reproduce some of the main results from the paper. NOTE: Being unable to get the exact same results as the authors doesn’t necessarily mean you are wrong! The authors might be wrong, or might have left out important details. Present what you find. Before starting lets check: The .csv file is saved into a folder on your computer and you have manually set this folder as your working directory. The .Rmd file is saved in the same folder as the .csv files. For assessments we ask that you save it with the format GUID_Level2_Lab6.Rmd where GUID is replaced with your GUID. Though this is a formative assessment, it may be good practice to do the same here. 6.5.1 Task 1A: Libraries In today’s assignment you will need both the tidyverse and broom packages. Enter code into the t1A code chunk below to load in both of these libraries. ## load in the tidyverse and broom packages 6.5.2 Task 1B: Loading in the data Use read_csv() to replace the NULL in the t1B code chunk below to load in the data stored in the datafile CRT_Data.csv. Store the data in the variable crt. Do not change the filename of the datafile. crt &lt;- NULL 6.5.3 Task 2: Selecting only relevant columns Have a look at crt. There are three variables in crt that you will need to find and extract in order to perform the t-test: the subject ID number (hint: each participant has a unique number); the independent variable (hint: each participant has the possibility of being in one of two treatments coded as 1 or 0); and the dependent variable (hint: the test specifically looks at which answers people get correct). Identify those three variables. It might help to look at the first few sentences under the heading The influence of testosterone on CRT performance and Figure 2a in the paper for further guidance on the correct variables. Having identified the important three columns, replace the NULL in the t2 code chunk below to select out only those three columns from crt and store them in the tibble crt2. Check your work: If correct, crt2 should be a tibble with 3 columns and 243 rows. crt2 &lt;- NULL NOTE: For the remainder, of this assignment you should use crt2 as the main source tibble and not crt. 6.5.4 Task 3: Verify the number of subjects in each group The Participants section of the article contains the following statement: 243 men (mostly college students; for demographic details, see Table S1 in the Supplemental Material available online) were randomly administered a topical gel containing either testosterone (n = 125) or placebo (n = 118). In the t3 code block below, replace the NULLs with lines of code to calculate: The number of men in each Treatment. This should be a tibble called cond_counts containing a column called Treatment showing the two groups and a column called n which shows the number of men in each group. The total number of men in the sample. This should be a single value, not a tibble, and should be stored in n_men. You know the answer to both of these tasks already. Make sure that your code gives the correct answer! cond_counts &lt;- NULL n_men &lt;- NULL Now replace the strings in the statements below, using inline R code, so that it reproduces the sentence from the paper exactly as it is shown above. In other words, in the statement below, anywhere it says &quot;(your code here)&quot;, replace that string (including the quotes), with inline R code. To clarify, when looking at the .Rmd file you should see R code, but when looking at the knitted file, you should see values. Look back at Lab 1 if you are unsure of how to use inline code. Hint: One solution is to do something with cond_counts similar to what we did with filter() and pull() in the in-class exercises for lab 6. &quot;(your code here)&quot; men (mostly college students; for demographic details, see Table S1 in the Supplemental Material available online) were randomly administered a topical gel containing either testosterone (n = &quot;(your code here)&quot;) or placebo (n = &quot;(your code here)&quot;). 6.5.5 Task 4: Reproduce Figure 2a Here is Figure 2A from the original paper: Figure 6.4: Figure 2A from Nave, Nadler, Zava, and Camerer (2017) which you should replicate Write code in the t4 code chunk to reproduce a version of Figure 2a - shown above. Before you create the plot, replace the NULL to make a table called crt_means with the mean and standard deviation of the number of CorrectAnswers for each group. Use crt_means as the source data for the plot. Hint: you will need to check out recode() to get the labels of treatments right. Again this webpage is highly recommended: https://debruine.github.io/posts/recode/ Don’t worry about including the error bars (unless you want to) or the line indicating significance in the plot. Do however make sure to pay attention to the labels of treatments and of the y-axis scale and label. Reposition the x-axis label to below the Figure. You can use colour if you like. crt_means &lt;- NULL ## TODO: add lines of code using ggplot 6.5.6 Task 5: Interpreting your Figure Always good to do a slight recap at this point to make sure you are following the analysis. Replace the NULL in the t5 code chunk below with the number of the statement that best describes the data you have calculated and plotted thus far. Store this single value in answer_t5: The Testosterone group (M = 2.10, SD = 1.02) would appear to have fewer correct answers on average than the Placebo group (M = 1.66, SD = 1.18) on the Cognitive Reflection Test suggesting that testosterone does in fact inhibit the ability to override incorrect intuitive judgements with the correct response. The Testosterone group (M = 1.66, SD = 1.18) would appear to have more correct answers on average than the Placebo group (M = 2.10, SD = 1.02) on the Cognitive Reflection Test suggesting that testosterone does in fact inhibit the ability to override incorrect intuitive judgements with the correct response. The Testosterone group (M = 1.66, SD = 1.18) would appear to have fewer correct answers on average than the Placebo group (M = 2.10, SD = 1.02) on the Cognitive Reflection Test suggesting that testosterone does in fact inhibit the ability to override incorrect intuitive judgements with the correct response. The Testosterone group (M = 2.10, SD = 1.02) would appear to have more correct answers on average than the Placebo group (M = 1.66, SD = 1.18) on the Cognitive Reflection Test suggesting that testosterone does in fact inhibit the ability to override incorrect intuitive judgements with the correct response. answer_t5 &lt;- NULL 6.5.7 Task 6: t-test Now that we have calculated the descriptives in our study we need to run the inferentials. In the t6 code chunk below, replace the NULL with a line of code to run the t-test taking care to make sure that the output table has the Placebo mean under Estimate1 (group 0) and Testosterone mean under Estimate2 (group 1). Assume variance is equal and use broom::tidy() to sweep and store the results into a data frame called t_table. t_table &lt;- NULL 6.5.8 Task 7: Reporting results In the t7A code chunk below, replace the NULL with a line of code to pull out the df from t_table. This must be a single value stored in t_df. t_df &lt;- NULL In the t7B code chunk below, replace the NULL with a line of code to pull out the t-value from t_table. Round it to three decimal places. This must be a single value stored in t_value. t_value &lt;- NULL In the t7C code chunk below, replace the NULL with a line of code to pull out the p-value from t_table. Round it to three decimal places. This must be a single value stored in p_value. p_value &lt;- NULL In the t7D code chunk below, replace the NULL with a line of code to calculate the absolute difference between the mean number of correct answers for the Testosterone group and the Placebo group. Round it to three decimal places. This must be a single value stored in t_diff. t_diff &lt;- NULL If you have completed t7A to t7D accurately, then when knitted, one of these statements below will produce an accurate and coherent summary of the results. In the t7E code chunk below, replace the NULL with the number of the statement below that best summarises the data in this study. Store this single value in answer_t7e The testosterone group performed significantly better ( fewer correct answers) than the placebo group, t() = , p = . The testosterone group performed significantly worse ( fewer correct answers) than the placebo group, t() = , p = . The testosterone group performed significantly better ( more correct answers) than the placebo group, t() = , p = . The testosterone group performed significantly worse ( fewer correct answers) than the placebo group, t() = , p = . answer_t7e &lt;- NULL Job Done - Activity Complete! Well done, you are finshed! Now you should go check your answers against the solution file which can be found on Moodle. You are looking to check that the resulting output from the answers that you have submitted are exactly the same as the output in the solution - for example, remember that a single value is not the same as a coded answer. Where there are alternative answers, it means that you could have submitted any one of the options as they should all return the same answer. If you have any questions please post them on the moodle forum or on the rguppies.slack.com forum #level2_2018. See you in the next lab! 6.6 Solutions to Questions Below you will find the solutions to the questions for the Activities for this chapter. Only look at them after giving the questions a good try and speaking to the tutor about any issues. 6.6.1 PreClass Activities 6.6.1.1 PreClass Task 1 ns_data &lt;- tibble(participant = 1:22, valid_rt = c(631.2,800.8,595.4,502.6,604.5, 516.9,658.0,502.0,496.7,600.3, 714.6,623.7,634.5,724.9,815.7, 456.9,703.4,647.5,657.9,613.2, 585.4,674.1)) Return to Task 6.6.1.2 PreClass Task 2 woods_mean &lt;- 590 n_participants &lt;- ns_data %&gt;% filter(valid_rt &gt; woods_mean) %&gt;% nrow() Giving an n_participants value of 16 Return to Task 6.6.1.3 PreClass Task 3 You can use the density function: sum(dbinom(n_participants:nrow(ns_data), nrow(ns_data), .5)) ## [1] 0.0262394 Or, the cumulative probability function: pbinom(n_participants - 1L, nrow(ns_data), .5, lower.tail = FALSE) ## [1] 0.0262394 Or, If you were to plug in the numbers directly into the code: sum(dbinom(16:22,22, .5)) ## [1] 0.0262394 Or, finally, remembering we need to specify a value lower than our minimum participant number as lower.tail = FALSE. pbinom(15, 22, .5, lower.tail = FALSE) ## [1] 0.0262394 It is better practice to use the first two solutions, which pull the values straight from ns_data, as you run the risk of entering an error into your code if you plug in the values manually. Return to Task 6.6.1.4 PreClass Task 4 For ns_data_mean use summarise() to calculate the mean and then pull() the value. For ns_data_sd use summarise() to calculate the sd and then pull() the value. # the mean ns_data_mean &lt;- ns_data %&gt;% summarise(m = mean(valid_rt)) %&gt;% pull(m) # the sd ns_data_sd &lt;- ns_data %&gt;% summarise(sd = sd(valid_rt)) %&gt;% pull(sd) NOTE: You could print them out on the screen if you wanted to “” is the end of line symbol so that they print on different lines cat(&quot;The mean number of hours was&quot;, ns_data_mean, &quot;\\n&quot;) cat(&quot;The standard deviation was&quot;, ns_data_sd, &quot;\\n&quot;) ## The mean number of hours was 625.4636 ## The standard deviation was 94.30693 Return to Task 6.6.1.5 PreClass Task 5 t_obs &lt;- (ns_data_mean - woods_mean) / (ns_data_sd / sqrt(nrow(ns_data))) Giving a t_obs value of 1.7638067 Return to Task 6.6.1.6 PreClass Task 6 If using values straight from ns_data, and multiplying by 2 for a two-tailed test, you would do the following: pval &lt;- pt(abs(t_obs), nrow(ns_data) - 1L, lower.tail = FALSE) * 2L Giving a pval of 0.0923092 But you can also get the same answer by plugging the values in yourself - though this method runs the risk of error and you are better off using the first calculation as those values come straight from ns_data. : pval2 &lt;- pt(t_obs, 21, lower.tail = FALSE) * 2 Giving a pval of 0.0923092 Return to Task 6.6.1.7 PreClass Task 7 The t-test would be run as follows, with the output shown below: t.test(pull(ns_data, valid_rt), mu = woods_mean) ## ## One Sample t-test ## ## data: pull(ns_data, valid_rt) ## t = 1.7638, df = 21, p-value = 0.09231 ## alternative hypothesis: true mean is not equal to 590 ## 95 percent confidence interval: ## 583.6503 667.2770 ## sample estimates: ## mean of x ## 625.4636 Return to Task 6.6.2 InClass Activities 6.6.2.1 InClass Task 1 library(&quot;tidyverse&quot;) library(&quot;broom&quot;) # you&#39;ll need broom::tidy() later evaluators &lt;- read_csv(&quot;evaluators.csv&quot;) evaluators %&gt;% summarize(mean_age = mean(age, na.rm = TRUE)) evaluators %&gt;% count(sex) # If using `recode()`: evaluators %&gt;% count(sex) %&gt;% mutate(sex_names = recode(sex, &quot;1&quot; = &quot;male&quot;, &quot;2&quot; = &quot;female&quot;)) Return to Task 6.6.2.2 InClass Task 2 load in the data ratings &lt;- read_csv(&quot;ratings.csv&quot;) First pull out the ratings associated with intellect iratings &lt;- ratings %&gt;% filter(Category %in% c(&quot;competent&quot;, &quot;thoughtful&quot;, &quot;intelligent&quot;)) Next calculate means for each evaluator imeans &lt;- iratings %&gt;% group_by(eval_id) %&gt;% summarise(Rating = mean(Rating)) Mutate on the Category variable. This way we can combine with ‘impression’ and ‘hire’ into a single table which will be very useful! imeans2 &lt;- imeans %&gt;% mutate(Category = &quot;intellect&quot;) And then combine all the information in to one single tibble. ratings2 &lt;- ratings %&gt;% filter(Category %in% c(&quot;impression&quot;, &quot;hire&quot;)) %&gt;% bind_rows(imeans2) %&gt;% inner_join(evaluators, &quot;eval_id&quot;) %&gt;% select(-age, -sex) %&gt;% arrange(eval_id, Category) Return to Task 6.6.2.3 InClass Task 4 First we calculate the group means: group_means &lt;- ratings2 %&gt;% group_by(condition, Category) %&gt;% summarise(m = mean(Rating), sd = sd(Rating)) And we can call them and look at them by typing: group_means Now to just look at intellect ratings we need to filter them into a new tibble: intellect &lt;- filter(ratings2, Category == &quot;intellect&quot;) And then we run the actual t-test and tidy it into a table. t.test() requires two vectors as input pull() will pull out a single column from a tibble, e.g. Rating from intellect tidy() takes information from a test and turns it into a table. Try running the t.test with and without piping into tidy() to see what it does differently. intellect_t &lt;- t.test(intellect %&gt;% filter(condition == &quot;listened&quot;) %&gt;% pull(Rating), intellect %&gt;% filter(condition == &quot;read&quot;) %&gt;% pull(Rating), var.equal = TRUE) %&gt;% tidy() Now we repeat for HIRE and IMPRESSION hire &lt;- filter(ratings2, Category == &quot;hire&quot;) hire_t &lt;- t.test(hire %&gt;% filter(condition == &quot;listened&quot;) %&gt;% pull(Rating), hire %&gt;% filter(condition == &quot;read&quot;) %&gt;% pull(Rating), var.equal = TRUE) %&gt;% tidy() And for Impression impress &lt;- filter(ratings2, Category == &quot;impression&quot;) impress_t &lt;- t.test(impress %&gt;% filter(condition == &quot;listened&quot;) %&gt;% pull(Rating), impress %&gt;% filter(condition == &quot;read&quot;) %&gt;% pull(Rating), var.equal = TRUE) %&gt;% tidy() Before combining all into one table showing all three t-tests results &lt;- bind_rows(&quot;hire&quot; = hire_t, &quot;impression&quot; = impress_t, &quot;intellect&quot; = intellect_t, .id = &quot;id&quot;) results Return to Task 6.6.2.4 Going Further with your coding An alternative solution to Task 4: There is actually a quicker way to do this analysis of three t-tests which you can have a look at below if you have the time. This uses very advanced coding with some functions we won’t really cover in level 2. Do not worry if you can’t quite follow it though; the main thing is to understand what we covered in the lab - the outcome is the same. ratings2 %&gt;% group_by(Category) %&gt;% nest() %&gt;% mutate(ttest = map(data, function(x) { t.test(Rating ~ condition, x, var.equal = TRUE) %&gt;% tidy() })) %&gt;% select(Category, ttest) %&gt;% unnest() ## Warning: `cols` is now required. ## Please use `cols = c(ttest)` ## # A tibble: 3 x 10 ## # Groups: Category [3] ## Category estimate1 estimate2 statistic p.value parameter conf.low ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 hire 4.71 2.89 2.62 0.0127 37 0.414 ## 2 impress~ 5.97 4.07 2.85 0.00709 37 0.548 ## 3 intelle~ 5.63 3.65 3.53 0.00114 37 0.845 ## # ... with 3 more variables: conf.high &lt;dbl&gt;, method &lt;chr&gt;, ## # alternative &lt;chr&gt; 6.6.3 Homework Activity 6.6.3.1 Assignment Task 1A: Libraries library(broom) library(tidyverse) Return to Task 6.6.3.2 Assignment Task 1B: Loading in the data Use read_csv() to read in data! crt &lt;- read_csv(&quot;data/06-s01/homework/CRT_Data.csv&quot;) crt &lt;- read_csv(&quot;CRT_Data.csv&quot;) Return to Task 6.6.3.3 Assignment Task 2: Selecting only relevant columns The key columns are: ID Treatment CorrectAnswers Creating crt2 which is a tibble with 3 columns and 243 rows. crt2 &lt;- select(crt, ID, Treatment, CorrectAnswers) Return to Task 6.6.3.4 Assignment Task 3: Verify the number of subjects in each group The Participants section of the article contains the following statement: 243 men (mostly college students; for demographic details, see Table S1 in the Supplemental Material available online) were randomly administered a topical gel containing either testosterone (n = 125) or placebo (n = 118). In the t3 code block below, replace the NULLs with lines of code to calculate: The number of men in each Treatment. This should be a tibble/table called cond_counts containing a column called Treatment showing the two groups and a column called n which shows the number of men in each group. The total number of men in the sample. This should be a single value, not a tibble/table, and should be stored in n_men. You know the answer to both of these tasks already. Make sure that your code gives the correct answer! For cond_counts, you could do: cond_counts &lt;- crt2 %&gt;% group_by(Treatment) %&gt;% summarise(n = n()) Or alternatively cond_counts &lt;- crt2 %&gt;% count(Treatment) For n_men, you could do: n_men &lt;- crt2 %&gt;% summarise(n = n()) %&gt;% pull(n) Or alternatively n_men &lt;- nrow(crt2) Solution: When formatted with inline R code as below: `r n_men` men (mostly college students; for demographic details, see Table S1 in the Supplemental Material available online) were randomly administered a topical gel containing either testosterone (n = `r cond_counts %&gt;% filter(Treatment == 1) %&gt;% pull(n)`) or placebo (n = `r cond_counts %&gt;% filter(Treatment == 0) %&gt;% pull(n)`). should give: 243 men (mostly college students; for demographic details, see Table S1 in the Supplemental Material available online) were randomly administered a topical gel containing either testosterone (n = 125) or placebo (n = 118). Return to Task 6.6.3.5 Assignment Task 4: Reproduce Figure 2A You could produce a good representation of Figure 2A with the following approach: crt_means &lt;- crt2 %&gt;% group_by(Treatment) %&gt;% summarise(m = mean(CorrectAnswers), sd = sd(CorrectAnswers)) %&gt;% mutate(Treatment = recode(Treatment, &quot;0&quot; = &quot;Placebo&quot;, &quot;1&quot; = &quot;Testosterone Group&quot;)) ggplot(crt_means, aes(Treatment, m, fill = Treatment)) + geom_col() + theme_classic() + labs(x = &quot;CRT&quot;, y = &quot;Number of Correct Answers&quot;) + guides(fill = &quot;none&quot;) + scale_fill_manual(values = c(&quot;#EEEEEE&quot;,&quot;#AAAAAA&quot;)) + coord_cartesian(ylim = c(1.4,2.4), expand = TRUE) Figure 6.5: A representation of Figure 2A Return to Task 6.6.3.6 Assignment Task 5: Interpreting your Figure Option 3 is the correct answer given that: The Testosterone group (M = 1.66, SD = 1.18) would appear to have fewer correct answers on average than the Placebo group (M = 2.10, SD = 1.02) on the Cognitive Reflection Test suggesting that testosterone does in fact inhibit the ability to override incorrect intuitive judgements with the correct response. answer_t5 &lt;- 3 Return to Task 6.6.3.7 Assignment Task 6: t-test You need to pay attention to the order when using this first approach, making sure that the 0 group are entered first. This will put the Placebo groups as Estimate1 in the output. In reality it does not change the values, but the key thing is that if you were to pass this code on to someone, and they expect Placebo to be Estimate1, then you need to make sure you coded it that way. t_table &lt;- t.test(crt2 %&gt;% filter(Treatment == 0) %&gt;% pull(CorrectAnswers), crt2 %&gt;% filter(Treatment == 1) %&gt;% pull(CorrectAnswers), var.equal = TRUE) %&gt;% tidy() Alternatively this formula approach would also work. t_table &lt;- t.test(CorrectAnswers ~ Treatment, data = crt2, var.equal = TRUE) %&gt;% tidy() Return to Task 6.6.3.8 Assignment Task 7: Reporting results The degrees of freedom (df) is found under parameter t_df &lt;- t_table$parameter The t-value is found under statistic t_value &lt;- t_table$statistic %&gt;% round(3) The p-value is found under p.value p_value &lt;- t_table$p.value %&gt;% round(3) The absolute difference between the two means can be calculated as follows: t_diff &lt;- (t_table$estimate1 - t_table$estimate2) %&gt;% round(3) %&gt;% abs() If you have completed t7A to t7D accurately, then when knitted, Option 4 would be stated as such The testosterone group performed significantly worse (0.438 fewer correct answers) than the placebo group, t(241) = 3.074, p = 0.002 and would therefore be the correct answer! answer_t7e &lt;- 4 Return to Task Chapter Complete! "],
["within-subjects-t-test.html", "Lab 7 Within-Subjects t-test 7.1 Overview 7.2 PreClass Activity 7.3 InClass Activity 7.4 Assignment 7.5 Solutions to Questions", " Lab 7 Within-Subjects t-test 7.1 Overview In the previous labs, we have looked at one-sample t-tests and between-samples t-tests. For today’s activities we are going to look at the remaining type of t-test; the within-subjects t-test (sometimes called the dependent sample or paired sample t-test). The within-subjects t-test is a statistical procedure used to determine whether the mean difference between two sets of observations from the same participants is zero. As in all tests, the within-subjects t-test has two competing hypotheses: the null hypothesis and the alternative hypothesis. The null hypothesis assumes that the true mean difference between the paired samples is zero. The alternative hypothesis assumes that the true mean difference between the paired samples is not equal to zero. Again like in all tests, and particularly parametric tests, the within-subjects t-test makes a number of assumptions: All participants appear in both conditions/groups. The dependent variable must be continuous (interval/ratio). The dependent variable should be normally distributed. Before beginning any analysis it is always a good idea to check if the data deviates from these assumptions and whether it contains any outliers, in order to assess the quality of the results. Portfolio Point - The assumption of variance One of the main differences in the assumptions between a between-subjects t-test and a within-subjects t-test is the assumption of equal variance. The between-subjects t-test classically would have an assumption of equal variance. Or at least what is known as the Student’s t-test has the assumption of equal variance. The top half of the equation of the formula (the numerator) is the difference in means of the two samples whereas the denominator relates to the variance of both groups. As such, in the Student’s t-test, if the variances are unequal then you can get erroneous results. Modern thinking however, and this is from your preclass reading, suggests that it is always better to run a Welch’s t-test for between-subjects where the assumption is that variance is not equal. From now on, unless stated otherwise, you should run a Welch’s t-test. In practice: to run a Student’s t-test you set var.equal = TRUE to run a Welch’s t-test you set var.equal = FALSE Conversely, there is no concern with variance in a within-subjects t-test because, as you will know from lectures, the top half of the equation of the formula (the numerator) is the mean difference between the two conditions, and so it is only one set of values and there is nothing to equate it to. This test does not have any concern with variance. 7.2 PreClass Activity A bit of a change of pace in this PreClass Activity. In order to give you a bit more of an understanding of the between-subjects t-test, and a viable alternative to the standard Student’s t-test, we ask that you read the following blog (and even the full paper if you have time) and then try out the couple of tasks below. 7.2.1 Reading Read the following blog on using Welch’s t-test. Blog: Always use Welch’s t-test instead of Student’s t-test by Daniel Lakens. For further reading you can look at the paper that resulted from this blog: Paper: Delacre, M., Lakens, D., &amp; Leys, C. (in press) Why Psychologists Should by Default Use Welch’s t-test Instead of Student’s t-test. International Review of Social Psychology. 7.2.2 Task Copy the script within the blog into an R script and try running it to see the difference between Welch’s t-test (the recommended in the blog) and Student’s t-test (the standard in the field). Note: You will need the car package. This is installed already in the Boyd Orr labs so if doing this in the labs, do not install the package, just call it to the library with library(car). Don’t worry if you don’t yet understand all the code. It is highly commented but it is tricky. The key thing is to try and run it and to look at the figures that come out of it - particularly the third one that you see in the blog, the one with the red line on it that compares p-values in the two tests. Look at how many tests (dots) are significant on one test and not the other. Change the values for n1, n2, sd1 and sd2 at the top of the script to see what effect this has on the Type 1 Error rate (alpha = .05). Again look at the figure with the red line, comparing significance on one test versus significance on the other. This is what should change depending on the n of each sample and whether the variance is equal or not. Think about the overall point of this blog and which test we should use when conducting a t-test. We will look at this more in the lectures and labs. Job Done - Activity Complete! That’s it for today! This is a bit of a change to the PreClass activities you have done so far, and you will start to see this approach more in Semester 2 - reading blogs and chapters. Don’t forget though that it is really important to store the information in your own words, to help you really understand it, so you might want to go back and add any informative points to your Portfolio. Post any questions on the Moodle forum or the slack forum rguppies.slack.com under the channel #level2_2018. See you in the lab! 7.3 InClass Activity Juror Decision Making: Does the order of information affect juror judgements of guilt or innocence? For this activity we will look at a replication of Furnham (1986) that the School of Psychology, University of Glasgow, carried out in 2016 - 2017. It would be worth familiarising yourself with the original study at some point for more information regarding the concepts of the study, but it is not essential in order to complete the assignment: Furnham, A. (1986), The Robustness of the Recency Effect: Studies Using Legal Evidence The overall aim of the original experiment was to investigate whether the decision a jury member makes about the innocence or guilt of a defendant could be influenced by something as simple as when crucial evidence is presented during a trial. During the experiment participants (Level 2 Psychology students) listened to a series of recordings that recreated the 1804 trial of a man known as Joseph Parker who was accused of assuming two identities and marrying two women; i.e. bigamy. Each participant listened to the same recordings of evidence, presented by both prosecution and defence witnesses, and were asked to judge how guilty they thought Mr. Parker was at 14 different points during the experiment on a scale of 1 to 9: 1 being innocent and 9 being guilty. The manipulation in the experiment was that the order of evidence was altered so that half the participants received one order and the other half received the second order. Key to the order change was the time at which a critical piece of evidence was presented. This critical evidence proved that the defendant was innocent. The middle group heard this evidence at Timepoint 9 of the trial whereas the late group heard this evidence at Timepoint 13. You will have an opportunity to look at all the data in due course but, for today’s exercise, we will only focus on the late group. In this exercise, your task is to analyse the data to examine whether the participants’ ratings of guilt significantly changed before and after the presentation of the critical evidence in the late condition. If the critical evidence, which proved the defendant’s innocence, had the desired effect then you should see a significant drop in ratings of guilt after hearing this evidence (Timepoint 13) compared to before (Timepoint 12). Or in other words, we hypothesised that there would be a significant decrease in ratings of guilt, caused by presentation of the critical evidence, from Timepoint 12 to Timepoint 13. 7.3.1 Task 1: Load the Data Download the data for this experiment from here or from Moodle. Unzip the data and save it into a folder you have access to and set that folder as your working directory. Open a new script. Today you will need the broom and tidyverse libraries. Load these in this order. Remember order matters. Using read_csv(), load in the data from the experiment contained in GuiltJudgements.csv and store it in a tibble called ratings. 7.3.2 Task 2: Wrangle the Data As above, you are only interested in the Late group for this assignment and only for Timepoints 12 (rating before key evidence) and 13 (rating after key evidence). But having had a look at ratings you will see that the Timepoints are in wide format (columns 1 to 14 - each a different timepoint) and the Evidence column contains the Middle group as well. Hmmmm! filter() only those participants from the Late condition. select() only the Timepoints 12 and 13. rename() these Timepoints as Twelve and Thirteen as numerical names are hard to deal with. Leave it in wide format because you will need this for the t.test() function for paired data. Calculate a difference score diff for each participant (Twelve minus Thirteen). Do this all as one pipe and store it in a tibble called lates. Check that your table looks like the table below. Table 7.1: How your table should look from Task 2 Participant Evidence Twelve Thirteen diff 1 Late 7 5 2 2 Late 5 3 2 3 Late 5 2 3 4 Late 4 4 0 Helpful Hint You need to specify the column you want to filter from, stating which variable (i.e. Late) that this column is ‘equal to’ (i.e. ‘==’) Other than the two columns representing Timepoints 12 and 13, there are two other columns you need to keep in order to identify the participant and group. Use the table as a guide. When renaming, first state the new variable name and then designate this to the old variable name. i.e. rename(data, new_column_name = old_column_name). If the old column is a number, put it in back ticks e.g. Five = 5. Use mutate(diff = …?) in your pipeline. Quickfire Questions To check you have completed this Task correctly, enter the appropriate values into the boxes. This dataset has: columns by rows. 7.3.3 Task 3: Look at the Histogram for Normality Before running an inferential analysis, we need to check the assumption of normality. Because the within-subject t-test is based on difference scores, we should calculate the difference scores and check that the distribution of these scores is approximately Normal. In this task we will do it through creating a histogram for the difference scores. Create a histogram and set the binwidth to something reasonable for this experiment. Helpful Hint ggplot() + geom_? A histogram only requires you to state ‘x’ and not ‘y’. We are examining the differences in guilt rating scores across participants. Which column from lates should be ‘x’? binwidth is an argument you can specify within geom_histogram(). Think about an appropriate binwidth. Your guilt rating scale runs from 1 to 9 in increments of 1. Beyond this point, you can think about adding appropriate labels and color if you like. 7.3.4 Task 4: A Boxplot of Outliers We can also check for outliers on the difference scores. Create a boxplot of the difference scores. Helpful Hint This time when using ggplot() to create a boxplot, you need to specify both ‘x’, which is the discrete/categorical variable, and ‘y’, which is the continuous variable. geom_boxplot() - see lab 3 for an example. Quickfire Questions How many outliers do you see? 0 1 2 3 too many to count Remember that outliers are represented as dots or stars beyond the whiskers of the boxplot. We won’t deal with outliers today but it would be worth thinking about how you could deal with them in future. We will now run some descriptives to start understanding the relationship between the two levels of interest: Timepoint 12 and Timepoint 13. 7.3.5 Task 5: Calculating Descriptives Calculate the mean, standard deviation, and Lower and Upper values of the 95% Confidence Interval for both levels of the Independent Variable (the two timepoints). You will need to also calculate the n() and the Standard Error to complete this task. Store all this data in a variable called descriptives. **Note: calculating the descriptives will be easier if you use gather() to reshape the Twelve and Thirteen columns to long format, creating new key field Timepoint to hold the values Twelve and Thirteen and value field GuiltRating to hold the responses. But before you gather() remove the diff column from lates since we won’t be working with this for the descriptives. Helpful Hint select(lates, -diff) %&gt;% gather(Timepoint, GuiltRating, …???) group_by() the categorical column Timepoint. This is the column you want to compare groups for. summarise() Different calculations can be used within the same summarise() function as long as they are calculated in the order which you require them. For example, you first need to calculate the participant number, n = n(), and the standard deviation, sd = sd(variable), in order to calculate the standard error, se = sd/sqrt(n), which is required to calculate your Confidence Intervals. For the 95% Confidence Interval, you need to calculate a LowerCI and an UpperCI using the appropriate formula. Quickfire Questions From the options, which equation would use to calculate the LowerCI? mean - 1.96 * sd mean * 1.96 - se mean - 1.96 * se mean * 1.96 - sd From the options, which equation would use to calculate the UpperCI? mean + 1.96 * sd mean * 1.96 + se mean + 1.96 * se mean * 1.96 + sd Portfolio Point - What is 1.96? For data that is normally distributed, you’re looking to calculate the 95% Confidence Interval (meaning there is a 95% probability that a data point lies within this specified parameter). To do this you require a z-score which tells you how many standard deviations you are from the mean. 95% of the area under a normal distribution curve lies within 1.96 standard deviations from the mean. If you were looking to calculate a 99% Confidence Interval you would instead use a z-score of 2.576. This takes into account a greater area under the normal distribution curve and so you are further away from the mean (i.e. closer to the tail ends of the curve), resulting in a higher z-score. 7.3.6 Task 6: Visualising Means and Descriptives Using the data in descriptives, produce a plot that visualises the mean and 95% Confidence Intervals. One way would be a basic barplot, shown in previous labs, with error bars indicating the 95% CI. To add the error bars you could add a line like below. Feel free to embellish the figure as you see fit. geom_errorbar(aes(ymin = LowerCI, ymax = UpperCI), position = &quot;dodge&quot;, width = .15) Helpful Hint We recommend using geom_col() Remember to add (+) the geom_errorbar() line above to your code! Don’t pipe it. In the above code for error bars, the aesthetic, aes(), allows you to set the min and max values. position = “dodge” does the same as position = position_dodge() and position = position_dodge(width = .9). There are a number of ways to use a position call and they all do the same thing. Important to remember: as we have mentioned in previous labs, barplots are not that informative in themselves. Going ahead in your research, keep in mind that you should look to use plots that incorporate a good indication of the distribution/spread of the individual data points as well. Group Discussion Point Think back to the hypothesis. We hypothesised that there would be a significant decrease in ratings of guilt, caused by presentation of the critical evidence, from Timepoint 12 to Timepoint 13. Spend a few minutes talking to your group about whether you think there will be a significant difference between the two timepoints. What evidence do you have? Think about the overlap of confidence intervals! Remember the key thing at this stage is that it is a subjective impression - “It appears that there might be….” or words to that effect. 7.3.7 Task 7: The t-test Now we have checked our assumptions and ran our dscriptives, the last thing we need to do is to perform the within-subjects t-test to test the differences between the time points. To perform the within-subjects t-test you use the same t.test function as you did in Lab 6. However, this time you have to do a few things differently. If you look at the help for t.test (?t.test) you can see that there are two different methods for calling t.test: The default method, with syntax t.test(x, y = NULL, ...); The “formula” method, with syntax t.test(formula, data, ...). The documentation is not clear on this, but you can only use the formula method for an independent-samples t-test. If you want to do a paired t-test, then you have to use the default method, specifying two vectors x and y and setting the paired argument to TRUE. The x and y vectors should have the participants’ data in the “Twelve” and “Thirteen” conditions in the same order, which we can ensure by passing the column Twelve as x and Thirteen as y. Perform a paired-sample t-test between guilt ratings at the crucial time points (Twelve and Thirteen) for the subjects in the late group. Store the data (e.g. tidy) in a tibble called results. Helpful Hint To pull out the Twelve and Thirteen columns to pass as x and y, you can use: lates %&gt;% pull(Twelve) and lates %&gt;% pull(Thirteen). Once you’ve calculated results, don’t forget to tidy() - you can add this using a pipe! If you don’t quite understand the use of tidy() yet, run your t.test() without tidy() and see what happens! Group Discussion Point Look within the tibble results. In groups, break down the results you can see. Was there a significant difference or not? We are about to write it up so best we know for sure. How can you tell? 7.3.8 Task 8: The Write-up Fill in the blanks below to complete this paragraph, summarising the results of the study. You will need to refer back to the information within results and descriptives to get the correct answers and to make sure you understand the output of the t-test. Enter all values to two decimal places and present the absolute t-value. &quot;A paired-samples t-test one-sample t-test between-subjects t-test independent-samples t-test was run to compare the change in guilt ratings before (M = , SD = ) and after (M = , SD = ) the crucial evidence was heard. A significant non-significant difference was found (t() = , p = .05 &gt; .05 = .001 &lt; .001) with Timepoint 13 having an average rating units lower than Timepoint 12. This tells us that the critical evidence did have an influence on the rating of guilt by jury members that the critical evidence did not have an influence on the rating of guilt by jury members that the critical evidence and the rating of guilt by jury members are unconnected something but I am not quite sure right now, I best ask! Helpful Hint t-tests take the following format: t(df) = t-value, p = p-value your results states degrees of freedom as parameter, and your t-value as statistic. estimate is your mean difference between ratings at Timepoints Twelve and Thirteen. Note: When writing a code for your own report, you can make your write-up reproducible as well by using the output of your tibbles and calling specific columns. For example, t(`r results$parameter`) = `r results$statistic %&gt;% abs()`, p &lt; .001, when knitted will become t(74) = 8.23, p &lt; .001. So code can prevent mistakes in write-ups! Job Done - Activity Complete! Well done, you have completed the activities for this week’s lab! You can see how performing the t-test is only a small part of the entire process: wrangling the data, calculating descriptives, and plotting the data to check the distributions and assumptions is a major part of the analysis process. Over the past labs, you have been building all of these skills and so you should be able to see them being put to good use now that we have moved onto more complex data analysis. Running the inferential part is usually just one line of code. If you’re wanting to practice your skills further, you could perform a t-test for the “middle” group where the crucial evidence was presented on time point 9. Otherwise, you should now be ready to complete the Homework Assignment for this lab. The assignment for this Lab is summative and should be submitted through the Moodle Level 2 Assignment Submission Page no later than 1 minute before your next lab. If you have any questions, please post them on the slack forum under the channel #level2_2018. Finally, don’t forget to add any useful information to your Portfolio before you leave it too long and forget. 7.4 Assignment This is a summative assignment. Instructions as to how to access and submit your assignment will be made available during the course. 7.5 Solutions to Questions Below you will find the solutions to the questions for the Activities for this chapter. Only look at them after giving the questions a good try and speaking to the tutor about any issues. 7.5.1 InClass Activities 7.5.1.1 InClass Task 1 library(broom) library(tidyverse) ratings &lt;- read_csv(&quot;GuiltJudgements.csv&quot;) Return to Task 7.5.1.2 InClass Task 2 lates &lt;- ratings %&gt;% filter(Evidence == &quot;Late&quot;) %&gt;% select(Participant, Evidence, `12`, `13`) %&gt;% rename(Twelve = `12`, Thirteen = `13`) %&gt;% mutate(diff = Twelve - Thirteen) Return to Task 7.5.1.3 InClass Task 3 lates %&gt;% ggplot(aes(diff)) + geom_histogram(binwidth = 1) + labs(x = &quot;Difference in GuiltRating&quot;, y = NULL) + theme_bw() Figure 7.1: Potential Solution to Task 3 Return to Task 7.5.1.4 InClass Task 4 The Task only asks for the boxplot. We have added some additional functions to tidy up the figure a bit that you might want to play with. lates %&gt;% ggplot(aes(y = diff)) + geom_boxplot() + theme_bw() Figure 7.2: Potential Solution to Task 4 Return to Task 7.5.1.5 InClass Task 5 descriptives &lt;- lates %&gt;% select(-diff) %&gt;% gather(Timepoint, GuiltRating, Twelve, Thirteen) %&gt;% group_by(Timepoint) %&gt;% summarise(n = n(), mean = mean(GuiltRating), sd = sd(GuiltRating), se = sd/sqrt(n), LowerCI = mean - 1.96*se, UpperCI = mean + 1.96*se) Return to Task 7.5.1.6 InClass Task 6 A basic barplot with 95% Confidence Intervals. We have embellished the figure a little but you can mess around with the code to see what each bit does. ggplot(descriptives, aes(x = Timepoint, y = mean, fill = Timepoint)) + geom_col(colour = &quot;black&quot;) + scale_fill_manual(values=c(&quot;#E2A458&quot;, &quot;#61589C&quot;)) + scale_x_discrete(limits = c(&quot;Twelve&quot;,&quot;Thirteen&quot;)) + labs(x = &quot;Timepoint of Evidence&quot;, y = &quot;GuiltRating&quot;) + guides(fill=&quot;none&quot;) + geom_errorbar(aes(ymin = LowerCI, ymax = UpperCI), position = &quot;dodge&quot;, width = .15) + scale_y_continuous(breaks = c(1:9), limits = c(0,9)) + coord_cartesian(ylim = c(1,9), xlim = c(0.5,2.5), expand = FALSE) + theme_classic() Figure 7.3: Possible Solution to Task 7 An alternative way to display just the means and errorbars would be to use the pointrange approach. This image shows again the 95% CI ggplot(descriptives, aes(x = Timepoint, y = mean, fill = Timepoint)) + geom_pointrange(aes(ymin = LowerCI, ymax = UpperCI))+ scale_x_discrete(limits = c(&quot;Twelve&quot;,&quot;Thirteen&quot;)) + labs(x = &quot;Timepoint of Evidence&quot;, y = &quot;GuiltRating&quot;) + guides(fill=&quot;none&quot;)+ scale_y_continuous(breaks = c(1:9), limits = c(0,9)) + coord_cartesian(ylim = c(1,9), xlim = c(0.5,2.5), expand = FALSE) + theme_bw() Figure 7.4: Alternative Solution to Task 7 Return to Task 7.5.1.7 InClass Task 7 Remember to set paired = TRUE to run the within-subjects t-test results &lt;- t.test(lates %&gt;% pull(Twelve), lates %&gt;% pull(Thirteen), paired = TRUE) %&gt;% tidy() Return to Task 7.5.1.8 InClass Task 8 A potential write-up for this study would be as follows: A paired-samples t-test was run to compare the change in guilt ratings before (M = 5.8, SD = 1.5) and after (M = 4.04, SD = 1.93) the crucial evidence was heard. A significant difference was found (t(74) = 8.23, p &lt; .001) with Timepoint 13 having an average rating 1.76 units lower than Timepoint 12. This tells us that the critical evidence did have an influence on the rating of guilt by jury members. When rounding off p-values that are less than .001, rounding will give you a value of 0 which is a bit silly. Values less than .001 would normally be written as p &lt; .001. To create a reader-friendly p-value, then you could try something like the following in your code: ifelse(results$p.value &lt; .001, &quot;p &lt; .001&quot;, paste0(&quot;p = &quot;, round(results$p.value,3))) Return to Task Chapter Complete! "],
["apes-alpha-power-effect-sizes-sample-size.html", "Lab 8 APES - Alpha, Power, Effect Sizes, Sample Size 8.1 Overview 8.2 PreClass Activity 8.3 InClass Activity 8.4 Assignment 8.5 Solutions to Questions", " Lab 8 APES - Alpha, Power, Effect Sizes, Sample Size 8.1 Overview p until now we have mainly spent time on data-wrangling, understanding probability, visualising our data, and more recently, running inferential tests, i.e. t-tests. In the lectures you have also started to learn about additional aspects of inferential testing and trying to reduce certain types of error in your analyses: Type I error - rejecting the null hypothesis when it is true (otherwise called alpha or \\(\\alpha\\)). Probably better recalled as False Positives Type II error - retaining the null hypothesis when it is false (otherwise called beta or \\(\\beta\\)). Probably better recalled as False Negatives Building from there we have started to discuss the idea of power (\\(1-\\beta\\)) which, from your lectures and from the pre-class reading, you should understand is the long-run probability of correctly rejecting the null hypothesis for a fixed effect size and fixed sample size; i.e. correctly concluding there is an effect when the effect is real. The concept of power is useful for making decisions when you plan a study. The higher the power of your planned study, the better, with the field standard proposed as \\(power &gt;= .8\\) for the targeted effect size and sample size. In fact, Registered Reports are often required to have a power of at least \\(power &gt;= .9\\). Keep in mind that power is defined as the probability of rejecting the null for a fixed effect size and fixed sample size. For a given sample size, power will be higher for effects that you assume to be larger (they are easier to detect). For a given effect size, power will be higher for a larger sample. Because you have little control over the size of the effect, typically you can increase the power of your study by either (1) increasing the size of your sample or (2) reducing sources of noise and measurement error in your study. Because power depends on several variables, it is useful to think of power as a function rather than as a single fixed quantity. Psychological research has been criticised for neglecting power during study planning, resulting in many underpowered studies. Low power, combined with undisclosed analytic flexibility and publication bias, is thought to be a key issue in the replication crisis within the field. As such there may be a large number of studies where the null hypothesis has been rejected when it should not have been; the field becomes noisy at that point and you are unsure which studies will replicate. It is issues like this that led us to redevelop our courses and why we really want you to understand power as much as possible. When planning a study any good researcher will consider four key elements, the APES: alpha - most commonly thought of as the significance level; usually set at \\(\\alpha = .05\\) power - the probability of rejecting the null for a given effect size and sample size, with .8 usually cited as the minimum power you should aim for; effect size - size of the asssociation or difference you are trying to detect; sample size - the number of observations (usually, participants, but sometimes also stimuli) in your study. And the beautiful thing is that if you know any three of these elements then you can calculate the fourth. The two most common calculations prior to a study would be: to determine the appropriate sample size required to reject the null, with high probability, for the effect size that you are interested in. That is, you fix \\(\\alpha\\), power, and effect size, and solve for the sample size. Generally, the smaller the assumed effect size, the more participants you will need, assuming power and alpha are held constant at .8 and .05 respectively. to determine the smallest effect size you can reliably detect given your sample size. That is, you know everything except the effect size. For example, say you are using an open dataset and you know they have run 100 participants, you can’t add any more participants, and you want to know what is the minimum effect size I could detect in this dataset with some probability. Note: Most papers would discourage you from calculating what is called Observed or Post-Hoc Power. This is where you calculate the power after running the study, based on your effect size and sample size. Similarly, this would be running an analysis on an open dataset, finding the outcome, and then calculating the power based on the outcome. Avoid this. You can read more about why, here, in your own time if you like: Lakens (2014) Observed Power, and what to do if your editor asks for post-hoc power analyses 8.2 PreClass Activity As in the last lab, the Preclass activity involves reading a blog and watching a video. We have selected this material to help give you a better understanding of power and how it interacts with effect size, sample size, and alpha. We have also suggested a couple of other links that you can look at and play with to get a rounder view. 8.2.1 Reading Read the following blog on Power. This is a blog on a fictional conversation between a professor and a student on the importance on power. Grab a coffee and have a read. Don’t worry about reading all the additional papers unless you want to; just the blog is fine to get an understanding. Blog: The Power Dialogues by PIGEE at the University of Illinois. 8.2.2 Watch You should also watch this short but nonetheless highly informative video by Daniel Lakens on Power and Sample Size. And his shirt is amazing! Video: Power Analysis and Sample Size Decisions by Daniel Lakens Remember to make notes about power, effect sizes, and sample sizes, as processing the concepts into your own words will really help you to understand them better. 8.2.3 Optional There are a number of great webpages and blogs that will help you understand the concepts in this chapter. Here are some that we think might be fun for you to look at. You don’t have to look at all of these for this lab but do come back to them as they will really help you as you progress in the course. We are deliberately giving you a number of options here as for everyone there is that one analogy that will work best for you and that one paper that will make everything click into place. That example will be different from person to person so having a variety of explanations will help. A YouTube video by Dan Quintana (University of Oslo) showing how to use the pwr package to calculate power in t-tests, correlations, and one-way ANOVAs https://www.youtube.com/watch?v=ZIjOG8LTTh8 A shiny app created by Lisa Debruine (University of Glasgow) on guessing the effect size between two conditions http://shiny.psy.gla.ac.uk/guess/ A blog by Daniel Lakens (Eindhoven University of Technology) on determining the smallest effect size you are interested in. This is often referred to as the Smallest Effect Size of Interest (SESOI) http://daniellakens.blogspot.com/2017/05/how-power-analysis-implicitly-reveals.html An interactive webpage by Kristoffer Magnusson (Karolina Instituet, Stockholm) on interpreting Cohen’s d effect size https://rpsychologist.com/d3/cohend/ A shiny app by Hause Lin (University of Toronto) showing the conversion of one effect size into another http://escal.site/ A Frontiers in Psychology paper by Daniel Lakens on calculating various effect sizes for t-tests and ANOVAs https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00863/full A blog by Daniel Lakens on what Type I and Type II errors are acceptable. In short, justify everything http://daniellakens.blogspot.com/2019/05/justifying-your-alpha-by-minimizing-or.html Job Done - Activity Complete! Hopefully this has given you a good basis to understanding power, sample sizes, alpha, and effect sizes. These are difficult concepts to grasp and it will take a lot of time thinking about them and interacting with them before they really start to sink in. Hopefully however, if nothing else, the least you come away with is the idea that the number of participants you should run in a study is not an arbitrary decision but is in fact a relationship between the effect size you want to test for and the level of error (Type I or Type II) you are willing to accept. As always, the best way to understand something is to put it into your own words so don’t forget to go back and add any informative points to your Portfolio. Post any questions on the the Moodle forum or the slack forum rguppies.slack.com under the channel #level2_2018. See you in the lab! 8.3 InClass Activity So let’s jump into this a bit now and start running some analyses to help further our understanding of alpha, power, effect sizes and sample size! Effect Sizes - Cohen’s \\(d\\) We will continue to focus on t-tests for this lab. There are a number of different effect sizes to choose from in the field but in the lectures we have looked at one type of effect size - Cohen’s d: the standardised difference between two means (in units of SD). The thing to note is that the formula is slightly different depending on the type of t-test used and it can sometimes change depending on who you read. For today, and this lab, let’s go with the following formulas: One-sample t-test &amp; paired-sample t-test: \\(d = \\frac{t}{sqrt(N)}\\) Independent t-test: \\(d = \\frac{2t}{sqrt(df)}\\) Let’s now try out some calculations. We will start with just looking at effect sizes from t-tests before calculating power in later tasks. 8.3.1 Task 1: Effect size from a one-sample t-test You run a one-sample t-test and discover a significant effect, t(25) = 3.24, p &lt; .05. Calculate d and determine whether the effect size is small, medium or large. Helpful Hint Use the appropriate formula from above for the one-sample t-tests. You have been given a t-value and df (degrees of freedom), you still need to determine n before you calculate d. According to Cohen (1988), the effect size is small (.2 to .5), medium (.5 to .8) or large (&gt; .8). Quickfire Questions Answer the following questions to check your answers. The solutions are at the end of the chapter: Enter, in digits, how many people were run in this study: Which of these codes is the appropriate calculation of d in this instance: d = t/sqrt(N) d = 2t/sqrt(df) Enter the correct value of d for this analysis rounded to 2 decimal places: According to Cohen (1988), the effect size for this t-test would be considered: small medium large 8.3.2 Task 2: Effect size from between-subjects t-test You run a between-subjects t-test and discover a significant effect, t(30) = 2.9, p &lt; .05. Calculate d and determine whether the effect size is small, medium or large. Helpful Hint Use the appropriate formula above for between-subjects t-tests. According to Cohen (1988), the effect size is small (.2 to .5), medium (.5 to .8) or large (&gt; .8). Quickfire Questions Answer the following questions to check your answers. The solutions are at the end of the chapter: Enter, in digits, how many people were run in this study: Which of these codes is the appropriate calculation of d in this instance: d = t/sqrt(N) d = 2t/sqrt(df) Enter the correct value of d for this analysis rounded to 2 decimal places: According to Cohen (1988), the effect size for this t-test would be considered: small medium large 8.3.3 Task 3: Effect Size from matched-pairs t-test You run a matched-pairs t-test between an ASD sample and a non-ASD sample and discover a significant effect t(39) = 2.1, p &lt; .05. How many people are there in each group? Calculate d and determine whether the effect size is small, medium or large. Helpful Hint You need the df value to determine N. A matched pairs is treated like a paired-sample t-test but with two separate groups. Quickfire Questions Answer the following questions to check your answers. The solutions are at the end of the chapter: Enter, in digits, how many people were in each group in this study. Note, not the total number of participants: Which of these codes is the appropriate calculation of d in this instance: d = t/sqrt(N) d = 2t/sqrt(df) Enter the correct value of d for this analysis rounded to 2 decimal places: According to Cohen (1988), the effect size for this t-test would be considered: small medium large Explain This - I don’t understand the number of people in each group answer! df in a paired-samples and in a matched-pairs t-test is calculated as df = N - 1. Conversely, to find the total number of participants: N = df + 1; so N = 40. Given that this is a matched-pairs t-test, by design there has to be an equal number of participants in each group. Therefore 40 participants in each group. 8.3.4 Task 4: t-value and effect size for a between-subjects Experiment You run a between-subjects design study and the descriptives tell you: Group 1, M = 10, SD = 1.3, n = 30; Group 2, M = 11, SD = 1.7, n = 30. Calculate t and d for this between-subjects experiment. Helpful Hint Before you can calculate d (using the appropriate formula for a between-subjects experiment), you need to first calculate t using the formula: t = (Mean1 - Mean2)/sqrt((var1/n1) + (var2/n2)) var stands for variance in the above formula. Variance is not the same as the standard deviation, right? Variance is measured in squared units. So for this equation, if you require variance to calculate t and you have the standard deviation, then you need to remember that var = SD^2. Now you have your t-value, but for calculating d you also need degrees of freedom. Think about how you would calculate df for a between-subjects experiment, taking n for both Group 1 and Group 2 into account. Remember that convention is that people report the t and d values as positive. Quickfire Questions Answer the following questions to check your answers. The solutions are at the end of the chapter: Enter the correct t-value for this test, rounded to two decimal places: Which of these codes is the appropriate calculation of d in this instance: d = t/sqrt(N) d = 2t/sqrt(df) Based on the above t-value above, enter the correct value of d for this analysis rounded to 2 decimal places: According to Cohen (1988), the effect size for this t-test would be described as: small medium large Excellent! Now that you are comfortable with calculating effect sizes, we will look at using them to establish the sample size needed to reach a target power. Remember, in analysis, in nearly all occasions we should set the effect size as the Smallest Effect Size Of Interest (SESOI). This can be determined through theoretical analysis, through previous studies, through pilot studies, or through rules of thumb like Cohen (1988). However, also keep in mind that the lower the effect size, the larger the sample size you will need. Everything is a trade-off. Power Calculations Today we will use the function power.t.test() to run our calculations. This is a function available in base R, meaning that it is included when you install R, so you do not need to install any additional package. However, in the lectures we have been using the pwr package library which has a very similar function called pwr.t.test(). Going forward we would recommend using the pwr package functions instead as they give more diversity in terms of tests and are actually easier to use. If you are using the Boyd Orr machines the pwr package is already installed and you will just need to call it like all other packages, e.g. library(pwr). Do not attempt to install it yourself on the Boyd Orr machines. If you are using your own laptop then feel free to install it. The reason we aren’t using the pwr package in this lab is because we can’t guarantee other machines across campus will have that package installed and that seems unfair on people without their own device. So feel free to explore the pwr package for future use, and we will create a version of this lab in the pwr library for you to practice with, but for now let’s just use the base function power.t.test(). Remember that for more information on this function, simply do ?power.t.test in the console. On doing this you will see that power.t.test() takes a series of inputs: n - observations/participants, per group for the independent samples version, or the number of subjects or matched pairs for the paired and one-sample designs. delta - the difference between means sd - standard deviation; note: if sd = 1 then delta = Cohen’s d sig.level or \\(\\alpha\\) power or \\(1-\\beta\\) type - the type of t-test; e.g. &quot;two.sample&quot;, &quot;one.sample&quot;, &quot;paired&quot; alternative - the type of hypothesis; &quot;two.sided&quot;, &quot;one.sided&quot; And it works on a leave one out principle. You give it all the info you have and it returns the element you are missing. So, for example, say you needed to know how many people per group you would need to detect an effect size as low as d = .4 with power = .8, alpha = .05 in a two.sample (between-subjects) t-test on a two.sided hypothesis test. You would do: power.t.test(delta = .4, sd = 1, power = .8, sig.level = .05, alternative = &quot;two.sided&quot;, type = &quot;two.sample&quot;) And it would tell you that you would need 99.080565 people per condition. But you only get whole people and we like to be conservative on our estimates so we would actually run 100 per condition. That is a lot of people!!! Let’s get started. But before you start with this next task, you will need to load in packages such as the tidyverse and broom. 8.3.5 Task 5: Sample size for standard power one-sample t-test Assuming you are interested in detecting a minimum Cohen’s d of d = .23, what would be the minimum number of participants you would need in a one-sample t-test, assuming power = .8, \\(\\alpha\\) = .05, on a two-sided hypothesis? Using a pipeline, store the answer as a single value called sample_size (e.g. think tidy() %&gt;% pull()) and round up to the nearest whole participant. You will probably need to use ceiling() instead of round(). For example, ceiling(1.1) gives you 2. ceiling() always rounds up! Helpful Hint Use the list of inputs above as a kind of checklist to clearly determine which inputs are known or unknown. This can help you enter the appropriate values to your code. The structure of the power.t.test() would be very similar to the one shown above except two.sample would become one.sample If sd = 1, delta = cohen’s d You will also need to use tidy() %&gt;% pull(n) to help you obtain the sample size and %&gt;% ceiling() to round up to the nearest whole participant. Quickfire Questions Answer the following question to check your answers. The solutions are at the end if you need them: Enter the minimum number of participants you would need in this one-sample t-test: 8.3.6 Task 6: Effect size from a high power between-subjects t-test Assuming you run a between-subjects t-test with 50 participants per group and want a power of .9, what would be the minimum effect size you can reliably detect? Assume standard \\(\\alpha\\) and alternative hypothesis settings. Using a pipeline, store the answer as a single value called cohens and round to two decimal places. Helpful Hint Again, use the list of inputs above as a kind of checklist to clearly determine which inputs are known or unknown. This can help you enter the values to your code. This time we know everything except delta. Assume sd = 1. You will also need to use tidy(), pull() to obtain Cohen’s d, delta, and round() so the value is rounded to two decimal places. Quickfire Questions Answer the following questions to check your answers. The solutions are at the end of the chapter: Based on the information given, what will you set type as in the function? one.sample two.sample Based on the output, enter the minimum effect size you can reliably detect in this test, rounded to two decimal places: According to Cohen (1988), the effect size for this t-test is small medium large Say you run the study and find that the effect size determined is d = .50. Given what you know about power, select the statement that is most accurate: the study is sufficiently powered as the analysis indicates you can only reliably detect effect sizes smaller than d = .65 the study is potentially underpowered as the analysis indicates you can really only reliably detect effect sizes larger than d = .65 8.3.7 Task 7: Power of Published Research Thus far we have used hypothetical situations - now go look at the paper on the Open Stats lab website called Does Music Convey Social Information to Infants? - we looked at it a little in the lectures. You can download the pdf and look at it, but here we will determine the power of the significant t-tests reported in Experiment 1 under the Results section on Pg489. There is a one-sample t-test and a paired-samples t-test to consider, summarised below. Assume testing was at power = .8, alpha = .05. Based on your calculations are either of the stated effects underpowered? one-sample: t(31) = 2.96, p = .006 paired t-test: t(31) = 2.42, p = .022 Helpful Hint A one-sample t-test and a paired t-test use the same formula for Cohen’s d. To calculate n: n = df + 1. Calculate the achievable Cohens d for the studies and then calculate the established Cohen’s d for the studies. Group Discussion Point Which of the t-tests do you believe to be potentially underpowered? Why do you think this may be? Additional information about this to further your discussion can be found in the solutions at the end of this chapter. One caveat to Tasks 6 and 7: We have to keep in mind that here we are looking at single studies using one sample from a potentially huge number of samples within a population. As such there will be a degree of variance in the true effect size within the population regardless of the effect size of one given sample. What that means is we have to be a little bit cautious when making claims about a study. Ultimately the higher the power the better as you can detect smaller effect sizes! Job Done - Activity Complete! Great! So hopefully you are now starting to see the interaction between alpha, power, effect sizes, and sample size. We should always want high powered studies and depending on the size of the effect we are interested in (small to large), and our \\(\\alpha\\) level, this will determine the number of observations we need to make sure our study is well powered. Points to note: Lowering the \\(\\alpha\\) level (e.g. .05 to .01) will reduce the power. Lowering the effect size (e.g. .8 to .2) will reduce the power. Increasing power (.8 to .9) will require more participants. It is also possible to increase power for a fixed sample size by reducing sources of noise in the study. A high-powered study looking to detect a small effect size at a low alpha may require a large number of participants! Another point probably to consider for the future: what about studies with multiple observations per participant? How do you calculate power for this? This is a very common situation. You should now be ready to complete the Homework Assignment for this lab. The assignment for this Lab is FORMATIVE and is NOT to be submitted and will NOT count towards the overall grade for this module. However, you are strongly encouraged to do the assignment as it will continue to boost your skills which you will need in future assignments. If you have any questions, please post them on the forums! 8.4 Assignment Lab 8: APES Assignment In order to complete this assignment you first have to download the assignment .Rmd file which you need to edit for this assignment: titled GUID_Level2_Semester1_Lab8.Rmd. This can be downloaded within a zip file from the below link. Once downloaded and unzipped you should create a new folder that you will use as your working directory; put the .Rmd file in that folder and set your working directory to that folder through the drop-down menus at the top Download the assignment zip file from here. NOTE: in nearly all of the problems below, you will need to replace NULL with a value or a pipeline of code that computes a value. Please pay special attention as to what the question is asking for as the output, e.g. value or a tibble; when asked for a value as an output, make sure it is a single value and not a value stored in a tibble. Finally, when altering code inside the code blocks, please do not re-order or rename the code blocks (T1, T2, … etc.). If you do, this may impact your grade! It’s also recommended that you “Knit” a report to be able to see what you’ve accomplished and spot potential errors. A great thing to do is close the whole programme, restart it, and then knit your code. This will test whether you have remembered to include essential elements, such as libraries, in your code. APES: Alpha, Power, Effect Size, and Sample Size In the lab we have been looking at the interplay between the four components of Alpha, Power, Effect Size, and Sample Size. This is a very important part of experimental design to understand as it will help you understand which studies are worth paying attention to and it will help you design your own studies in the coming years so that you know just how many people to run and what to make of the effect that you find. If you have not yet done so, we highly recommend reading the blog suggested as PreClass reading material and carrying out the activities in the inclass activity. These will help you will both the practicalities and the interpretation of the following assignment. Remember that this assignment is formative but the knowledge gained from the practical activities in this lab will be super important to your future-self! Before starting let’s check: The .Rmd file is saved in your working directory. For assessments we ask that you save it with the format GUID_Level2_Semester1_Lab8.Rmd where GUID is replaced with your GUID. Though this is a formative assessment, it may be good practice to do the same here. Libraries You will need to use the tidyverse and broom libraries in this assignment, so load them in the library code chunk below. You do not need the pwr library as all of the below questions can be completed with base functions but if you prefer to use the pwr library then load it below in the libary code chunk as well. # TO DO, load in the two stated libraries as you will need functions from these libraries Basic Calculations 8.4.1 Assignment Task 1 You set up a study so that it has a power value of \\(power = .87\\). To two decimal places, what is the Type II error rate of your study? Replace the NULL in the T1 code chunk below with either a single value, or with mathematical notation, so that error_rate returns the actual value of the Type II error rate for this study. By mathematical notation we mean you to use the appropriate formula but insert the actual values. error_rate &lt;- NULL 8.4.2 Assignment Task 2 You run an independent t-test and discover a significant effect, t(32) = 3.26, p &lt; .05. Using the appropriate formula, given in the inclass activity, calculate the effect size of this t-test. Replace the NULL in the T2 code chunk below with mathematical notation so that effect1 returns the value of the effect size. Do not round the value. effect1 &lt;- NULL 8.4.3 Assignment Task 3 You run a dependent t-test and discover a significant effect, t(43) = 2.24, p &lt; .05. Using the appropriate formula, given in the inclass activity, calculate the effect size of this t-test. Replace the NULL in the T3 code chunk below with mathematical notation so that effect2 returns the value of the effect size. Do not round the value. effect2 &lt;- NULL Using the Power function 8.4.4 Assignment Task 4 Replace the NULL in the T4 code chunk below with a pipeline combining power.t.test(), tidy(), pull() and ceiling(), to determine how many participants are needed to sufficiently power a paired-samples t-test at \\(power = .9\\) with \\(delta = .5\\), \\(sd = 1\\)? Assume a two-sided hypothesis with \\(\\alpha = .05\\). Ceiling the answer to the nearest whole participant and store this value in participants. Note1: If using the pwr package pwr.t.test() would be the ideal function. Note2: If using the pwr package you do not declare sd Note3: If using the pwr package delta is the same as d participants &lt;- NULL 8.4.5 Assignment Task 5 Using a pipeline similar to Task 4, what is the minimum effect size that a one-sample t-test study (two-tailed hypothesis) could reliably detect given the following details : \\(\\beta = .16, \\alpha = 0.01, n = 30\\). Round to two decimal places and replace the NULL in the T5 code chunk below to store this value in effect3. effect3 &lt;- NULL 8.4.6 Assignment Task 6 Study 1 You run a between-subjects study and establish the following descriptives: Group 1 (M = 5.1, SD = 1.34, N = 32); Group 2 (M = 4.4, SD = 1.27, N = 32). Replace the NULL in the T6 code chunk below with the following formula, substituting in the appropriate values, to calculate the t-value of this test. Calculate as Group1 minus Group2. Store the t-value in tval. Do not round tval and do not include the t = part of the formula. \\[ t = \\frac {{\\bar{x_{1}}} - \\bar{x_{2}}}{ \\sqrt {\\frac {{s_{1}}^2}{n_{1}} + \\frac {{s_{2}}^2}{n_{2}}}}\\] tval &lt;- NULL 8.4.7 Assignment Task 7 Using the tval calculated in Task 6, calculate the effect size of this study and store it as d1 in the T7 code chunk below, replacing the NULL with the appropriate formula and values. Do not round d1. d1 &lt;- NULL 8.4.8 Assignment Task 8 Assuming \\(power = .8\\), \\(\\alpha =.05\\) on a two-tailed hypothesis, based on the d1 value in Task 7 and the smallest achievable effect size of this study, which of the below statements is correct. The smallest effect size that this study can determine is d = .71. The detected effect size, d1, is larger than this and as such this study is potentially suitably powered The smallest effect size that this study can determine is d = .17. The detected effect size, d1, is larger than this and as such this study is potentially suitably powered The smallest effect size that this study can determine is d = .17. The detected effect size, d1, is smaller than this and as such this study is potentially suitably powered The smallest effect size that this study can determine is d = .71. The detected effect size, d1, is smaller than this and as such this study is potentially not suitably powered Replace the NULL in the T8 code chunk below with the number of the statement that is a true summary of this study. It may help you to calculate and store the smallest achievable effect size of this study in poss_d. #hint: use poss_d to calculate the smallest possible effect size of this study to help you answer this question. poss_d &lt;- NULL answer_T8 &lt;- NULL 8.4.9 Assignment Task 9 Study 2 Below is a paragraph from the results of Experiment 4 from Schroeder, J., &amp; Epley, N. (2015). The sound of intellect: Speech reveals a thoughtful mind, increasing a job candidate’s appeal. Psychological Science, 26, 877-891. We saw this paper in Lab 5 but you can find out more details at &lt;a href=“https://sites.trinity.edu/osl/data-sets-and-activities/t-test-activities”, target = &quot;_blank&quot;&gt;Open Stats Lab. Recruiters believed that the job candidates had greater intellect - were more competent, thoughtful, and intelligent - when they listened to pitches (M = 5.63, SD = 1.61, n = 21) than when they read pitches (M = 3.65, SD = 1.91, n = 18), t(37) = 3.53, p &lt; .01, 95% CI of the difference = [0.85, 3.13], d1 = 1.16. The recruiters also formed more positive impressions of the candidates - rated them as more likeable and had a more positive and less negative impression of them - when they listened to pitches (M = 5.97, SD = 1.92) than when they read pitches (M = 4.07, SD = 2.23), t(37) = 2.85, p &lt; .01, 95% CI of the difference = [0.55, 3.24], d2 = 0.94. Finally, they also reported being more likely to hire the candidates when they listened to pitches (M = 4.71, SD = 2.26) than when they read the same pitches (M = 2.89, SD = 2.06), t(37) = 2.62, p &lt; .01, 95% CI of the difference = [0.41, 3.24], d3 = 0.86. Using the power.t.test() function, what is the minimum effect size that this paper could have reliably detected? Test at \\(power = .8\\) for a two-sided hypothesis. Use the \\(\\alpha\\) stated in the paragraph and the smallest n stated; store the value as effect4 in the T9 code chunk below. Replace the NULL with your pipeline and round the effect size to two decimal places. effect4 &lt;- NULL 8.4.10 Assignment Task 10 Given the value of effect4 calculated in Task 9, and the stated alpha in the paragraph and the smallest n of the two groups, which of these statements is true. This study has enough power to reliably detect effects at the size of d3 and larger. This study has enough power to reliably detect effects at the size of only d1. This study has enough power to reliably detect effects at the size of d2 and larger, but not d3. This study does not have enough power to reliably detect effect sizes at d1 or lower. Replace the NULL in the T10 code chunk below with the number of the statement that is TRUE, storing the single value in answer_t10. answer_t10 &lt;- NULL 8.4.11 Assignment Task 11 Last but not least: Read the following statements. In general, increasing sample size will increase the power of a study. In general, smaller effect sizes require fewer participants to detect at \\(power = .8\\). In general, lowering alpha (from .05 to .01) will decrease the power of a study. Now look at the below four summary statements of the validity of the statements a, b and c. Statements a, b and c are all TRUE. Statements a and c are both TRUE. Statements b and c are both TRUE. None of the statements are TRUE. Replace the NULL in the T11 code chunk below with the number of the statement that is correct, storing the single value in answer_t11. answer_t11 &lt;- NULL 8.4.12 The pwr package An alternative solution to Task 9 would be to use the pwr.t2n.test() function from the pwr package (Champely 2018). This would allow you to enter the n of both groups as there is an n1 and an n2 argument. Were you to use this, entering n1 = 18, n2 = 21, alpha = .01, the d drops just a little, changing the interpretation of Task 10. Feel free to try this analysis and see if you can figure out what would be the alternative answer to Task 10. Job Done - Activity Complete! Well done, you are finshed! Now you should go check your answers against the solution file which can be found at the end of this chapter. You are looking to check that the resulting output from the answers that you have submitted are exactly the same as the output in the solution - for example, remember that a single value is not the same as a coded answer. Where there are alternative answers, it means that you could have submitted any one of the options as they should all return the same answer. If you have any questions please post them on the forums. 8.5 Solutions to Questions Below you will find the solutions to the questions for the Activities for this chapter. Only look at them after giving the questions a good try and speaking to the tutor about any issues. 8.5.1 InClass Activities 8.5.1.1 InClass Task 1 d &lt;- 3.24 / sqrt(25 +1) Giving an effect size of d = 0.64 and as such a medium to large effect size according to Cohen (1988) Return to Task 8.5.1.2 InClass Task 2 d &lt;- (2*2.9) / sqrt(30) Giving a effect size of d = 1.06 and as such a large effect size according to Cohen (1988) Return to Task 8.5.1.3 InClass Task 3 N = 39 + 1 d &lt;- 2.1 / sqrt(N) Giving an N = 40 and an effect size of d = 0.33. This would be considered a small effect size according to Cohen (1988) Return to Task 8.5.1.4 InClass Task 4 t = (10 - 11)/sqrt((1.3^2/30) + (1.7^2/30)) d = (2*t)/sqrt((30-1) + (30-1)) Giving a t-value of t = 2.56 and an effect size of d = 0.67. Remember that convention is that people tend to report the t and d as positive values. Return to Task 8.5.1.5 InClass Task 5 sample_size &lt;- power.t.test(delta = .23, sd = 1, power = .8, sig.level = .05, alternative = &quot;two.sided&quot;, type = &quot;one.sample&quot;) %&gt;% tidy() %&gt;% pull(n) %&gt;% ceiling() Giving a sample size of n = 151 Return to Task 8.5.1.6 InClass Task 6 cohens &lt;- power.t.test(n = 50, power = .9, sig.level = .05, alternative = &quot;two.sided&quot;, type = &quot;two.sample&quot;) %&gt;% tidy() %&gt;% pull(delta) %&gt;% round(2) Giving a cohen’s effect size of d = 0.65 Return to Task 8.5.1.7 InClass Task 7 Example 1 ach_d_exp1 &lt;- power.t.test(power = .8, n = 32, type = &quot;one.sample&quot;, alternative = &quot;two.sided&quot;, sig.level = .05) %&gt;% tidy() %&gt;% pull(delta) %&gt;% round(2) exp1_d &lt;- 2.96/sqrt(31+1) Giving an achievable effect size of 0.51 and they found an effect size of 0.52. This study seems ok as the authors could achieve an effect size as low as .51 and found an effect size at .52 Example 2 ach_d_exp2 &lt;- power.t.test(power = .8, n = 32, type = &quot;paired&quot;, alternative = &quot;two.sided&quot;, sig.level = .05) %&gt;% tidy() %&gt;% pull(delta) %&gt;% round(2) exp2_d &lt;- 2.42/sqrt(31+1) Giving an achievable effect size of 0.51 and they found an effect size of 0.43. This effect might not be reliable given that the effect size found was much lower than the achievable effect size. The issue here is that the researchers established their sample size based on a previous effect size and not on the minimum effect size that they would find important. If an effect size as small as .4 was important then they should have powered all studies to that level and ran the appropriate n ~52 babies (see below). Flipside of course is that obtaining 52 babies isnt easy; hence why some people consider the Many Labs approach a good way ahead. ONE CAVEAT to the above is that before making the assumption that this study is therefore flawed, we have to keep in mind that this is one study using one sample from a potentially huge number of samples within a population. As such there will be a degree of variance in the true effect size within the population regardless of the effect size of one given sample. What that means is we have to be a little bit cautious when making claims about a study. Ultimately the higher the power the better. Below you could calculate the actual sample size required to achieve a power of .8: sample_size &lt;- power.t.test(power = .8, delta = .4, sd = 1, type = &quot;paired&quot;, alternative = &quot;two.sided&quot;, sig.level = .05) %&gt;% tidy() %&gt;% pull(n) %&gt;% ceiling() Suggesting a sample size of n = 52 would be appropriate. Return to Task 8.5.2 Homework Activity Libraries library(pwr) library(broom) library(tidyverse) 8.5.2.1 Assignment Task 1 error_rate &lt;- 1 - .87 The Type II error rate of your study would be \\(\\beta\\) = 0.13. Return to Task 8.5.2.2 Assignment Task 2 effect1 &lt;- (2*3.26)/sqrt(32) The effect size would be d = 1.1525841 Return to Task 8.5.2.3 Assignment Task 3 effect2 &lt;- 2.24/sqrt(43+1) The effect size would be d = 0.3376927 Return to Task 8.5.2.4 Assignment Task 4 participants &lt;- power.t.test(power = .9, delta = .5, sd = 1, sig.level = 0.05, type = &quot;paired&quot;, alternative = &quot;two.sided&quot;) %&gt;% tidy() %&gt;% pull(n) %&gt;% ceiling() Given the detailed scenario, the appropriate number of participants would be n = 44 Return to Task 8.5.2.5 Assignment Task 5 effect3 &lt;- power.t.test(power = 1-.16, n = 30, sig.level = 0.01, type = &quot;one.sample&quot;, alternative = &quot;two.sided&quot;) %&gt;% tidy() %&gt;% pull(delta) %&gt;% round(2) Given the detailed scenario, we would be able to detect an effect size of d = 0.69 Return to Task 8.5.2.6 Assignment Task 6 tval &lt;- (5.1 - 4.4) / sqrt((1.34^2/32) + (1.27^2/32)) Given the stated means and standard deviations, the t-value for this study would be t = 2.1448226 Return to Task 8.5.2.7 Assignment Task 7 d1 &lt;- (2*tval)/sqrt((32-1)+(32-1)) Given the t-value in Task 6, the effect size of this study would be d = 0.5447855. Return to Task 8.5.2.8 Assignment Task 8 poss_d &lt;- power.t.test(power = .8, n = 32, sig.level = 0.05, type = &quot;two.sample&quot;, alternative = &quot;two.sided&quot;) %&gt;% tidy() %&gt;% pull(delta) %&gt;% round(2) answer_T8 &lt;- 4 The smallest effect size that this study can determine is d = 0.71. The detected effect size, d1, is smaller than this (d1 = 0.5447855) and as such this study is not suitably powered. Given that outcome, the 4th statement is the most suitable answer - answer_T8 = 4. Return to Task 8.5.2.9 Assignment Task 9 effect4 &lt;- power.t.test(power = .8, n = 18, sd = 1, sig.level = .01, alternative = &quot;two.sided&quot;, type = &quot;two.sample&quot;) %&gt;% tidy() %&gt;% pull(delta) %&gt;% round(3) The smallest stated n is n = 18 and the stated \\(\\alpha\\) is \\(\\alpha\\) = .01 Given these details, the minimum effect size that this paper could have reliably detected was d = 1.198 Return to Task 8.5.2.10 Assignment Task 10 answer_t10 &lt;- 4 This study does not have enough power to detect effect sizes at d1 or lower and as such answer_t10 = 4 However, it is worth keeping in mind that we are only looking at one study here which drew one sample from a population of samples. This means that there is always uncertainty about the true effect size of a difference or association - taking a different sample may have given a different effect size. As such, the comparison we are making here is not entirely valid and we should see it more as a reminder that we should always think of power as more in the planning of studies rather than in the search for criticism. Return to Task 8.5.2.11 Assignment Task 11 answer_t11 &lt;- 2 In general, increasing sample size will increase the power of a study whereas lowering alpha (from .05 to .01) will decrease the power of a study. As such, statements a and c, answer_t11 = 2. Return to Task Chapter Complete! A References "],
["reflection-semester-1.html", "Lab 9 Reflection - Semester 1 9.1 Overview 9.2 PreClass Activity 9.3 InClass Activity 9.4 Assignment 9.5 Solutions to Questions", " Lab 9 Reflection - Semester 1 9.1 Overview We have covered a lot of material in these labs and now would be a good time to stop, recap, and reflect on what we have learnt. As such, this lab is more about looking back at what you have learnt, testing your skills, resolving issues, and looking at other cool applications of R that have not been covered in this lab series. 9.2 PreClass Activity As we are reflecting in this lab, your preclass activities this time are: Review the labs from this semester and note any issues you have with the elements covered - both in terms of concepts and code. Post these issues on the Practical labs forum on Moodle and bring them to the lab next week. 9.3 InClass Activity Like the PreClass, we want to spend some time reflecting on what we have learnt and as such this InClass is about looking at ideas, concepts, and codes, that you have had issue with and seeing if we can resolve those issues. In class, we will spend some time looking at any issues you have had along the way. We will also look at some other interesting things you can do in R should you wish to expand your own knowledge and skills, such as: Popping out the Source Window to make working easier - Using Source Windows Analysing Twitter data with the rtweet package (Kearney 2019) Animating plots with the ggganimate package (Pedersen and Robinson 2019) Creating quickfire quizzes with the webex (Barr and DeBruine 2019) Make your own memes using the meme package (Yu 2019) The Hex Sticker Memory game and the background behind it Creating interactive plots using ggplot and plotly Even funkier visualtions using the ggforce package - check out facet_zoom() (Pedersen 2019) Many diverse fields are now using R and this is a good example: R for Journalists Using the knitr::read_chunk() function to call R script code through R Markdown (Xie 2019) 9.4 Assignment This is a summative assignment. Instructions on how to access and submit your assignment will be made available during the course. 9.5 Solutions to Questions Instructions on how to access the solution to this lab will be made available during the course. A References "],
["correlations.html", "Lab 10 Correlations 10.1 Overview 10.2 PreClass Activity 10.3 InClass Activity 10.4 Assignment 10.5 Solutions to Questions", " Lab 10 Correlations 10.1 Overview As you will read in Miller and Haden (2013) as part of the preclass reading, correlations are used to detect and quantify relationships among numerical variables. In short, you measure two variables and the correlation analysis tells you whether or not they are related in some manner - positively (i.e. one increases as the other increases) or negatively (i.e. one decreases as the other increases). However, unfortunately, you may have heard people say lots of negative things about correlations: they do not prove causality they suffer from the bidirectional problem (A vs B is the same as B vs A) any relationship found may be the result of an unknown third variable (e.g. murders and ice-creams) But whilst these are true, correlations are an incredibly useful and commonly used measure in the field, so don’t be put off from using them or designing a study that uses correlations. In fact, they can be used in a variety of areas. Some examples include: looking at reading ability and IQ scores such as in Miller and Haden Chapter 11, which we will also look at today; exploring personality traits in voices (see Phil McAleer’s work on Voices); or traits in faces (see Lisa DeBruine’s and Ben Jones’s work); brain-imaging analysis looking at activation in say the Amygdala in relation to emotion of faces (see Alex Todorov’s work at Princeton); social attidues, both implicit and explicit, as Helena Paterson will discuss in her Social lectures this semester; or a whole variety of fields where you simply measure the variables of interest. To actually carry out a correlation is very simple and we will show you that today in a little while: you just need the cor.test() function. The harder part of correlations is really wrangling the data (which you’ve learned to do in Semester 1) and interpreting the data (which we will focus more on today). So, we are going to run a few correlations today, showing you how to do one, and asking you to peform others to give you some good practice at running and interpreting the relationships between two variables. Note: When dealing with correlations it is better to refer to relationships and NOT predictions. In a correlation, X does not predict Y, that is really more regression which we will look at later on this semester. In a correlation, what we can say is that X and Y are related in some way. Try to get the correct terminology and please feel free to pull us up if we say the wrong thing in class. It is an easy slip of the tongue to make! In this lab we will: Introduce the Miller and Haden book, which we will be using throughout the rest of the semester. Show you the thought process of a researcher running a correlational analysis. Give you practice in running and writing up a correlation. 10.2 PreClass Activity The majority of the PreClass activities this semester will involve reading a chapter or two from Miller and Haden (2013) and trying out a couple of tasks. This is an excellent free textbook that we will use to introduce you to the General Linear Model, a model that underlies all the analysis you have seen and will see this year. But, for now, we will start off with some introduction to correlations. Have a read at the chapter, use the visualiser, play the game, and we will see you in the lab. 10.2.1 Read Chapter Read Chapters 10 and 11 of Miller and Haden (2013). Both chapters are really short but give a good basis to understanding correlational analysis. Please note, in Chapter 10 you might not know some of the the terminology yet, e.g. ANOVA means Analysis of Variance and GLM means General Linear Model (Reading Chapter 1 might help). We will go into depth on these terms in the coming chapters. 10.2.2 Watch Visualisation Have a look at this visualisation of correlations by Kristoffer Magnusson: https://rpsychologist.com/d3/correlation/. After having read Miller and Haden Chapter 11, use this visualisation page to visually replicate the scatterplots in Figures 11.3 and 11.4 - use a sample of 100. After that, visually replicate the scatterplots in Figure 11.5. Each time you change the correlation, pay attention to the shared variance (the overlap between the two variables) and see how this changes with the changing level of association. Also, try setting the correlation to r = .5 and then moving a sinlge dot to see how one data point, a potential outlier, can change the stated correlation value between two variables 10.2.3 Play Guess the correlation Now that you are well versed in interpreting scatterplots (scattergrams) have a go at this online game: http://guessthecorrelation.com/. Read the instuctions on the ‘About’ tab of the menu and then play the game. Try to achieve your highest score possible before giving up - at worst, 50 points. Last year’s top score was 206; do not let us down here! Post your amazing scores on the lab forum pages. The best posted score will win a free copy of Rstudio!!! This sounds a bit of fun but being able to interpret scatterplots with a reasonable degree of accuracy can really help your understanding of a relationship. Job Done - Activity Complete! 10.3 InClass Activity We are going to jump straight into this one! To get you used to running a correlation we will use the examples in Miller and Haden (2013), Chapter 11, looking at the relationship between four variables: reading ability, intelligence (IQ), the number of minutes per week spent reading at home (Home); and the number of minutes per week spent watching TV at home (TV). You can see in this situation that it would be unethical to manipulate these variables so measuring them as they exist in the environment is most appropriate; hence the use of correlations. Click here to download the data for today or download it from Moodle. 10.3.1 Task 1 - The Data After downloading the data folder, unzip the folder, set your working directory appropriately, open a new script, and load in the Miller and Haden data (MillerHadenData.csv), storing it in a tibble called mh. Note 1: Remember that in reality you could store the data under any name you like but to make it easier for a demonstrator to debug with you it is handy if we all use the same names. Note 2: You will find that the instructions for tasks are sparse this semester as we want you to really push the skills you learnt in Semester 1. Don’t worry though, we are always here to help, so if you get stuck, ask! Task 1: Hints for loading in Data Hint 1: We are going to need the following libraries: tidyverse, broom Hint 2: mh &lt;- read_csv() Remember that we will always ask you to use read_csv() to load in data. Let’s look at your data - we showed you a number of ways to do this in Semester 1. As in Miller and Haden, we have 5 columns: the particpant (Participant), Reading Ability (Abil), Intelligence (IQ), number of minutes spent reading at home per week (Home), and number of minutes spent watching TV per week (TV). For the lab we will focus on the relationship between Reading Ability and IQ but for further practice you can look at other relationships in your free time. A probable hypothesis for today could be that as Reading Ability increases so does Intelligence (but do you see the issue with causality and direction). Or phrasing the hypothesis more formally, we hypothesise that the reading ability of school children, as measured through a standardized test, and intelligence, again measured through a standardized test, are positively correlated. This is the hypothesis we will test today but remember that we could always state the null hypothesis that there is no relationship between reading ability and IQ. First, however, we must check some assumptions of the correlation tests. The main assumptions we need to check are: Is the data interval, ratio, or ordinal? Is there a data point for each participant on both variables? Is the data normally distributed in both variables? Does the relationship between variables appear linear? Does the spread have homoscedasticity? We will look at these in turn. Assumption 1: Level of Measurement 10.3.2 Task 2 - Interval or Ordinal Group Discussion Point If we are going to run a Pearson correlation then we need interval or ratio data; Spearman correlations can run with ordinal, interval or ratio data. What type of data do we have? Discuss with your group for a few minutes and then answer the following question. Check your thinking: the type of data in this analysis is most probably ratio interval ordinal nominal as the data is continuous discrete and there is unlikely to be a true zero Hints on data type are the variables continuous? is the difference between 1 and 2 on the scale equal to the difference between 2 and 3? Assumption 2: Pairs of Data All correlations must have a data point for each participant in the two variables being correlated. This should make sense as to why - you can’t correlate against an empty cell! So now go check that you have a data point in both columns for each participant. After that, try answering the question in Task 3 10.3.3 Task 3 - Missing Data It looks like everyone has data in all the columns but let’s test our skills a little whilst we are here. Answer the following questions: How is missing data represented in a tibble? an empty cell NA a large number don’t know Which code would leave you with just the participants who were missing Reading Ability data in mh: filter(mh, is.na(Ability) filter(mh, is.na(Abil) filter(mh, !is.na(Ability) filter(mh, !is.na(Abil) Which code would leave you with just the participants who were not missing Reading Ability data in mh: filter(mh, is.na(Ability) filter(mh, is.na(Abil) filter(mh, !is.na(Ability) filter(mh, !is.na(Abil) Hints on removing missing data points filter(dat, is.na(variable)) versus filter(dat, !is.na(variable)) Assumption 3: The shape of my data The remaining assumptions are all best checked through visualisations. You could use histograms to check that the data (Abil and IQ) are both normally distributed, and you could use a scatterplot (scattergrams) of IQ as a function of Abil to check whether the relationship is linear, with homoscedasticity, and without outliers! You could also use z-scores to check for outliers with the cut-off usually being set at around \\(\\pm2.5SD\\), and you can check this in your free time using the mutate function (e.g. mutate(z = (X - mean(X))/SD(X))), but today we will just use visual checks. 10.3.4 Task 4 - Normality Create the following figures and discuss the outputs with your group: A histogram for Ability and a histogram for IQ. Are they both normally distributed? A scatterplot of IQ (IQ) as a function of Ability (Abil). Do you see any outliers? Does the relationship appear linear? Does the spread appear ok in terms of homoscedasticity? Hints to create figures ggplot(mh, aes(x = )) + geom_histogram() ggplot(mh, aes(x = , y = )) + geom_point() Normality: something to keep in mind is that there are only 25 participants, so how ‘normal’ do we expect relationships to be homoscedasticity is that the spread of data points around the (imaginary) line of best fit is even on both sides along the line; as opposed to very narrow at one point and very wide at others. Remember these are all judgement calls! Descriptives of a correlation A key thing to keep in mind is that the scatterplot is actually the descriptive of the correlation. Meaning that in an article, or in a report, you would not only use the scatterplot to determine which type of correlation to use but also to describe the potential relationship in regards to your hypothesis. So you would always expect to see a scatterplot in the write-up of this type of analysis 10.3.5 Task 5 - Descriptives Group Discussion Point Looking at the scatterplot you created in Task 4, spend a couple of minutes discussing and describing the relationship between Ability and IQ in terms of your hypothesis. Remember this is a descriptive analysis at this stage, so nothing is confirmed. Does the relationship appear to be as we predicted in our hypothesis? Hints on discussing descriptives Hint 1: We hypothesised that reading ability and intelligence were positively correlated. Is that what you see in the scatterplot? Hint 2: Keep in mind it is subjective at this stage. Hint 3: Remember to only talk about a relationship and not a prediction. This is correlational work, not regression. Hint 4: Can you say something about both the strength (weak, medium, strong) and the direction (positive, negative)? The correlation Finally we will run the correlation using the cor.test() function. Remember that for help on any function you can type ?cor.test in the console window. The cor.test() function requires: the column name of Variable 1 the column name of Variable 2 the type of correlation you want to run: e.g. “pearson”, “spearman” the type of NHST tail you want to run: e.g. “one.sided”, “two.sided” For example, if your data is stored in dat and you are wanting a two-sided pearson correlation of the variables (columns) X and Y, then you would do: cor.test(dat$X, dat$Y, method = &quot;pearson&quot;, alternative = &quot;two.sided&quot;) where dat$X means the column X in the tibble dat. The dollar sign ($) is a way of indexing, or calling out, a specific column. 10.3.6 Task 6 - Pearson or Spearman? Based on your answers to Task 5, spend a couple of minutes deciding with your group which correlation method to use (e.g. pearson or spearman) and the type of NHST tail to set (e.g. two.sided or one.sided). Now, run the correlation between IQ and Ability and save it in a tibble called results (hint: broom::tidy()). Hints to correlation Hint 1: the data looked reasonably normal and linear so method would be? Hint 2: results &lt;- cor.test(mh$Abil……, method = ….., alternative….) %&gt;% tidy() Interpreting the Correlation You should now have a tibble called results that gives you the output of the correlation between Reading Ability and IQ for the school children measured in Miller and Haden (2013) Chapter 11. All that is left to do now, is interpret the output of the correlation. 10.3.7 Task 7 - Interpretation Look at results. Locate your correlation value, e.g. results %&gt;% pull(estimate) and then with your group, answer the following questions: The direction of the relationship between Ability and IQ is: positive negative no relationship The strength of the relationship between Ability and IQ is: strong medium weak Based on \\(\\alpha = .05\\) the relationship between Ability and IQ is: significant not significant Based on the output, given the hypothesis that the reading ability of school children, as measured through a standardized test, and intelligence, again through a standardized test, are positively correlated, we can say that the hypothesis: is supported is not supported is proven is not proven Hints to interpretation Hint1: If Y increases as X increases then the relationship is positive. If Y increases as X decreases then the relationship is negative. If there is no change in Y as X changes then there is no relationship Hint2: Depending on the field most correlation values greater than .5 would be strong; .3 to .5 as medium, and .1 to .3 as small. Hint3: The field standard says less than .05 is significant. Hint4: Hypotheses can only be supported or not supported, never proven. Recap so far Great, so far we have set a hypothesis for a correlation, checked the assumptions, run the correlation and interpreted it appropriately. So as you can see running the correlation is the easy bit. As in a lot of analyses it is getting your data in order, checking assumptions, and interpreting your output that is the hard part. We have now walked you through one analysis but you can always go run more with the Miller and Haden dataset. There are six in total that could be run but watch out for Multiple Comparisons - where your Type 1 Error rate is inflated and where the chance of finding a significant effect is inflated by simply running numerous tests. Alternatively, we have another data set below that we want you to run a correlation on yourself but first we want to show you something that can be very handy when you want to view lots of correlations at once. 10.3.8 Advanced 1: Matrix of Scatterplots Above we ran one correlation and if we wanted to do a different correlation then we would have to edit the cor.test() line and run it again. However, when you have lots of variables in a dataset, to get a quick overview of patterns, one thing you might want to do is run all the correlations at the same time or create a matrix of scatterplots at the one time. You can do this with functions from the Hmisc library - already installed in the Boyd Orr Labs. We will use the Miller and Haden data here again which you should still have in a tibble called mh. First, we need to get rid of the Participant column as we don’t want to correlate that with anything. It won’t tell us anything. Copy and run the below line of code library(&quot;Hmisc&quot;) library(&quot;tidyverse&quot;) mh &lt;- read_csv(&quot;MillerHadenData.csv&quot;) %&gt;% select(-Participant) Now run the following line. The pairs() function from the Hmisc library creates a matrix of scatterplots which you can then use to view all the relationships at the one time. pairs(mh) Figure 10.1: Matrix of Correlation plots of Miller and Haden (2013) data And the rcorr() function creates a matrix of correlations and p-values. But watch out, it only accepts the data in matrix format. Run the following two lines of code. mh_mx = as.matrix(mh, type = &quot;pearson&quot;) rcorr(mh_mx) ## Abil IQ Home TV ## Abil 1.00 0.45 0.74 -0.29 ## IQ 0.45 1.00 0.20 0.25 ## Home 0.74 0.20 1.00 -0.65 ## TV -0.29 0.25 -0.65 1.00 ## ## n= 25 ## ## ## P ## Abil IQ Home TV ## Abil 0.0236 0.0000 0.1624 ## IQ 0.0236 0.3337 0.2368 ## Home 0.0000 0.3337 0.0005 ## TV 0.1624 0.2368 0.0005 10.3.9 Task 8 - The Matrix After running the above lines, spend a few minutes answering the following questions with your group. The solutions are at the end of the chapter. The first table outputted is the correlation values and the second table is the p-values. Why do the tables look symmetrical around a blank diagonal? What is the strongest positive correlation? What is the strongest negative correlation? Hints to Matrix of correlations Hint1: There is no hint, this is just a cheeky test to make sure you have read the correlation chapter in Miller and Haden, like we asked you to! :-) If you are unsure of these answers, the solutions are at the end of the chapter. 10.3.10 Advanced 2: Attitudes towards Vaping Great work so far! Now we really want to see what you can do yourself. In the data folder there is another file called VapingData.csv. This data comes from a lab we used to run looking at implicit and explicit attitudes towards vaping. Explicit attitudes were measured via a questionnaire where higher scores indicated a positive attitude towards vaping. Implicit attitudes were measured through an Implicit Association Test (IAT) using images of Vaping and Kitchen utensils and associating them with positive and negative words. The IAT works on the principal that associations that go together (that are congruent, e.g. warm and sun) should be quicker to respond to than associations that do not go together (that are incongruent, e.g. warm and ice). You can read up more on the procedure at a later date here on the Noba Project which has a good description of the procedure under the section “Subtle/Nonsconscious Research Methods”. For today, you need to know that “Block 3” in the experiment tested reaction times and accuracy towards congruent associations, pairing positive words with Kitchen utensils and negative words with Vaping. “Block 5” in the experiment tested reaction times and accuracy towards incongruent associations, pairing positive words with Vaping and negative words with Kitchen Utensils. As such, if reaction times were longer in Block 5 than in Block 3 then people are considered to hold the view that Vaping is negative (i.e. congruent associations are quicker than incongruent associations). However, if reaction times were shorter in Block 5 than in Block 3 then people are considered to hold the view that Vaping is positive (i.e. incongruent associations were quicker than congruent associations). The difference between reaction times in Block5 and Block3 is called the participants IAT score. 10.3.11 Task 9 - Attitudes to Vaping Load in the data in VapingData.csv and analyse it to test the hypothesis that Implicit and Explicit attitudes towards Vaping are positively related. Here are some pointers, hints and tips. Start of by looking at the data. You have 8 columns. Reaction times and Accuracy scores for blocks 3 and 5. The Explicit Vaping Questionnaire Scores, Sex and Age. Accuracy is as a proportion and as such can’t go above 1. Participants entered their own data so some might have made a mistake. Get rid of everybody who had an accuracy greater than 1 in either block 3 or block 5. We also only want participants that were paying attention so best remove anybody whose average accuracy score across Blocks 3 and 5 was less than 80%. Note - this value is aribtrary and if you wanted, in your own experiment, you could use a more relaxed or strict cut-off based on other studies or guidance. Note that these decisions should be set out at the start of your research. Finally, in this instance, remember, the values are in proportions not percentages (so 80% will be .8). Now create an IAT score for participants by subtracting Block 3 reaction times (RT) away from Block 5 reaction times e.g (B5-B3). Use the paragraph above to understand how the scores relate to attitudes. Create a descriptives summary of the number of people, the mean RT and Vaping Questionnaire Score. Why might these averages be useful? Why are averages not always useful in correlations? Check your assumptions of correlations as we did above and descriptives, thinking about how it compares to the hypothesis. Run the appropriate correlation based on your assumptions and interpret the output. Hints for Vaping libraries might include tidyverse and broom Hint Step 1: read_csv() Hint Step 2: filter(Accuracy &lt; 1 OR Accuracy &lt;= 1 OR Accuracy &gt; 1 OR Accuracy &gt;= 1) Hint Step 3: average accuracy: mutate(data, name = (1 + 2)/2) %&gt;% filter(name &gt; …) Hint Step 4: RT: mutate(data, nom = 1 - 2) Hint Step 5: descriptives &lt;- summarise() Hint Step 6: assumptions would be type of data, normality, linear relationship, homoscedasicity, data points for everyone! Hint Step 7: results &lt;- cor.test(method = “pearson”)? Job Done - Activity Complete! Excellent work, who would have thought that about Explicit and Implicit attitudes towards Vaping?! You should now be ready to complete the Homework Assignment for this lab. The assignment for this Lab is summative and should be submitted through the Moodle Level 2 Assignment Submission Page no later than 1 minute before your next lab. If you have any questions, please post them on the slack forum under the channel #level2_2019. Finally, don’t forget to add any useful information to your Portfolio before you leave it too long and forget. We won’t incorporate Portfolio points this semester as by now you should know what sort of information you need to make a note of for yourself, and the more independent you are in your learning the better it will be, but please don’t think they are no longer relevant! 10.4 Assignment This is a summative assignment. Instructions as to how to access and submit your assignment will be made available during the course. 10.5 Solutions to Questions Below you will find the solutions to the questions for the Activities for this chapter. Only look at them after giving the questions a good try and speaking to the tutor about any issues. 10.5.1 InClass Activity 10.5.1.1 InClass Task 1 Loading in the data and the two libraries needed Good point to remind you that: we use read_csv() to load in data the order that libraries are read in is important. If there are any conflicts in terms of libraries then the last library that is loaded will be the functions you are using. library(&quot;broom&quot;) library(&quot;tidyverse&quot;) mh &lt;- read_csv(&quot;MillerHadenData.csv&quot;) Return to Task 10.5.1.2 InClass Task 2 Actually the information within the textbook is unclear as to whether the data is interval or ordinal so we have accepted both as you could make a case for both arguments. A quick google search will show just as many people who think that IQ is interval as think it is oridinal. In terms of Reading Ability, again we probably don’t know enough information about this scale to make a clear judgement but it is at least ordinal and could well be interval. Return to Task 10.5.1.3 InClass Task 3 Missing data is represented by NA. It stands for Not Available but is a very good way of improving your Scottish accent. For example, “is that a number” can be replied with “NA!”. If you want to keep everybody from the whole dataset that has a score for Ability you would use: filter(mh, !is.na(Abil)) ## # A tibble: 25 x 5 ## Participant Abil IQ Home TV ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 61 107 144 487 ## 2 2 56 109 123 608 ## 3 3 45 81 108 640 ## 4 4 66 100 155 493 ## 5 5 49 92 103 636 ## 6 6 62 105 161 407 ## 7 7 61 92 138 463 ## 8 8 55 101 119 717 ## 9 9 62 118 155 643 ## 10 10 61 99 121 674 ## # ... with 15 more rows Alternatively, if you want to keep everybody from the whole dataset that does not have a score for Ability you would use: filter(mh, is.na(Abil)) ## # A tibble: 0 x 5 ## # ... with 5 variables: Participant &lt;dbl&gt;, Abil &lt;dbl&gt;, IQ &lt;dbl&gt;, ## # Home &lt;dbl&gt;, TV &lt;dbl&gt; Remember that you would need to store the output of this step, so really it would be something like mh &lt;- filter(mh, !is.na(Abil)) Return to Task 10.5.1.4 InClass Task 4 Reading ability data appears as normal as expected for 25 participants. Hard to say how close to normality something should look when there are so few participants. ggplot(mh, aes(x = Abil)) + geom_histogram(binwidth = 5) Figure 10.2: Histogram showing the distribution of Reading Ability Scores from Miller and Haden (2013) IQ data appears as normal as expected for 25 participants ggplot(mh, aes(x = IQ)) + geom_histogram(binwidth = 5) Figure 10.3: Histogram showing the distribution of IQ Scores from Miller and Haden (2013) The relationship between reading ability and IQ scores appears appears linear and with no clear outliers. Data also appears homeoscedastic. ggplot(mh, aes(x = Abil, y = IQ)) + geom_point() Figure 10.4: Scatterplot of IQ scores as a function of Reading Ability from Miller and Haden (2013) data Return to Task 10.5.1.5 InClass Task 5 Based on the scatterplot we might suggest that as reading ability scores increase, IQ scores also increase and as such it would appear that our data is inline with our hypothesis that the two variables are positively correlated. This appears to be a medium strength relationship. Return to Task 10.5.1.6 InClass Task 6 We are going to run a pearson correlation as we would argue the data is interval and the relationship is linear. The correlation would be run as follows - tidying it into a nice and useable table. results &lt;- cor.test(mh$Abil, mh$IQ, method = &quot;pearson&quot;, alternative = &quot;two.sided&quot;) %&gt;% tidy The output of the table would look as follows: Table 10.1: The correlation output of the Reading Ability and IQ relationship. estimate statistic p.value parameter conf.low conf.high method alternative 0.451 2.425 0.024 23 0.068 0.718 Pearson’s product-moment correlation two.sided Return to Task 10.5.1.7 InClass Task 7 In the Task 6 output: The correlation value (r) is stored in estimate The degrees of freedom (N-2) is stored in parameter The p-value is stored in p.value And statistic is the t-value associated with this analysis as correlations use the t-distribution (same as in Semester 1) to determine probability of an outcome. pvalue &lt;- results %&gt;% pull(p.value) %&gt;% round(3) df &lt;- results %&gt;% pull(parameter) correlation &lt;- results %&gt;% pull(estimate) %&gt;% round(2) And we can use that information to write-up the following using inline coding for accuracy: A pearson correlation found reading ability and intelligence to be positively correlated with a medium to strong relationship, (r(`r df`) = `r correlation`, p = `r pvalue`). As such we can say that our hypothesis is supported and that there appears to be a relationship between reading ability and IQ in that as reading ability increases so does intelligence. Which when knitted would read as: A pearson correlation found reading ability and intelligence to be positively correlated with a medium to strong relationship, (r(23) = 0.45, p = 0.024). As such we can say that our hypothesis is supported and that there appears to be a relationship between reading ability and IQ in that as reading ability increases so does intelligence. Return to Task 10.5.1.8 InClass Task 8 The table looks the same across the diaganol because the correlation of e.g. Abil vs Abil is not shown, and the correlation of Abil vs Home is the same as the correlation of Home vs Abil The strongest positive correlation is between the number of minutes spend reading at home (Home) and Reading Ability (abil), r(23) = .74, p &lt; .001 The strongest negative correlation is between the number of minutes spend reading at home (Home) and minutes spent watching TV per week (TV), r(23) = -.65, p &lt; .001 Return to Task 10.5.1.9 InClass Task 9 Step 1 Reading in the Vaping Data using read_csv() dat &lt;- read_csv(&quot;VapingData.csv&quot;) Steps 2 to 4 The main wrangle of parts 2 to 4 dat &lt;- dat %&gt;% filter(IAT_BLOCK3_Acc &lt;= 1) %&gt;% filter(IAT_BLOCK5_Acc &lt;= 1) %&gt;% mutate(IAT_ACC = (IAT_BLOCK3_Acc + IAT_BLOCK5_Acc)/2) %&gt;% filter(IAT_ACC &gt; .8) %&gt;% mutate(IAT_RT = IAT_BLOCK5_RT - IAT_BLOCK3_RT) Step 5 It is always worth thinking about which averages are informative and which are not. Knowing the average explicit attitude towards vaping could well be informative. In contrast, if you are using an ordinal scale and people use the whole of the scale then the average may just tell you the middle of the scale you are using - which you already know and really isnt that informative. So it is always worth thinking about what your descriptives are calculating. descriptives &lt;- dat %&gt;% summarise(n = n(), mean_IAT_ACC = mean(IAT_ACC), mean_IAT_RT = mean(IAT_RT), mean_VPQ = mean(VapingQuestionnaireScore, na.rm = TRUE)) Step 6 A couple of visual checks of normality through histograms ggplot(dat, aes(x = VapingQuestionnaireScore)) + geom_histogram(binwidth = 10) ## Warning: Removed 11 rows containing non-finite values (stat_bin). Figure 10.5: Histogram showing the distribution of Scores on the Vaping Questionnaire (Explicit) ggplot(dat, aes(x = IAT_RT)) + geom_histogram(binwidth = 10) Figure 10.6: Histogram showing the distribution of IAT Reaction Times (Implicit) A check of the relationship between reaction times on the IAT and scores on the Vaping Questionnaire Remember that, often, the scatterplot is considered the descriptive of the correlation, hence why you see them including in journal articles to support the stated relationship. The scatterplot can be used to make descriptive claims about the direction of the relationship, the strength of the relationship, whether it is linear or not, and to check for outliers and homeoscedasticity. ggplot(dat, aes(x = IAT_RT, y = VapingQuestionnaireScore)) + geom_point() + theme_bw() ## Warning: Removed 11 rows containing missing values (geom_point). Figure 10.7: A scatterplot showing the relationship between implicit IAT reaction times (x) and explicit Vaping Questionnaire Scores (y) A quick look at the data reveals some people do not have a Vaping Questionnaire score and some don’t have an IAT score. The correlation only works when people have a score on both factors so we remove all those that only have a score on one of the factors. dat &lt;- dat %&gt;% filter(!is.na(VapingQuestionnaireScore)) %&gt;% filter(!is.na(IAT_RT)) Step 7 The analysis and the write-up. results &lt;- cor.test(dat$VapingQuestionnaireScore, dat$IAT_RT, method = &quot;pearson&quot;) %&gt;% tidy() correlation &lt;- results %&gt;% pull(estimate) df &lt;- results %&gt;% pull(parameter) pvalue &lt;- results %&gt;% pull(p.value) With inline coding: Testing the hypothesis that there would be a relaionship beween implicit and explicit attitudes towards vaping, a pearson correlation found no significant relationship between IAT reaction times (implicit attitude) and answers on a Vaping Questionnaire (explicit attitude), r(`r df`) = `r correlation`, p = `r pvalue`. Overall this suggests that there is no direct relationship between implicit and explicit attitudes when relating to Vaping and as such our hypothesis was not supported. and appears as when knitted: Testing the hypothesis that there would be a relaionship beween implicit and explicit attitudes towards vaping, a pearson correlation found no significant relationship between IAT reaction times (implicit attitude) and answers on a Vaping Questionnaire (explicit attitude), r(143) = -0.1043797, p = 0.2115059. Overall this suggests that there is no direct relationship between implicit and explicit attitudes when relating to Vaping and as such our hypothesis was not supported. Remember though that r-values and p-values are often rounded to three decimal places, so a more appropriate write up would be: Testing the hypothesis that there would be a relaionship beween implicit and explicit attitudes towards vaping, a pearson correlation found no significant relationship between IAT reaction times (implicit attitude) and answers on a Vaping Questionnaire (explicit attitude), r(143) = -.104, p = .212. Overall this suggests that there is no direct relationship between implicit and explicit attitudes when relating to Vaping and as such our hypothesis was not supported. Chapter Complete! "],
["introduction-to-glm-one-factor-anova.html", "Lab 11 Introduction to GLM: One-factor ANOVA 11.1 Overview 11.2 PreClass Activity 11.3 InClass Activity 11.4 Assignment 11.5 Solutions to Questions", " Lab 11 Introduction to GLM: One-factor ANOVA 11.1 Overview A key way that we attempt to learn from data is to build a statistical model that captures relationships among variables. In this chapter we will introduce you to the General Linear Model (GLM) and you will read about this approach in the Miller and Haden (2013) textbook (Chapters 1-3) as part of the PreClass. This is a common approach in statistics in Psychology and it encapsulates a range of common analytical techniques that you are already familiar with and will become even more familiar with throughout this semester as we will spend some of the next few lessons looking at it and reading about it. The GLM covers all the t-tests and correlations you have looked at, and the ANOVA and regression we are going to come on to. One of the best ways to learn about linear models is to do things “by hand” on a “toy” dataset. As such, the goals of this inclass assignment are: to recap and practice entering data into a “tibble” (tidyverse data frame - as in Semester 1 Lab 5); to learn how to estimate model parameters from a dataset; to learn how to derive a decomposition matrix that expresses each observation as a linear sum of model components and error. These terms will become more familiar to you over the labs and from reading Miller and Haden, but remember to make notes for yourself to help your solidify your learning. Note: You will notice a slight change in the assignments this semester in that you are required to do a little more computation than before, but all the skills you need will of course be shown to you or you already have them. Last semester we developed your general practical data skills and now this semester we want to up your understanding of the analysis and data you are working with by working through the models by hand before running the functions. We did a little of this with the t-test but this time we will focus on the General Linear Model (GLM) as this one modelling approach covers a number of different analyses. As always ask as many questions as you like! 11.2 PreClass Activity The PreClass Activity for this Chapter is just reading. It is quite a bit of reading but don’t worry if you don’t understand it all first time round. The best way to look at it will be to read through as prep, get the gist, then use it to support the class activity and re-read to consolidate knowledge for the assessment. 11.2.1 Read Chapters As preparation, please read chapters 1 to 3 of Miller and Haden (2013). In this week’s InClass activity we will be working up to the concept of Sums of Squares which is around Section 3.4 of Miller and Haden (2013) Job Done - Activity Complete! 11.3 InClass Activity One-factor ANOVA: Worked example Today we are going to start with a step-by-step example of building a model for an ANOVA (Analysis of Variance). After that you will perform the steps yourself. If you feel comfortable with the examples in Chapter 3 of Miller and Haden, feel free to skim this part and move onto the exercises below. You can find further examples and step-by-step walkthroughs at the end of Miller and Haden (2013), Chapter 3. NOTE: We assume from the preclass reading that you are now at least familiar with the terms estimation equations, decomposition matrices, and sums of squares, though perhaps you don’t quite fully understand them. This activity will help. An ANOVA is a method of analysis for analysing data where you have more than two conditions (levels) for an independent variable (factor), and/or you have more than one independent variable (factors). Thinking back, a t-test is where you normally only have two conditions (levels), right? Well an ANOVA is just an extension of that. For example instead of just comparing IQ scores for professors vs. hooligans (t-test), you can compare IQ scores for professors vs. hooligans vs. politicians (ANOVA); though one might argue that the latter two conditions (levels) are one and the same. Let’s assume that you have data from a one-factor design with three-levels (i.e. one independent variable with three conditions like above). To make this example concrete, let’s pretend you are studying how consuming food before an exam affects student performance. You randomly assign 12 participants to three separate groups (four participants per group): (1) no food, glass of water only (Control); (2) all-you-can-eat buffet (Buffet); and (3) side salad (Salad). (We chose a small number of participants to simplify the computations; obviously if you were going to do this study in real life, you’d need far more than 12 participants to make this worthwhile). For your dependent variable, you measure the number of questions answered correctly on a difficult exam (100 points possible). The exam is administered right after consuming the meal (or drinking water, for the control group). This is called a one-factor design because there is a single factor, which we might call “pre-exam consumption”, that has three different levels: water, buffet, and salad. It is a between-subjects design because there are different people in each group. In textbooks you might see this referred to as a one-way between-subjects ANOVA. For our analysis, we want to test whether there is any difference in exam performance across the levels of the factor. We won’t complete the analysis today but we will look at setting up our model which we would then take on further to see if there is a difference between groups. Here’s how the exam performance looks like for each of the three groups: Control: 37, 80, 64, 51 Buffet: 33, 47, 55, 41 Salad: 59, 23, 50, 60 Quickfire Questions To make sure you understand, answer the following questions about the above experiment: Factor is another name for a level condition control variable of the experiment Level is another name for a factor condition control variable of the experiment In this experiment we have 1 factor with 3 levels 1 level with 3 factors 3 variables with 1 condition Because each group contains different participants then this is a within-subjects design between-subjects design mixed design The fourth participant in the Control condition scored 60 41 51 64 on the exam Estimating model components Great so we understand our experiment! Now, the General Linear Model (GLM) that we will fit to these data is: \\(Y_{ij} = \\mu + A_i + S(A)_{ij}\\) Where: \\(Y_{ij}\\) is the observed value for observation \\(j\\) of group \\(i\\) - i.e. a given participant’s score in a given group; \\(\\mu\\) (pronounced “mu”) is the population grand mean (estimated by the sample grand mean); \\(A_i\\) is the deviation of the population mean of group \\(i\\) from the population grand mean; \\(S(A)_{ij}\\) is the error or residual, defined as the observed value (\\(Y_{ij}\\)) minus the model prediction (\\(\\hat{Y}_{ij}\\)), or the actual value minus the predicted value. Finally, the sum of \\(\\mu\\) + \\(A_i\\) is known as the fitted value or the typical value or the predicted value of a participant in a condition and is written as: \\(\\hat{Y}_{ij}\\). The “party hat” that \\({Y}_{ij}\\) is wearing in this part, i.e. \\(\\hat{Y}_{ij}\\), is there to remind us that it is not the actual score of your participant (in a given condition), but an estimate of that value. So, when we are working in predicted values (values we haven’t actually collected, just predicted) then we stick the “party hat” on the symbol. If we are working in real values (that we have collected) then no party hat! We begin by applying the estimation equations. Our estimate of the population grand mean \\(\\mu\\), will be based on the grand mean of the sample. We will call this \\(\\hat{\\mu}\\) (notice once again, the “party hat”). The estimation equations for our model (seen in Table 3.3 on page 18 of Miller and Haden (2013)) are: \\(\\hat{\\mu} = Y_{..}\\) \\(\\hat{A}_i = Y_{i.} - \\hat{\\mu}\\) \\(\\widehat{S(A)}_{ij} = Y_{ij} - (\\hat{\\mu} + \\hat{A}_i) = Y_{ij} - \\hat{\\mu} - \\hat{A}_i\\) where \\(Y_{..}\\) is the mean of all 12 observations in the sample a.k.a. the baseline \\(Y_{i.}\\) is the mean of the 4 observations in group \\(i\\). \\(\\hat{A}_i\\) is the respective group mean minus the sample mean a.k.a. between-subjects variance \\(\\widehat{S(A)}_{ij}\\) is the individual error term for a given participant, or how much they deviate from the group contribution. Applying these estimation equations to the data above yields the following decomposition matrix: Table 11.1: Decomposition Matrix of our data i j Yij mu Ai err 1 1 37 50 8 -21 1 2 80 50 8 22 1 3 64 50 8 6 1 4 51 50 8 -7 2 1 33 50 -6 -11 2 2 47 50 -6 3 2 3 55 50 -6 11 2 4 41 50 -6 -3 3 1 59 50 -2 11 3 2 23 50 -2 -25 3 3 50 50 -2 2 3 4 60 50 -2 12 In the above table: the column mu represents the value of \\(\\hat{\\mu}\\), the columns Ai represents the value of \\(\\hat{A}_i\\), and the column err represents the value of \\(\\widehat{S(A)}_{ij}\\). Spend a few moments understanding how this table expresses each of the 12 observed values in our example (the \\(Y_{ij}\\)s) in terms of the linear model: \\(Y_{ij} = \\mu + A_i + S(A)_{ij}\\). For example, if the Control group is group \\(i\\) = 1, then for the first participant, \\(j\\) = 1, you would get: \\(Y_{ij} = \\mu + A_i + S(A)_{ij}\\) \\(Y_{ij} = mu + Ai + err\\) \\(37 = 50 + 8 + -21\\) Quickfire Questions To make sure you understand the above equations before going on to calculate your own, answer the following questions about the above table which expresses the GLM decomposition matrix, and then check your answers. In which column of the table are the 12 observed values - i.e. the 12 original scores from the participants? Answer The column named Yij What is the estimated grand mean of this sample? (hint: \\(\\hat{\\mu}\\)) Answer The estimate grand mean of the sample is 50 Which rows of the table contain the data and model estimates for the Buffet group if they are group 2? Answer The rows where i is equal to 2; in other words, rows 5-8 What is the value of \\(\\hat{A}_1\\) and in what rows does it appear? Answer The value would be 8 This value appears in column Ai, rows 1-4, where i equals 1. This can be thought of as the difference between that given group and the sample mean that is applied to all participants within that group. In other words this would be the value of the typical participant in that group - as opposed to an individual participant in that group. Note that, conceptually this gives you the effect of your manipulation (food consumption style) and the different i_s allow you do so for each of the different styles you look at. What is the value of \\(\\widehat{S(A)}_{32}\\)? (hint: this can be read as where i = 3 and j = 2) Answer The value would be -25 This value appears in column err, where i equals 3 and j equals 2. This can be thought of the unique difference for that participant from the sample mean and group mean, meaning that we take into consideration that each subject is unique. What is the model’s prediction for a ‘typical’ participant in the “Salad” group? (hint: (\\(\\hat{Y}_{ij}\\) = mu + Ai) Answer The prediction would be \\(\\hat{Y}_{ij} = \\hat{\\mu} + \\hat{A}_3\\) = 50 + -2 = 48 A ‘typical’ participant is one where the residual is 0. The model prediction is also known as the “fitted value” for this group. Where in the table are the differences found between this ‘typical’ participant prediction and the observed values in the salad group? Answer These are the called the “residuals” (\\(\\widehat{S(A)}_{ij}\\)s) and are found in the err column of the table in rows 9-12. As above they are the difference between that specific individual participant and the typical participant for that group. 11.3.1 Recreate decomposition matrix from the raw data So we have shown you where all the parts of the table come from and how to calculate them. Now, for this part, your task is to reproduce the decomposition matrix tibble shown above, reproduced here: Table 11.2: Decomposition Matrix of our data i j Yij mu Ai err 1 1 37 50 8 -21 1 2 80 50 8 22 1 3 64 50 8 6 1 4 51 50 8 -7 2 1 33 50 -6 -11 2 2 47 50 -6 3 2 3 55 50 -6 11 2 4 41 50 -6 -3 3 1 59 50 -2 11 3 2 23 50 -2 -25 3 3 50 50 -2 2 3 4 60 50 -2 12 You will do this by typing the observed values into a tibble, and then writing code to add columns with estimates of the individual components. At the end, your table should look exactly like the one above. You already know how to do all the data-wrangling elements, so today really try to focus on understanding what the values mean. 11.3.2 Step 1: Create the basic tibble Create a tibble named dmx (short for decomposition matrix). It will eventually contain all of the columns in the one above, but for now, just create the columns i, j, and Yij as they appear above. You already know how to create a tibble (don’t forget to load the tidyverse package first). In case you need to refresh your memory, see page 2 of this cheatsheet on data input or refer back to the preclass activities of Lab 5 Semester 1. You should just type in the values for Yij but try to use the rep() function for columns i (the group) and j (the participant). Step 1 Hints You will need some wrangling functions so don’t forget to load in tidyverse Create a tibble as dmx &lt;- tibble(i = NA, j = NA, Yij = NA) When using the rep() function remember that you can use each or times as calls in rep: rep(1:3, each = 4) When typing in numbers the c() function will allow you to put in numbers such as Column = c(37, 80, 64, 31) 11.3.3 Step 2: Estimate the Grand Mean \\(\\hat{\\mu}\\) Great, we have our group numbers \\(i\\), our participant numbers \\(j\\), and our participant scores \\(Yij\\). Now we need to start expanding out dmx. First thing we need is the grand mean: \\(\\hat{\\mu}\\) Add a column to the table, called mu representing \\(\\hat{\\mu}\\). Call the resulting table dmx2. Remember that you can add a column to a table using mutate(). Step 2 Hints When calculating mu keep in mind that each value of mu should be the grand mean of the sample; the mean of all participants regardless of group. dmx2 &lt;- dmx %&gt;% mutate(mu = ???) 11.3.4 Step 3: Entering the estimates \\(\\hat{A}_1\\), \\(\\hat{A}_2\\), \\(\\hat{A}_3\\) Good! Now we need to add on a column showing the typical effect of being a member of a particular group; the Ai column. Add a column to the table dmx2 called Ai, with the three estimates for \\(\\hat{A}_i\\). Store the resulting tibble in dmx3. \\(\\hat{A}_i\\) is the difference between the grand mean \\(\\hat{\\mu}\\) and the mean of individual groups. This means that you will need to group people by the group they belong to. Add the ungroup() function to the end of your pipeline as you won’t need this grouping after. Step 3 Hints To calculate the column \\(\\hat{A}_i\\) would be something like: dmx3 &lt;- dmx2 %&gt;% group_by() %&gt;% mutate(Ai = something - something) %&gt;% ungroup() 11.3.5 Step 4: Calculate Residuals \\(\\widehat{S(A)}_{ij}\\) Well done, you’re almost there! We just need to add on the final column, err, which is called the residuals or in other words, the difference between the score of a typical participant for that group (mu + Ai) and a given individual participant’ score (Yij). Add a column called err to dmx3 and store the result in dmx4. The column err should contain the residuals - the difference between the observed (Yij) and fitted (typical) values. Step 4 Hints For calculating err you would use: \\(\\widehat{S(A)}_{ij} = Y_{ij} - \\hat{Y}_{ij}\\) where: \\(\\hat{Y}_{ij} = \\hat{\\mu} + \\hat{A}_i\\) 11.3.6 Step 5: Sums of squares Once you have your dmx, you can start performing the calculations. Sums of squares are used in calculations for performing tests on model components, which we will learn more about soon, but you can practice them for now as shown in Section 3.4 of Miller and Haden (2013). Square all the individual values in the columns Yij, mu, Ai, and err in dmx4, Now sum up the squared values for each of these columns. Save these in a variable called sstbl. Here is an example of how you would square a value x: x^2. The ^2 means take x to the power of 2. So typing 3^2 in the console will give you 9 (try it if you’re unsure). It also works for columns! the Sums of Squares of Yij is called the Sums of Squares total or \\(SS_{total}\\). the Sums of Squares of mu is called the Sums of Squares of the grand mean or \\(SS_{\\mu}\\) and sometimes called the intercept. the Sums of Squares of Ai is called the Sums of Squares of A or \\(SS_{A}\\) and sometimes called \\(SS_{between}\\). the Sums of Squares of err is called the Sums of Squares error or \\(SS_{error}\\) and sometimes called \\(SS_{within}\\). Step 5 Hints Mutate on the squared values: dmx4 %&gt;% mutate(Yij2 = Yij^2, …) And sum up using: summarise(ss_Y = sum(Yij2), ss_mu = …) Job Done - Activity Complete! Well done! Display dmx4 in the console and compare it to the table above. How did you do? If the values in your dmx4 do not match the table above then you need to go back and look at where it has gone wrong. Alternatively you should look at the solutions at the end of the chapter. To recap, what we are doing here is setting up the ANOVA to compare the three groups to see if there is a significant difference between each group. Doing it this way allows us to get an understanding of where the numbers come from, and it highlights that the analysis is about comparing variance within groups and variance between groups. If the variance between groups is larger than the variance within groups then it is likely that your experimental manipulation has had an effect. Conversely, if the variance within groups is larger than the variance between groups then there is likely to be no effect of your experimental manipulation. You should now be ready to complete the Homework Assignment for this lab. The assignment for this Lab is FORMATIVE and is NOT to be submitted and will NOT count towards the overall grade for this module. However, you are strongly encouraged to do the assignment as it will continue to boost your skills which you will need in future assignments. If you have any questions, please post them on the slack forum under the channel #level2_2019 11.4 Assignment Lab 11: Introduction to GLM: One-factor ANOVA In order to complete this assignment you first have to download the assignment .Rmd file which you need to edit for this assignment: titled GUID_Level2_Semester2_Lab2.Rmd. This can be downloaded within a zip file from the below link. Once downloaded and unzipped you should create a new folder that you will use as your working directory; put the .Rmd file in that folder and set your working directory to that folder through the drop-down menus at the top. Download the Assignment .zip file from here or on Moodle. Single Answer and Multiple Choice Questions For this assignment you will answer a series of short single and Mulitple Choice Questions, followed by a calculation of a decomposition matrix in the final task. In order to complete this formative assignment you will need to have completed the inclass activity and have read Miller and Haden Chapter 3. Before starting let’s check: The .Rmd file is saved in a folder and that you have set your working directory to that folder. For assessments we ask that you save it with the format GUID_Level2_Semester2_Lab2.Rmd where GUID is replaced with your GUID. Though this is a formative assessment, it may be good practice to do the same here. Let’s Begin! 11.4.1 Question 1 Consider the following description of a study. You are investigating whether there is seasonal variation in students’ bodyweight. In other words, is there any evidence that bodyweight differs across the four seasons (Winter, Spring, Summer, and Fall - #AllYouGotToDoIsCall)? Which of the models shown below would be the the general linear model corresponding to this study? \\(Y_{ij} = \\mu + A_{ij} + S(A)_{ij}\\) \\(Y_{ijkm} = \\mu + A_{i} + B_{j} + C_{k} + D_{m} + S_{ijkm}\\) \\(Y_{ij} = \\beta_0 + \\beta_1 X_1 + e_{ij}\\) \\(Y_{ijkm} = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + \\beta_4 X_4 + e_{ijkm}\\) Replace the NULL in the Q1 code chunk with the statement number that corresponds to the correct answer (e.g. 1, 2, 3 or 4). mcq1 &lt;- NULL For the next few questions consider the decomposition matrix for a one-factor design with three groups, shown below . \\(i\\) \\(j\\) \\(Y_{ij}\\) \\(\\hat{\\mu}\\) \\(\\hat{A}_{i}\\) \\(\\widehat{S(A)}_{ij}\\) 1 1 4 6 -1 -1 1 2 6 6 -1 1 2 1 4 6 0 -2 2 2 8 6 0 2 3 1 2 6 1 -5 3 2 12 6 1 5 11.4.2 Question 2 According to the above decomposition matrix, the population grand mean is estimated to be: 0 6 36 can’t answer; not observed Replace the NULL in the Q2 code chunk with the statement number that corresponds to the correct answer (e.g. 1, 2, 3 or 4). mcq2 &lt;- NULL 11.4.3 Question 3 According to the above decomposition matrix, the value of \\(\\hat{A}_3\\) is: 6 0 1 can’t answer; not observed Replace the NULL in the Q3 code chunk with the statement number that corresponds to the correct answer (e.g. 1, 2, 3 or 4). mcq3 &lt;- NULL 11.4.4 Question 4 According to the above decomposition matrix, the predicted value for a participant in group 1 is what? Hint: this is the “fitted” or “typical” for that group (\\(\\hat{Y}_{ij}\\)) as opposed to the actual value (\\(Y_{ij}\\)) Replace the NULL in the Q4 code chunk with the actual value of the correct answer (e.g a number). Q4 &lt;- NULL # replace NULL with your answer (a number) 11.4.5 Question 5 Which observation or observations has/have the largest residual(s)? \\(Y_{21}\\) \\(Y_{21}\\) and \\(Y_{22}\\) \\(Y_{31}\\) \\(Y_{31}\\) and \\(Y_{32}\\) Replace the NULL in the Q5 code chunk with the statement number that corresponds to the correct answer (e.g. 1, 2, 3 or 4). Q5 &lt;- NULL 11.4.6 Question 6 From your reading of Miller and Haden Chapter 3, and from the inclass activity Section 5, based on the above decomposition matrix, what would \\(SS_{total}\\) be for this model? Replace the NULL in the Q6 code chunk with the actual value of the correct answer (e.g a number). Q6 &lt;- NULL # replace NULL with your answer (a number) 11.4.7 Question 7 From your reading of Miller and Haden Chapter 3, and from the inclass activity Section 5, based on the above decomposition matrix, what would \\(SS_{error}\\) be for this model? Replace the NULL in the Q7 code chunk with the actual value of the correct answer (e.g a number). Q7 &lt;- NULL # replace NULL with your answer (a number) 11.4.8 Question 8 From reading Miller and Haden Chapter 3, and from the inclass activity Section 5, a study with a one-factor design with GLM \\(Y_{ij} = \\mu + A_{ij} + S(A)_{ij}\\) is found to have the following SS: \\(SS_{total} = 280\\), \\(SS_{\\mu} = 40\\), and \\(SS_{error} = 60\\). Given those values, what is the value of \\(SS_{A}\\)? hint: \\(SS_{total}\\) = \\(SS_{\\mu}\\) + \\(SS_{A}\\) + \\(SS_{error}\\) Replace the NULL in the Q8 code chunk with the actual value of the correct answer (e.g a number). Q8 &lt;- NULL # replace NULL with your answer (a number) 11.4.9 Question 9: Create your own decomposition matrix Finally, this last task tests your ability to set up a decomposition matrix as shown inclass. The code chunk below creates the basic table structure you will need to complete this task. Run the code and have a look at the table, but DO NOT CHANGE IT! ## run this block, have a look at the structure of dsetup, ## but don&#39;t change anything library(&quot;tidyverse&quot;) dsetup &lt;- tibble(i = rep(1:4, each = 3), j = rep(1:3, times = 4), Yij = NA, mu = NA, Ai = NA, err = NA) In the code chunk below, flesh out the values in dsetup to create a decomposition matrix for the data shown below (a one-factor design with four levels), but with the actual numeric values replacing the NA values. Group 1: 84, 86, 61 Group 2: 83, 71, 95 Group 3: 56, 95, 92 Group 4: 68, 76, 93 IMPORTANT! Make sure the final table with your result has the name dmx. Check spelling and capitalization. The values should be computed based on the Yij values such that if the Yij values were to change then your code would still produce the correct decomposition matrix. DO NOT change the column names, the column ordering, and make sure it has the right number of rows and columns. You should have 12 rows by 6 columns. Make sure your code runs without error in a fresh R session, and make sure no warnings are generated by the code chunk named dmx_warning, which validates your response. # TODO: DO STUFF WITH dsetup # you can change or remove the line below, # but make sure your final table is called dmx dmx &lt;- NULL ## Warning: If this warning appears in the output, you have not yet defined ## &#39;dmx&#39; properly. Check column names (including capitalization), column data ## types, and table structure. Job Done - Activity Complete! Well done, you are finshed! Now you should go check your answers against the solutions which can be found at the end of this Chapter. You are looking to check that the resulting output from the answers that you have submitted are exactly the same as the output in the solution - for example, remember that a single value is not the same as a coded answer. Where there are alternative answers it means that you could have submitted any one of the options as they should all return the same answer. If you have any questions please post them on the moodle forum or on the rguppies.slack.com forum #level2_2019. See you in the next lab! 11.5 Solutions to Questions Below you will find the solutions to the questions for the Activities for this chapter. Only look at them after giving the questions a good try and speaking to the tutor about any issues. 11.5.1 InClass Activities 11.5.1.1 InClass Step 1 The basic tibble would be created as follows. When it comes to \\(Y_{ij}\\), simply typing in the values in order was what was needed. library(&quot;tidyverse&quot;) dmx &lt;- tibble(i = rep(1:3, each = 4), j = rep(1:4, times = 3), Yij = c(37, 80, 64, 51, 33, 47, 55, 41, 59, 23, 50, 60)) Return to Task 11.5.1.2 InClass Step 2 The Grand Mean can be added as follows: dmx2 &lt;- dmx %&gt;% mutate(mu = mean(Yij)) And would appear as: Table 11.3: Decomposition Matrix with Grand Mean added i j Yij mu 1 1 37 50 1 2 80 50 1 3 64 50 1 4 51 50 2 1 33 50 2 2 47 50 2 3 55 50 2 4 41 50 3 1 59 50 3 2 23 50 3 3 50 50 3 4 60 50 Return to Task 11.5.1.3 InClass Step 3 The estimates \\(\\hat{A}_1\\), \\(\\hat{A}_2\\), \\(\\hat{A}_3\\), or in other words the unique contribution of each group, are calculated as follows. The key point is grouping by i so that each group is accounted for individually. dmx3 &lt;- dmx2 %&gt;% group_by(i) %&gt;% mutate(Ai = mean(Yij) - mu) %&gt;% ungroup() And would appear as: Table 11.4: Decomposition Matrix with Group Estimates added i j Yij mu Ai 1 1 37 50 8 1 2 80 50 8 1 3 64 50 8 1 4 51 50 8 2 1 33 50 -6 2 2 47 50 -6 2 3 55 50 -6 2 4 41 50 -6 3 1 59 50 -2 3 2 23 50 -2 3 3 50 50 -2 3 4 60 50 -2 Return to Task 11.5.1.4 InClass Step 4 The residuals are calculated as follows: dmx4 &lt;- dmx3 %&gt;% mutate(err = Yij - (mu + Ai)) And would appear as: Table 11.5: Decomposition Matrix with Residuals added i j Yij mu Ai err 1 1 37 50 8 -21 1 2 80 50 8 22 1 3 64 50 8 6 1 4 51 50 8 -7 2 1 33 50 -6 -11 2 2 47 50 -6 3 2 3 55 50 -6 11 2 4 41 50 -6 -3 3 1 59 50 -2 11 3 2 23 50 -2 -25 3 3 50 50 -2 2 3 4 60 50 -2 12 Return to Task 11.5.1.5 InClass Step 5 (version 1) mutate() on the squared values column select() only those columns summarise(sum) those columns sstbl &lt;- dmx4 %&gt;% mutate(Yij2 = Yij^2, mu2 = mu^2, Ai2 = Ai^2, err2 = err^2) %&gt;% select(Yij2, mu2, Ai2, err2) %&gt;% summarise(ss_Y = sum(Yij2), ss_mu = sum(mu2), ss_Ai = sum(Ai2), ss_err = sum(err2)) 11.5.1.6 InClass Step 5 (version 2) There is an alternative way to do the above in a supercool, superquick, two lines of code using dplyr’s “scoping” technique. Have a look at ?dplyr::scoped and ?dplyr::summarise_all. Don’t worry if you don’t understand this yet, as it is pretty advanced, but as you can see it gives the same output as we created in class. sstbl &lt;- dmx4 %&gt;% select(Yij:err) %&gt;% summarise_all(list(name = ~ sum(.^2))) Return to Task 11.5.2 Homework Activities 11.5.2.1 Assignment Question 1 The correct model for this scenario would be: \\(Y_{ij} = \\mu + A_{ij} + S(A)_{ij}\\) As such the correct answer is: mcq1 &lt;- 1 Return to Task 11.5.2.2 Assignment Question 2 The population grand mean for the shown decomposition matrix is \\(\\hat{\\mu}\\) = 6 As such the correct answer is: mcq2 &lt;- 2 Return to Task 11.5.2.3 Assignment Question 3 The value for the shown decomposition matrix is \\(\\hat{A}_3\\) = 1 As such the correct answer is: mcq3 &lt;- 3 Return to Task 11.5.2.4 Assignment Question 4 The “fitted” or “typical” value for a participant in Group 1 would be \\(\\hat{Y}_{ij}\\) = \\(\\mu\\) + \\(A_i\\).. As such the correct answer is: Q4 &lt;- 5 Return to Task 11.5.2.5 Assignment Question 5 The participants/observations with the largest residuals are \\(Y_{31}\\) and \\(Y_{32}\\) As such the correct answer is: Q5 &lt;- 4 Return to Task 11.5.2.6 Assignment Question 6 The \\(SS_{total}\\) for this model would be calculated as: Q6 &lt;- 4^2 + 6^2 + 4^2 + 8^2 + 2^2 + 12^2 As such giving a \\(SS_{total}\\) of 280 Return to Task 11.5.2.7 Assignment Question 7 The \\(SS_{error}\\) for this model would be calculated as: Q7 &lt;- (-1)^2 + 1^2 + (-2)^2 + 2^2 + (-5)^2 + 5^2 As such giving a \\(SS_{error}\\) of 60 Return to Task 11.5.2.8 Assignment Question 8 From reading Miller and Haden Chapter 3, and from the inclass activity Section 5, a study with a one-factor design with GLM \\(Y_{ij} = \\mu + A_{ij} + S(A)_{ij}\\) is found to have the following SS: \\(SS_{total} = 280\\), \\(SS_{\\mu} = 40\\), \\(SS_{error} = 60\\). \\(SS_{total}\\) = \\(SS_{\\mu}\\) + \\(SS_{A}\\) + \\(SS_{error}\\) Given the above values and formula the value of \\(SS_{A}\\) would be \\(SS_{A}\\) = 180 As such the correct answer is: Q8 &lt;- 180 Return to Task 11.5.2.9 Assignment Question 9 Entering the following values: Group 1: 84, 86, 61 Group 2: 83, 71, 95 Group 3: 56, 95, 92 Group 4: 68, 76, 93 dmx can be created as shown: dsetup &lt;- tibble(i = rep(1:4, each = 3), j = rep(1:3, times = 4), Yij = NA, mu = NA, Ai = NA, err = NA) dmx &lt;- dsetup %&gt;% mutate(Yij = c(84, 86, 61, 83, 71, 95, 56, 95, 92, 68, 76, 93), mu = mean(Yij)) %&gt;% group_by(i) %&gt;% mutate(Ai = mean(Yij) - mu) %&gt;% ungroup() %&gt;% mutate(err = Yij - (mu + Ai)) Producing the following output: Table 11.6: Decomposition Matrix of Ch11 Assignment Task 9 i j Yij mu Ai err 1 1 84 80 -3 7 1 2 86 80 -3 9 1 3 61 80 -3 -16 2 1 83 80 3 0 2 2 71 80 3 -12 2 3 95 80 3 12 3 1 56 80 1 -25 3 2 95 80 1 14 3 3 92 80 1 11 4 1 68 80 -1 -11 4 2 76 80 -1 -3 4 3 93 80 -1 14 Return to Task Chapter Complete! "],
["continuing-the-glm-one-factor-anova.html", "Lab 12 Continuing the GLM: One-factor ANOVA 12.1 Overview 12.2 PreClass Activity 12.3 InClass Activity 12.4 Assignment 12.5 Solutions to Questions", " Lab 12 Continuing the GLM: One-factor ANOVA 12.1 Overview In the previous lab you learned how to decompose a dependent variable into components of a linear model, expressing them in terms of a decomposition matrix, before finishing up with calculating the sums of squares. In this lab, we will take it a step further and start exploring the relationships between sums of squares (SS), mean squares (MS), degrees of freedom (df), and F-ratios. In short, in the first part of this class activity, we will show you how you go from the decomposition matrix to actually determining if there is a significant difference or not. We’ve had you work through the calculations step-by-step by hand and in R in order to gain a conceptual understanding. However, when you run an ANOVA, typically the computer does all of these calculations for you. As such, in the second part of the activities, we’ll show you how to run a one-factor ANOVA using the ezANOVA() function in the ez add-on package. From there you will see how the output of this function maps onto the concepts you’ve been learning about. As such, the goals of this chapter are to: to demonstrate how Sums of Squares leads to an F-value, finishing off the decomposition matrix to determine the probability of a F-value for given degrees of freedom (df) to explore using the ezANOVA() function and how the outcome compares to your decomposition matrix calculations. Note: The package ez is already installed on the Boyd Orr machines and only needs read into the library in the normal fashion. Do not install it on the Boyd Orr Lab machines. If you are using your own laptop you will need to make sure you have installed the ez package. 12.2 PreClass Activity As per the previous Chapter, the PreClass activity for this lab is reading (re-reading in fact) and trying out some of the activities in the book. We will go over similar activities in the lab so it will really help you to spend a few minutes trying out the activities in the book and thinking about them. 12.2.1 Read Chapters Re-read Chapter 3 of Miller and Haden (2013) and make sure you are understanding it. Particularly focus on how the decomposition matrix leads to the ANOVA output through sums of squares, dfs, and mean squares. 12.2.2 Try Activities Test your understanding by working through Computational Exercise #1 in section 3.12 of Miller and Haden (page 31) all the way to the summary table. The answer is in section 3.13 but be sure to work through the example first. It may also help to review the probability labs and lectures from Semester 1 as we move onto understanding F distributions and p-values. Job Done - Activity Complete! 12.3 InClass Activity One-factor ANOVA: Worked example Let’s start with some simulated data corresponding to a between-subjects design with three groups (conditions/levels) on one factor (variable). In this hypothetical study, you’re investigating the effects of ambient noise on concentration. You have participants transcribe a handwritten document onto a laptop and count the number of typing errors (DV = typos) each participant makes under their respective different conditions: while hearing ambient conversation such as you would find in a busy cafe (“cafe” condition); while listening to mellow jazz music (“jazz” condition); or in silence (“silence” condition). Again for practice we will only use small, highly under-powered groups. You have three different participants in each condition. As such, your data are as follows: cafe: 111, 102, 111 jazz: 89, 127, 90 silence: 97, 85, 88 Below is the decomposition matrix for this data set, based on the GLM: \\(Y_{ij} = \\mu + A_i + S(A)_{ij}\\). This is what we did last week in the lab and what you did for the homework activity. You can have a go at creating it yourself from scratch if you like, as good practice, or, in the interests of time, feel free to reveal the code and run that code to create the dmx. Note that we have also included a column called sub_id with a unique identifier for each participant. This is not that important for the dmx but we will definitely need it later for running the ANOVA using the ez::ezANOVA() function, so let’s just include it now so we don’t forget. Reveal DMX code dmx &lt;- tibble(sub_id = 1:9, i = rep(1:3, each = 3), j = rep(1:3, times = 3), typos = c(111, 102, 111, 89, 127, 90, 97, 85, 88), sound = rep(c(&quot;cafe&quot;, &quot;jazz&quot;, &quot;silence&quot;), each = 3)) %&gt;% mutate(mu = mean(typos)) %&gt;% group_by(i) %&gt;% mutate(Ai = mean(typos) - mu) %&gt;% ungroup() %&gt;% mutate(err = typos - (mu + Ai)) Table 12.1: Decomposition Matrix for Typos Example sub_id i j typos sound mu Ai err 1 1 1 111 cafe 100 8 3 2 1 2 102 cafe 100 8 -6 3 1 3 111 cafe 100 8 3 4 2 1 89 jazz 100 2 -13 5 2 2 127 jazz 100 2 25 6 2 3 90 jazz 100 2 -12 7 3 1 97 silence 100 -10 7 8 3 2 85 silence 100 -10 -5 9 3 3 88 silence 100 -10 -2 We finished off last week by calculating the Sums of Squares for the different columns. Remember that the Sums of Squares (or often shortend to \\(SS\\)) is literally squaring the values within a column and summing them up, and that it is a measure of the variance attributable to that part of the model (or that column). The Sums of squares for the above model has the following relationship: \\(SS_{total} = SS_{\\mu} + SS_{A} + SS_{error}\\) Have a go at calculating the SS of the above dmx table using the code we showed you towards the end of the inclass activity last week. If unsure, then the solution is below: Calculating Sums of Squares dat_ss &lt;- dmx %&gt;% summarise(total = sum(typos^2), ss_mu = sum(mu^2), ss_sound = sum(Ai^2), ss_err = sum(err^2)) Which would give: Table 12.2: Sums of Squares for Typos Example total ss_mu ss_sound ss_err 91574 90000 504 1070 We can check that we have calculated everything correctly by using the following relationship: if: \\(SS_{total} = SS_{\\mu} + SS_{A} + SS_{error}\\) then: 91574 = 90000 + 504 + 1070. 12.3.1 Task 1 - Quick Checks SlowBurner Questions Answer the following questions. The solutions are at the end of the chapter. Calculate the corrected total sum of squares where the corrected total is the \\(SS_{total}\\) minus the part of the total attributable to the intercept (i.e., the grand mean, \\(SS_{\\mu}\\)). What proportion of the corrected total sum of squares is attributable to the main effect of sound? (hint: \\(SS_{sound} = SS_{A}\\)) What proportion of the corrected total is attributable to residual error? (hint: \\(SS_{error}\\)) 12.3.2 Task 2 - Mean squares and degrees of freedom Great, so now we know how to create our decomposition matrix and how to calculate our sums of squares. The only thing left to do is to calculate the F ratio to determine if there is a significant effect between our groups. To do that we first need to calculate some Mean Squares. But, as it is always good to have a view of the whole picture, let’s not forget that the whole purpose here is to show you where the numbers come from in our quest to determine if there is a significant difference between our groups, or in other words, is there an effect of listening condition on concentration! You will remember from your lectures and from your reading of Miller &amp; Haden (2013) that the F value is a ratio of two estimates of population variance: \\(F = \\frac{MS_{between}}{MS_{within}}\\) also sometimes seen as \\(F = \\frac{MS_{treatment}}{MS_{error}}\\) and in Miller and Haden as \\(F = \\frac{MS_{A}}{MS_{S(A)}}\\) And you will also remember that the mean square (MS) is a sums of squares (SS) divided by its degrees of freedom (df). If you don’t remember what degrees of freedom are, go back to pages 21-23 of Miller and Haden (2013). They have a good explanation for it, however, these things are easy to forget, so make sure to qucikly skim back through the book. \\(MS = \\frac{SS}{df}\\) So let’s start putting this together! If we know the SS of our group/treatment (\\(SS_{A}\\) - also called the between variance) and we know the SS of our error/residuals (\\(SS_{error}\\) - also called the within variance), then we can convert both of them to Mean Squares (MS) (i.e. the average variance for that condition) by dividing them by their respective degrees of freedom (df). We can then calculate F observed (also called F ratio) by \\(MS_{A} / MS_{error}\\). If the \\(MS_{error}\\) is larger than \\(MS_{A}\\) (the group effect) then F will be small and there will be no significant effect of group - any difference in groups is purely due to individual differences (another way of thinking about error). On the other hand, if \\(MS_{A}\\) (the group effect) is larger than \\(MS_{error}\\) then F will be large, and depending on how large F is, there may be a significant difference caused by your group variable. With all that in mind, and it may take a couple of readings, try to answer the following questions (consulting Miller &amp; Haden Ch. 3 and your lecture slides where needed). The solutions are at the end of the chapter. SlowBurner Questions Stated in terms of \\(\\mu_{jazz}\\), \\(\\mu_{cafe}\\), and \\(\\mu_{silence}\\), what is the null hypothesis for this specific study of the effects of sound on typographic errors? How many degrees of freedom are there for \\(A_{i}\\), the main effect of sound, if \\(dfA_{i}\\) = k - 1? How many degrees of freedom are there for \\(S(A)_{ij}\\), the error term, if \\(dfS(A)_{ij}\\) = N - \\(dfA_{i}\\) - 1? Calculate \\(MS_{A}\\), where \\(A\\) is the factor sound. Note: You can access individual columns in a table using double square brackets [[]]; for instance dat_ss[[“ss_mu”]] gives you the column ss_mu from dat_ss. This is an alternative to $ that some may know; e.g. dat_ss$mu. Calculate \\(MS_{S(A)}\\). Hints for Task 2 Remember that the null says that there are no differences between conditions. Sound, our factor, has three levels. N is the total number of participants \\(MS_{A} = \\frac{SS_{A}}{dfA_{i}}\\) \\(MS_{S(A)} = \\frac{SS_{error}}{dfS(A)_{ij}}\\) 12.3.3 Task 3 - F-ratios Last step, the F ratio. As above, if the null hypothesis is true, then both estimates of the population variance (\\(MS_{between}\\) and \\(MS_{within}\\)) should line up, and the \\(F\\)-ratio should approach 1 (because \\(x/x = 1\\)). Now, we can’t expect these two estimates to be exactly equal because of sampling bias, so to see how unlikely our observed F-ratio is under the null hypothesis, we have to compare it to the F distribution. To learn a bit about the F distribution we have created a shiny app to play with. Shiny Apps are interactive webpages and applications made through R. Download the app from Moodle or from this link. Unzip the folder, open up the file app.R through R Studio, and click Run app in R Studio to launch it (found at the top right-hand side of script window - there is a green play sign). The App window will open showing you some parameters to adjust and a wonderful F distribution plot. Note: When we are finished with the App, close the App window to start typing in console again. The F distribution is a representation of the probability of various values of F under the null hypothesis. It depends upon two parameters: \\(df_{numerator}\\) and \\(df_{denominator}\\). Play around with the sliders corresponding to these two parameters and observe how the shape of the distribution changes. There is also a slider that lets you specify an observed \\(F\\) ratio (to one digit of precision). It is represented on the blue line of the graph. Move this slider around and watch how the p-values change. The p-value is the total area under the curve to the right of the blue line. The red line on the plot denotes the critical value of F required for a significant difference, given the \\(\\alpha\\) (type 1 error rate) and the \\(df_{numerator}\\) and \\(df_{denominator}\\) . Slow Burner Questions Try using your data and the app to answer the following questions. The solutions are at the end of the chapter. From your data, calculate the observed F ratio (called f_obs) for the effect of sound on typos (concentration). (hint: \\(MS_{between} / MS_{within}\\) Using the app, set \\(\\alpha = .05\\), set the degrees of freedom to correspond to those in your study, and set the observed F ratio as close as you can to the value you got in the above question. Now, according to the app, what is the critical value for \\(F\\) (hint: red line)? According to the app, what is the approximate \\(p\\) value associated with your observed \\(F\\) ratio? Based on these values, do you reject or retain the null hypothesis? Tricky question: Note that you can use the distribution functions for \\(F\\) in the same way you did in previous Semester 1 labs (e.g. Lab 4) for the normal distribution (pnorm(), dnorm(), qnorm()) or for the binomial distribution (pbinom(), dbinom(), qbinom()), keeping in mind however that the F distribution, being continuous, is more analogous to the normal distribution. See ?df for the distribution functions associated with \\(F\\). Using the appropriate distribution function, calculate the \\(p\\) value associated with \\(F_{obs}\\). This will be more precise than the app. Hints for Question 5 look at inputs for the function - ?pf ignore ncp f_obs = q lower.tail? What is the probability of obtaining an F_obs higher than your value. 12.3.4 Task 4 - Using ez::ezANOVA() Great, so we have calculated F for this test and made a judgement about whether it is significant or not. But that was quite a long way of doing it, and whilst it is always great to understand where the data comes from, you don’t want to have to do that each time you run a test. So now we are going to re-analyse the same dataset but this time we are going to have the computer do all the computational work for us. There are various options for running ANOVAs in R, but the function we will be using for this course is ezANOVA() function in the ez add-on package. Note that to use ezANOVA() you either have to load in the package using library(&quot;ez&quot;), or you can call it directly without loading using ez::ezANOVA() (the package_name::function syntax). If you’re just using the function once, the latter often makes more sense. The ez package is already installed in the Boyd Orr machines so it only needs called to the library. On your own machines you will need to install the package if you haven’t already done so. Have a qucik read through the documentation for ezANOVA (type ?ezANOVA in the console) and pay specific attention to how you stipulate the datafile, the dv, the factor, the participants, etc. It also helps to look at some examples. Then try to specify the call to ezANOVA() so that it reproduces the results you got when you did it by hand above. What do you conclude about the effects of ambient noise on concentration? Hints for ezANOVA create a tibble called dat keeping only the columns you need from dmx you need your dv column, your condition column, and your participant id column that we created at the start ezANOVA(data = ?, dv = ?, wid = ?, between = ?) you will get two outputs. One is the F-test. One is Levene’s homogeniety of variance. Make sure you can identify both. Conclusion Not much to be honest with you! The study returns a non-significant finding suggesting that there is no significant effect of ambient noise on concentration, F(2, 6) = 1.413, p = .31, ges = .32. However, before you go off and publish this highly underpowered study we should probably look to replicate it with a larger sample (which you could calculate using your skills from Chapter 8). Job Done - Activity Complete! Excellent work today! And super interesting as well, huh? Quick, everyone to the cafe and don’t worry about the typos!!!! Only joking, we are all going to the cafe to replicate! You should now be ready to complete the Homework Assignment for this lab. The assignment for this Lab is summative and should be submitted through the Moodle Level 2 Assignment Submission Page no later than 1 minute before your next lab. If you have any questions, please post them on the slack forum under the channel #level2_2019. Finally, don’t forget to add any useful information to your Portfolio before you leave it too long and forget. 12.4 Assignment This is a summative assignment. Instructions as to how to access and submit your assignment will be made available during the course. 12.5 Solutions to Questions Below you will find the solutions to the questions for the Activities for this chapter. Only look at them after giving the questions a good try and speaking to the tutor about any issues. 12.5.1 InClass Activities 12.5.1.1 InClass Task 1 Question 1 \\(SS_{corrected\\ total} = SS_{total} - SS_{\\mu}\\) \\(SS_{total}\\) = 91574 \\(SS_{\\mu}\\) = 90000 \\(SS_{corrected\\ total}\\) = 91574 - 90000 = 1574 Question 2 \\(SS_{sound}\\) = 504 \\(SS_{corrected\\ total}\\) = 1574 \\(SS_{sound} / SS_{corrected\\ total}\\) = 504 / 1574 = 0.32 Question 3 \\(SS_{err}\\) = 1070 \\(SS_{corrected\\ total}\\) = 1574 \\(SS_{err} / SS_{corrected\\ total}\\) = 1070 / 1574 = 0.68 Return to Task 12.5.1.2 InClass Task 2 Question 1 \\(\\mu_{cafe} = \\mu_{jazz} = \\mu_{silence}\\) Question 2 k = number of conditions, so: df = k - 1 = 3 - 1 = 2 Question 3 N = 9, and \\(dfA_{i}\\) = 2, so df = 9 - 2 - 1 = 6 Question 4 The factor df = 2 ms_a &lt;- dat_ss[[&quot;ss_sound&quot;]] / 2L Giving \\(MS_{A}\\) = 252 Question 5 The residual df = 6 ms_err &lt;- dat_ss[[&quot;ss_err&quot;]] / 6L Giving \\(MS_{S(A)}\\) = 178.3333333 Return to Task 12.5.1.3 InClass Task 3 Question 1 f_obs &lt;- ms_a / ms_err Giving a F-value of F = 1.413 when rounded to three decimal places. Question 2 The red line should be about F = 5.143, meaning that that is the minimum value of F, for those dfs, considered to be statistically significant at \\(\\alpha = .05\\) Question 3 Reading off the app, the approximate p-value for f_obs is p = .317. Question 4 As \\(p\\) &gt; \\(\\alpha\\) we would retain the null hypothesis that there is no significant effect of group in this study Question 5 This could alternatively calculated using coding as such: pf(f_obs, 2, 6, lower.tail = FALSE) Which would give a p-value of p = 0.314 and again could be written up as F(2, 6) = 1.413, p = 0.314, suggesting no significant effect of ambiance or location on concentration as measured through the number of typos. Return to Task 12.5.1.4 InClass Task 4 dat &lt;- select(dmx, sub_id, typos, sound) results &lt;- ez::ezANOVA(dat, dv = typos, wid = sub_id, between = sound) ## Coefficient covariances computed by hccm() You didn’t need to store the output but we have done so out of habit, as often we want to do something with the results. And remember we said that you get two tables in this output. One of the tables is Levene’s Test for Homogeneity of Variance and it looks like: Table 12.3: Levene’s test for Homogeneity of Variance DFn DFd SSn SSd F p p&lt;.05 2 6 169.556 984.667 0.517 0.621 The other table is the main ANOVA output. Make sure you can locate both individual tables and you know which is which. Table 12.4: ANOVA output Effect DFn DFd F p p&lt;.05 ges sound 2 6 1.413 0.314 0.32 From your lectures, remember that Levene’s test relates to an assumption of equal variance across conditions in a between-subjects design. A non-significant finding in Levene’s suggests equal variance across conditions as required by the assumptions of the ANOVA. The ANOVA is the actual output of the test and will match closely with the values you have found in Task 3. Return to Task Chapter Complete! "],
["continuing-the-glm-two-factor-designs.html", "Lab 13 Continuing the GLM: Two-factor designs 13.1 Overview 13.2 PreClass Activity 13.3 InClass Activity 13.4 Assignment 13.5 Solutions to Questions", " Lab 13 Continuing the GLM: Two-factor designs 13.1 Overview For the past couple of weeks we have been building our understanding of the General Linear Model and in particular how it applies to a one-factor between-subjects ANOVA. Remember this is the scenario where you have one IV (categorical) and one DV (continuous) and you want to know if there is a significant effect at the different levels of your factor; where factor is another name for variable (or IV) and level is another name for condition (or group). We started out with the decomposition matrix, calculated our sums of squares, and from there our F-value to determine if there was a significant effect. One thing that is really worth keeping in mind is that the ANOVA is an omnibus test in that it tells you there is a significant effect of that factor, but it doesn’t specifically say in which way is that effect manifested; you always have to do a little work there to tease out the pattern of the effect. Say for instance you test a one-way ANOVA on three animal categories on some test (dogs, cats, gerbils). The ANOVA will tell you if there is an overall effect (or difference between groups) but you need to do a little work to find out is the difference between cats and dogs, dogs and gerbils, etc etc. But more on that another time. One-way ANOVAs are great when you only have one IV but the really useful thing about ANOVAs, and the GLM really, is that it can handle much more complex situations; which we are going to look at a little today. You were asked to read up on Chapter 4 of Miller and Haden (2013) looking at two-factor, between-subjects designs. This is the scenario where you have two factors (IVs) and it is different people in each condition. For example, say your IVs were people who can/can’t juggle, and people who do/don’t have pets. You have 4 groups here as you have people who can juggle and have pets, people who can’t juggle and have pets, people who can juggle and don’t have pets, and people who can’t juggle and who don’t have pets (how sad!!!). This would be an example of a two-way between-subjects factorial ANOVA (also a 2x2 ANOVA). And it is this scenario that we will be looking at today. The goals of this chapter are to: extend our knowledge of ANOVAs and GLMs to deal with two factors between-subject designs. understand the concepts of and calculate main effects and interactions. be able to plot and interpret data from factorial ANOVAs. 13.2 PreClass Activity Only one chapter to read this week. Some of the terms will be familiar but some novel so remember to take notes and think of examples which would use the same design but in a different scenario. Once you have read the chapter, try the suggested exercise from the chapter and then the MCQs below to see if you are following things correctly. Anything you are unsure of, post questions on the forum or ask them in the lab. 13.2.1 Read Chapter Read Chapter 4 of Miller and Haden (2013) and try to understand the situation where you have two factors with at least two levels each. In this lab we will look at interactions. 13.2.2 TRY Test your understanding of Miller and Haden (2013) Chapter 4 To test your understanding, work through Computational Exercise #1 in Section 4.9 of Miller and Haden - the answer is in Section 4.10 so check your working but be sure to work through the example first. The concept of interactions should be familiar to you from your statistics lectures this semester. Try these short MCQs on two factor, between-subjects designs: A 2x2 factorial design contains how many cells? two four six eight who do we appreciate! What effect is / effects are tested in a 2x2 ANOVA with factors A and B? the main effects of A and B, and the AB interaction only the main effects of A and B only the AB interaction a correlation between A and B What is a marginal mean? the mean DV at a given level of one factor, averaged over the levels of the other factors the mean DV at a given level of one factor, at a particular level of the other factor a mean that is nearly statistically significant a mean defined based on marginal likelihood What is a cell mean? the mean DV at a given level of one factor, averaged over the levels of the other factors the mean DV at a given level of one factor, at a particular level of the other factor a mean that is nearly statistically significant a mean defined based on marginal likelihood A statistical test for a main effect tests the null hypothesis that simple effects in the sample are equivalent simple effects in the population are equivalent sample marginal means are equivalent population marginal means are equivalent A statistical test for an interaction tests the null hypothesis that the effect of one factor is constant across the levels of the other in the population the effect of one factor is constant across the levels of the other in the sample sample marginal means are equivalent population marginal means are equivalent If you are not sure about the above questions, go back and read the chapter and make sure you understand the difference between a main effect (the effect at one of the IVs) and the interaction (the effect of one factor dependent on the levels of the other factor). Those are the key elements to really wrap your head around in a factorial ANOVA. Note: factorial ANOVAs can get really complex with three, four, or more IVs, so when writing about one, it is often good to state something like two-way (meaning two IVs) or three-way (meaning three IVs) etc. Be clear for your reader. Job Done - Activity Complete! 13.3 InClass Activity 13.3.1 Estimation equations and decomposition matrix We will start today by working with a decomposition matrix for a two-way between-subjects ANOVA and then finish by using the ez::ezANOVA() function to show you how you might practically carry out this analysis. Consider the data below from a 2x2 between-subjects design with 3 observations per cell. Keep in mind that each cell is a particular combination of levels of A and B, and each value in a cell, in this instance, is a unique participant. Table 13.1: Data for today’s example B1 B2 A1 74, 65, 77 70, 74, 66 A2 67, 67, 64 78, 78, 84 The decomposition matrix for these data is shown below; however, rather unfortunately for us, it is missing the columns AB_ij (\\(\\hat{A}_{ij}\\)) and err (\\(\\widehat{S(AB)}_{ijk}\\)) which we will need to calculate to complete our analysis. Here is a little recap of the columns (plus the two we will add): \\(i\\) - the first factor (here with two levels) \\(j\\) - the second factor (again here with two levels) \\(k\\) - the participant number within that \\(ij\\) combination \\(Y_{ijk}\\) - a participants score on a DV \\(\\mu\\) (mu) - the overall grand mean (or baseline effect) \\(A_i\\) - the effect of the first factor \\(i\\) \\(B_j\\) - the effect of the second factor \\(j\\) \\(\\hat{AB}_{ij}\\) - the effect of the AB interaction \\(\\widehat{S(AB)}_{ijk}\\) - the effect of within-group variability or error, err Table 13.2: Incomplete Decomposition Matrix i j k Y_ijk mu A_i B_j 1 1 1 74 72 -1 -3 1 1 2 65 72 -1 -3 1 1 3 77 72 -1 -3 1 2 1 70 72 -1 3 1 2 2 74 72 -1 3 1 2 3 66 72 -1 3 2 1 1 67 72 1 -3 2 1 2 67 72 1 -3 2 1 3 64 72 1 -3 2 2 1 78 72 1 3 2 2 2 78 72 1 3 2 2 3 84 72 1 3 The code to create the above matrix is in the solutions at the end of this chapter in case you want to create the matrix yourself as practice. If not, copy and paste the code from the solutions into a code chunk of an R Markdown file or into an R script (and make sure you also load tidyverse so that you can use the dplyr functions and pipes.) Run the code and look at decomp to confirm to yourself that it worked. Note the use of the group_by() function so that the values calculated with in mutate() are only calculated for each group. 13.3.2 Adding the missing columns Once you understand the table and code, try writing code to add the two missing columns to our matrix. Store the resulting table in decomp2. Here are some hints but again the code is in the solutions if you can’t quite get it - but remember, to paraphrase Dumbledore: “Help will always be given at Glasgow to those that look for it.” Hints: AB_ij - (\\(\\hat{A}_{ij}\\)) - is what is left of the mean value of all participants in that group once you have removed the effect of the grand mean, the effect of factor one, and the effect of factor two err - (\\(\\widehat{S(AB)}_{ijk}\\)) - is what is left from an individual’s score after removing the effect of the grand mean, the effect of factor A, the effect of factor B, and the interaction effect. 13.3.3 Understanding the two-factor decomposition matrix If you have performed the above steps correctly, then the decomp matrix should now look like Table 13.3: The complete decomposition matrix i j k Y_ijk mu A_i B_j AB_ij err 1 1 1 74 72 -1 -3 4 2 1 1 2 65 72 -1 -3 4 -7 1 1 3 77 72 -1 -3 4 5 1 2 1 70 72 -1 3 -4 0 1 2 2 74 72 -1 3 -4 4 1 2 3 66 72 -1 3 -4 -4 2 1 1 67 72 1 -3 -4 1 2 1 2 67 72 1 -3 -4 1 2 1 3 64 72 1 -3 -4 -2 2 2 1 78 72 1 3 4 -2 2 2 2 78 72 1 3 4 -2 2 2 3 84 72 1 3 4 4 So let’s now make sure we understand the table that we have and that we can pinpoint different elements of it by answering the following questions. The solutions are at the end of the chapter. From the options, what was the DV-value of participant \\(Y_{212}\\)? 64 65 66 67 Type in the value of \\(SS_{B}\\): Type in the value of \\(SS_{error}\\) is: Type in the value of \\(MS_{B}\\) is: Type in the value of \\(MS_{error}\\) (to one decimal places) is: The value of \\(F_{B}\\) (the F-ratio for the main effect of B) to 3 decimal places is: The numerator and denominator degrees of freedom associated with this \\(F\\) ratio are and respectively The \\(p\\) value associated with this F ratio to three decimal places is (HINT: ?pf): 13.3.4 Get your data ready for analysis It is excellent that you now understand a decomposition matrix and how it relates to an F-ratio. In reality however, rarely will you ever derive a decomposition matrix by hand; the point was to improve your understanding of the calculations behind an ANOVA. Let’s continue using the simulated data from above and run through the analysis steps we would normally follow. But first, let’s put it in a more useful format. The first thing we might want to do is to add columns that more clearly indicate the levels of our two factors. Right now the levels of A are represented by i and the levels of B are represented by j. But the data should really look more like this below: Table 13.4: Converted Decomposition Matrix id A B Y_ijk 1 A1 B1 74 2 A1 B1 65 3 A1 B1 77 4 A1 B2 70 5 A1 B2 74 6 A1 B2 66 7 A2 B1 67 8 A2 B1 67 9 A2 B1 64 10 A2 B2 78 11 A2 B2 78 12 A2 B2 84 We will need the id column (a unique value for each participant) for when we run ezANOVA() later. Again the code to convert decomp into this table (named dat) is in the solutions at the end of the chapter and you can use it if you like, but if you want to practice your skills first and convert the table yourself, that is also fine. Don’t spend too long on it though as it is more the output we want to look at. So if you are stuck, copy the code and run it in your session. We’ll be working with the new table dat for the remaining exercises. 13.3.5 Visualizing 2x2 designs: The interaction plot A critical part of data analysis is visualization. When dealing with factorial data, one of the most important visualizations is the interaction plot showing the cell means. You have already seen some of these in the Miller and Haden chapter and in the previous labs. Remember that before you can make an interaction plot, you need to calculate the cell means. First create a table called cell_means with the cell means stored in the column m. HINT - think group_by and summarise to leave three columns only, each with 4 rows. For example, one row will show A, A, mean-value Next, reproduce the plot below. Don’t look at the solution below until you’ve really tried! You will need two geoms: one to draw the points, one to draw the lines. And think about what is your x and y axes and how do you group the lines. Figure 13.1: Interaction plot 13.3.6 Running a 2x2 between-subjects ANOVA with ezANOVA Excellent! So we have a figure now! In reality, you would want to embellish this figure to make it look more professional, add some error bars, make sure the whole of the y-axis is shown, give proper names to the factors and levels, but it will do for now. You also want to look at the figure and think about what it is telling you. Do you think there will be: A main effect of Factor A? A main effect of Factor B? An interaction between Factors A and B? To some degree these are the three basic hypotheses laid out in any two-way ANOVA. To answer these you can think about: Are the means of \\(A_1\\) and \\(A_2\\) different, disregarding the effect of Factor B? Are the means of \\(B_1\\) and \\(B_2\\) different, disregarding the effect of Factor A? Are the means of \\(A_1\\) and \\(A_2\\) influenced by the effect of Factor B? What do non-parallel (or crossing lines) suggest about an interaction? Looking at the figure you might suggest, no main effect of A, no main effect of B, but that there is an interaction between A and B. Let’s test this using the ez:ezANOVA() function! Perform the 2x2 ANOVA on dat and store the output in result. Try not to look at the solution until you have tried the ?ezANOVA to see how to add more than one condition of the same design. a second hint is that both factors are “between”, so you want to focus on adding a second “between” condition We have also placed a short summary of the output of the ANOVA below to give you an idea of the outcome. Think about the outcome for a moment or two before having a go at writing one out and then look at the summary to compare. Summarising the output A two-way between-subjects factorial ANOVA was conducted. A significant interaction was found between Factor A and Factor B, F(1, 8) = 10.97, p = .011, ges = .59. Furthermore, a main effect of Factor B was found, F(1, 8) = 6.17, p = .038, ges = .44, which showed that the mean of \\(B_2\\) (M = 75) was significantly larger than the mean of \\(B_1\\) (M = 69). However, no main effect of Factor A was found, F(1, 8) = 0.686, p = .43, ges = .08. The mean of \\(A_1\\) (M = 71) was similar to the mean of \\(A_2\\) (M = 73). So it turns out we were sort of right and sort of wrong. There was no main effect of Factor A as we predicted. The means at \\(A_1\\) and \\(A_2\\) are very similar when you disregard the effect of Factor B. However, there actually was a main effect of Factor B; i.e. there was a significant difference between the means of \\(B_1\\) and \\(B_2\\) when you disregard the effect of Factor A. And finally, we predicted that there would be a significant interaction and there was one because the effect of Factor A is modulated by Factor B, and vice versa. One thing to point out here, when there are only two conditions in a factor and there is a significant main effect of that Factor (in this example Factor B) then to further qualify that effect you simply have to say which of the two conditions was bigger than the other! Group 2 bigger than Group 1 or Group 1 bigger than Group 2. When there is more than two conditions in a factor (e.g. three) or in the interaction, it is not that straightforward and you need to do further comparisons such as pairwise comparisons, t-test, simple main effects, or TUKEY HSDs, to tease those effects a part. We will cover more of that in the lecture series. 13.3.7 App: Understanding main effects and interactions If time permits (or on your own time), check out the accompanying shiny app on main effects and interactions. This allows you to move sliders and change the sizes of main effects / interactions and see how this affects cell means and effect decompositions. This will help sharpen your intuitions about these concepts. Download the app from Moodle or from this link. Job Done - Activity Complete! You should now be ready to complete the Homework Assignment for this lab. The assignment for this Lab is FORMATIVE and is NOT to be submitted and will NOT count towards the overall grade for this module. However you are strongly encouraged to do the assignment as it will continue to boost your skills which you will need in future assignments. If you have any questions, please post them on the slack forum under the channel #level2_2019 13.4 Assignment This is a summative assignment. Instructions as to how to access and submit your assignment will be made available during the course. Lab 13: Two-Factor ANOVA: Perspective-Taking in Language Comprehension In order to complete this assignment you first have to download the assignment .Rmd file which you need to edit for this assignment: titled GUID_Level2_Semester2_Lab4.Rmd. This can be downloaded within a zip file from the below link. Once downloaded and unzipped you should create a new folder that you will use as your working directory; put the .Rmd file in that folder and set your working directory to that folder through the drop-down menus at the top. Download the Assignment .zip file from here or on Moodle. Background: Perspective-Taking in Language Comprehension For this assignment, you will be looking at real data from Experiment 2 of Keysar, Lin, and Barr (2003), “Limits on Theory of Mind Use in Adults”, Cognition, 89, 29–41. This study used eye-tracking to investigate people’s ability to take another’s perspective during a kind of communication game. (The data that you will be analysing, while real, did not appear in the original report.) The communication game that participants played was as follows. Each participant sat at a table opposite a confederate participant (a stooge from the lab who pretended to be a naive participant). Between the two participants was an upright set of shelves (see figure below). The participants played a game in which the real participant was assigned the role of the “matcher” and the confederate the role of the “director”. The director was given a picture with a goal state for the grid, showing how the objects needed to be arranged. However, the director was not allowed to touch the objects. To get the objects in their proper places, the director needed to give instructions to the matcher to move the objects. For example, the director might say, “take the red box in the top corner and move it down to the very bottom row,” and the matcher would then perform the action. The matcher’s eye movements were tracked as they listened to and interpreted the director’s instructions. Figure 13.2: Director-Matcher Viewpoints from Keysar, Lin, and Barr (2003) To investigate perspective taking, the instructions given by the director were actually scripted beforehand in order to create certain ambiguities. Most of the objects in the grid, such as the red box, were mutually visible to both participants (i.e., visible from both sides of the grid). However, some of objects, like the brush and the green candle, were occluded from the director’s view; the matcher could see them, but had no reason to believe that the director knew the contents of these occluded squares, and thus had no reason to expect her to ever refer to them. However, sometimes the director would refer to a mutually visible object using a description that also happened to match one of the hidden objects. For instance, the director might instruct the matcher to “pick up the small candle.” Note that for the director, the small candle is the purple candle. A given matcher would see this grid in one of two conditions: In the Experimental condition, the matcher saw an additional green candle in a hidden box that was even smaller than the purple candle (see middle panel of the above figure). This object was called a “competitor” because it matched the description of the intended referent (the purple candle). In the Baseline condition, the green candle was replaced with an object that did not match the director’s description, such as an apple. These ambiguous situations provided the main data for the experiment, and in total there were eight different grids in the experiment that presented analogous situations to the example above. A previous eye-tracking study by the same authors had found the presence of the competitors severely confused the matchers, suggesting that people were surprisingly egocentric—they found it hard to ignore “privileged” information when interpreting another person’s speech. For example, when the director said to “pick up the small candle,” they spent far more time looking at a hidden green candle than a hidden apple, even though neither one of these objects, being hidden, was a viable referent. We refer to the difference in looking time as the ‘egocentric interference effect’. Experiment 2 by Keysar, Lin, and Barr aimed to follow up on this finding. In the previous article, the matcher had reason to believe that the director was merely ignorant of the identity of the hidden objects. But what would happen if the matcher was given reason to believe that the director actually had a false belief about the hidden object? For example, would the matcher experience less egocentric interference if he or she had reason to think that the director thought that the hidden candle was a toy truck? To test this, half of the participants were randomly assigned to a false belief condition, where the matcher was led to believe that the director had a false belief about the identity of the hidden object; the other half participated in the ignorance condition, where as in previous experiments, they were led to believe that the director simply did not know what was in the hidden squares. There were 40 participants in this study, 20 in the false belief condition, and 20 in the ignorance condition. There were also an equal number of male and female participants in the study. To spoil the plot a bit, Keysar, Lin and Barr did not find any effect of condition on looking time. However, they did not consider sex as a potential moderating variable. Thus, we will explore the effects of ignorance vs. false belief on egocentric interference, broken down by the sex of the matcher. Before starting lets check: The .csv file is saved into a folder on your computer and you have manually set this folder as your working directory. The .Rmd file is saved in the same folder as the .csv files. For assessments we ask that you save it with the format GUID_Level2_Semester2_Lab4.Rmd where GUID is replaced with your GUID. Though this is a formative assessment, it may be good practice to do the same here. 13.4.1 Task 1A: Libraries In today’s assignment you will need both the tidyverse and ez packages. Enter code into the t1A code chunk below to load in both of these libraries. # load in the packages 13.4.2 Task 1B: Loading in the data Use read_csv() to replace the NULL in the t1B code chunk below to load in the data stored in the datafile keysar_lin_barr_2003.csv. Store the data in the variable dat. Do not change the filename of the datafile. dat &lt;- NULL Take a look at your data (dat) in the console using glimpse() or View(), or just display it by typing in the name. You will see the following columns: variable description subject unique identifier for each subject sex whether the subject was male or female condition what condition the subject was in looktime egocentric interference We have simplified things from the original experiment by collapsing the baseline vs. experimental conditions into a single DV. Our DV, egocentric interference, is the average difference in looking time for each subject (in milliseconds per trial) for hidden competitors (e.g., small candle) versus hidden non-competitors (e.g., apple). The larger this number, the more egocentric interference the subject experienced. 13.4.3 Task 2: Calculate cell means Today we are going to focus on just the main analysis and write-up, and not the assumptions, but as always before running any analysis you should check that your assumptions hold. One of the elements we will need for our write-up is some descriptives. We want to start by creating some summary statistics for the four conditions. Remember, two factors (sex and condition) with 2 levels each (sex: female vs. male; condition: false belief vs. ignorance) will give you four conditions, and as such in our summary table, four cells created by factorially combining sex and condition. Replace the NULL in the t2 code chunk below to create the four cells created by factorially combining sex and condition, calculating the mean and standard deviation for each cell. Store the descriptives in the tibble called cell_means Call the column for the mean m and the column for the standard deviation sd. Your table should have four rows and four columns as shown below but with your values replacing the XXs Follow the case and spelling exactly. cell_means &lt;- NULL sex condition m sd female false belief XX XX female ignorance XX XX male false belief XX XX male ignorance XX XX 13.4.4 Task 3: Marginal means for sex We will also need to have some descriptives where we just look at the means of a given factor; the marginal means - the means of the levels of one factor regardless of the other factor. Replace the NULL in the t3 code chunk below to calculate the marginal means and standard deviations for the factor sex. Store these descriptives in the tibble marg_sex Call the column for the mean m and the column for the standard deviation sd. Your table should have two rows and three columns as shown below but with your values replacing the XXs Follow the case and spelling exactly. marg_sex &lt;- NULL sex m sd female XX XX male XX XX 13.4.5 Task 4: Marginal means for condition And now do the same for condition. Replace the NULL in the t4 code chunk below to calculate the marginal means and standard deviations for the factor, condition Store these descriptives in the tibble marg_cond Call the column for the mean m and the column for the standard deviation sd. Your table should have two rows and three columns as shown below but with your values replacing the XXs Follow the case and spelling exactly. marg_cond &lt;- NULL condition m sd false belief XX XX ignorance XX XX 13.4.6 Task 5: Interaction plot And finally we are going to need a plot. When you have two factors, you want to show both factors on the plot to give the reader as much information as possible and save on figure space. The best way to do this is through some sort of interaction plot as shown in the lab. It is really a lot easier than it looks and it only requires you to think about setting the aes by the different conditions. Insert code into the t5 code chunk below to replicate the figure shown to you. Pay particular attention to labels, axes dimensions, color and background. Note that the figure must appear when your code is knitted. Note: The figure below is a nice figure but should really have error bars on it if I was including it in an actual paper. Including the error bars may help in clarifying the descriptive statistics and you will see that here, although the means are different, there is huge overlap in terms of error bars which may indicate no overall effect. # to do: something with ggplot to replicate the figure Figure 13.3: Replicate this Figure 13.4.7 Task 6: Recap Question 1 Thinking about the above information, one of the below statements would be an acceptable hypothesis for the interaction effect of sex and condition, but which one: In the t6 code chunk below, replace the NULL with the number of the statement below that best summarises this analysis. Store this single value in answer_t6 We hypothesised that there will be a significant difference between males and females in egocentric interference (mean looking time (msecs)) regardless of condition. We hypothesised that there will be a significant difference between participants in the false belief condition and those in the ignorance condition in terms of egocentric interference (mean looking time (msecs)) regardless of sex of participant. We hypothesised that there would be a significant interaction between condition and sex of participant on egocentric interference (mean looking time (msecs)) We hypothesised that there will be no significant difference between males and females in egocentric interference (mean looking time (msecs)) regardless of condition but that there would be a significant difference between participants in the false belief condition and those in the ignorance condition in terms of egocentric interference (mean looking time (msecs)) regardless of sex of participant. answer_t6 &lt;- NULL 13.4.8 Task 7: Recap Question 2 Thinking about the above information, one of the below statements is a good description of the marginal means for sex, but which one: In the t7 code chunk below, replace the NULL with the number of the statement below that best summarises this analysis. Store this single value in answer_t7 The female participants have an average longer looking time (M = 777.98, SD = 911.53) than the male participants (M = 555.04, SD = 707.81) which may suggest a significant main effect of sex. The female participants have an average shorter looking time (M = 777.98, SD = 911.53) than the male participants (M = 555.04, SD = 707.81) which may suggest a significant main effect of condition. The female participants have an average shorter looking time (M = 777.98, SD = 911.53) than the male participants (M = 555.04, SD = 707.81) which may suggest a significant main effect of sex. The female participants have an average longer looking time (M = 777.98, SD = 911.53) than the male participants (M = 555.04, SD = 707.81) which may suggest a significant main effect of condition. answer_t7 &lt;- NULL 13.4.9 Task 8: Recap Question 3 Thinking about the above information, one of the below statements is a good description of the marginal means for condition, but which one: In the t8 code chunk below, replace the NULL with the number of the statement below that best summarises this analysis. Store this single value in answer_t8 The participants in the false belief group had an average longer looking time (M = 549.58, SD = 775.91) than the participants in the ignorance group (M = 749.58, SD = 861.23), which may suggest a significant main effect of condition. The participants in the false belief group had an average shorter looking time (M = 549.58, SD = 775.91) than the participants in the ignorance group (M = 749.58, SD = 861.23), which may suggest a significant main effect of condition. The participants in the false belief group had an average longer looking time (M = 549.58, SD = 775.91) than the participants in the ignorance group (M = 749.58, SD = 861.23), which may suggest a significant main effect of sex. The participants in the false belief group had an average shorter looking time (M = 549.58, SD = 775.91) than the participants in the ignorance group (M = 749.58, SD = 861.23), which may suggest a significant main effect of sex. answer_t8 &lt;- NULL 13.4.10 Task 9: Running the factorial ANOVA Great, so we have looked at our descriptives and thought about what effects there might be. What we need to do now is run the ANOVA using the ezANOVA() function. The ANOVA we are going to run is a two-way between-subjects ANOVA because both conditions are between-subjects variables. You may need to refer back to the lab or to have a look at the help on ezANOVA() to see how to add a second variable/factor. Replace the NULL in the t9 code chunk below to run this two-way between-subjects ANOVA. Look at the inclass for guidance. You need the data, the dv, the two between condition, and the participant id. Do not tidy() the output. Do nothing to the output other than store it in the variable named mod (note: technically it will store as a list). You will see in red in the output that the code will convert the condition and participant ids to factors automatically. This is fine. mod &lt;- NULL 13.4.11 Task 10: Interpreting the ANOVA output Question Thinking about the above information, one of the below statements is a good summary of the outcome ANOVA, but which one: In the t10 code chunk below, replace the NULL with the number of the statement below that best summarises this analysis. Store this single value in answer_t10 There is a significant main effect of sex, but no main effect of condition and no interaction between condition and sex. There is a significant main effect of condition, but no main effect of sex and no interaction between condition and sex. There is no significant main effect of sex or condition and there is no significant interaction between condition and sex. There is a significant main effect of sex, a significant main effect of condition, and a significant interaction between condition and sex. answer_t10 &lt;- NULL 13.4.12 Task 11: Report your results Write a paragraph reporting your findings. NOTE: You can use inline code to report the \\(F\\) values, but note that you cannot use broom::tidy() for objects created by ezANOVA(). Here is a hint: mod$ANOVA gives you a table of your results (ezANOVA() returns a list with two elements; mod$ANOVA returns the element of the list called ANOVA that has the results). You can pull() and pluck() whatever you need from this table. Write your summary between the two green lines shown in the Assignment file. Job Done - Activity Complete! Well done, you are finished! Now you should go check your answers against the solution file which can be found on Moodle. You are looking to check that the resulting output from the answers that you have submitted are exactly the same as the output in the solution - for example, remember that a single value is not the same as a coded answer. Where there are alternative answers it means that you could have submitted any one of the options as they should all return the same answer. If you have any questions please post them on the moodle forum or on the rguppies.slack.com forum #level2_2019. See you in the next lab! 13.5 Solutions to Questions Below you will find the solutions to the questions for the Activities for this chapter. Only look at them after giving the questions a good try and speaking to the tutor about any issues. 13.5.1 InClass Activities 13.5.1.1 Estimation equations and decomposition matrix Creating the Decomposition Matrix decomp &lt;- tibble(i = rep(1:2, each = 6), j = rep(rep(1:2, each = 3), times = 2), k = rep(1:3, times = 4), Y_ijk = c(74, 65, 77, 70, 74, 66, 67, 67, 64, 78, 78, 84)) %&gt;% mutate(mu = mean(Y_ijk)) %&gt;% # calculate mu group_by(i) %&gt;% mutate(A_i = mean(Y_ijk) - mu) %&gt;% # calculate A_i group_by(j) %&gt;% mutate(B_j = mean(Y_ijk) - mu) %&gt;% # calculate B_j ungroup() Return to Task 13.5.1.2 Adding the missing columns decomp2 &lt;- decomp %&gt;% group_by(i, j) %&gt;% mutate(AB_ij = mean(Y_ijk) - mu - A_i - B_j) %&gt;% ungroup() %&gt;% mutate(err = Y_ijk - mu - A_i - B_j - AB_ij) Return to Task 13.5.1.3 Understanding the two-factor decomposition matrix Q1 The DV value of participant \\(Y_{212}\\) or the 2nd Participant in \\(I_2\\), \\(J_1\\), is 67 Q2 The Sums of Squares of Factor B is 108 Q3 The Sums of Squares of the Error is 140 Q4 The \\(MS_{B}\\) is 108 Q5 The \\(MS_{error}\\) (to one decimal places) is 17.5 Q6 The F-ratio for the main effect of B to 3 decimal places is 6.171 Q7 The numerator and denominator degrees of freedom associated with this \\(F\\) ratio are 1 and 8 respectively Q8 And based on pf(fratio, 1,8, lower.tail = FALSE) the \\(p\\)-value associated with this F ratio to three decimal places is 0.038 or .038 Return to Task 13.5.1.4 Get your data ready for analysis dat &lt;- decomp %&gt;% mutate(A = paste0(&quot;A&quot;, i), B = paste0(&quot;B&quot;, j), id = row_number()) %&gt;% select(id, A, B, Y_ijk) Return to Task 13.5.1.5 Visualizing 2x2 designs: The interaction plot Creating the Cell means cell_means &lt;- dat %&gt;% group_by(A, B) %&gt;% summarise(m = mean(Y_ijk)) Reproducing the Plot ggplot(cell_means, aes(A, m, group = B, shape = B, color = B)) + geom_point(size = 3) + geom_line() Figure 13.4: The plot that the code gives Easter Egg Figure Solution The plot above is functional but sometimes you want something a bit more communicative. It is worth working on your figures so, here is an example of what you can think about for your report. Remember to look back through previous labs and homework as well (Semester 1: Lab 3, Lab 7, Lab 6, Lab 5 &amp; 9 assignments, for instance) to see how figures can be improved. The code below adds another few dimensions to the above figure. Copy and run the code in your Rmd, knitting it to HTML, and play with the different parts to see what they do. We have changed the legends to be more descriptive and to have more readable text, fixed the scale for the vertical axis, made the figure black and white without a box, we also added 95% confidence intervals and a figure caption. There are of course various ways to do these changes, in particular the caption, but this is an option. Note: This will only run if you have the tibble dat from earlier in this worksheet cell_means1 &lt;- dat %&gt;% group_by(A, B) %&gt;% summarise(m = mean(Y_ijk), n = n(), sd_scores = sd(Y_ijk), ste_scores = sd_scores/sqrt(n), ci = 1.96 * ste_scores) ggplot(cell_means1, aes(A, m, group = B)) + geom_point(aes(shape = B), size = 3) + geom_line() + geom_errorbar(aes(ymin = m - ci, ymax = m + ci), width = 0.05, size = .5) + coord_cartesian(ylim = c(0,100)) + labs(x = &quot;Groups in Factor A&quot;, y = &quot;Mean Scores&quot;, caption = &quot;Figure 1. Mean scores from the example data for the two-way \\nbetween-subjects design ANOVA. Error bars indicate 95% \\nconfidence intervals.&quot;) + scale_shape_discrete(&quot;Groups in Factor B&quot;) + theme_classic() + theme(axis.text.x = element_text(size = 12), axis.text.y = element_text(size = 12), axis.title = element_text(size = 14), legend.title = element_text(size = 14), legend.text = element_text(size = 14), plot.caption = element_text(size = 14, hjust = 0)) Figure 13.5: A nice figure example Return to Task 13.5.1.6 ANOVA Using Ez The code result &lt;- ezANOVA(dat, dv = Y_ijk, wid = id, between = .(A, B), detailed = TRUE) %&gt;% print() ## Coefficient covariances computed by hccm() ## $ANOVA ## Effect DFn DFd SSn SSd F p p&lt;.05 ges ## 1 A 1 8 12 140 0.6857143 0.43163404 0.07894737 ## 2 B 1 8 108 140 6.1714286 0.03786084 * 0.43548387 ## 3 A:B 1 8 192 140 10.9714286 0.01066139 * 0.57831325 ## ## $`Levene&#39;s Test for Homogeneity of Variance` ## DFn DFd SSn SSd F p p&lt;.05 ## 1 3 8 14.25 82.66667 0.4596774 0.718037 Including the detailed = TRUE will give us the sums of squares information The output Table 13.5: The ANOVA output Effect DFn DFd SSn SSd F p p&lt;.05 ges A 1 8 12 140 0.686 0.432 0.079 B 1 8 108 140 6.171 0.038 0.435 A:B 1 8 192 140 10.971 0.011 0.578 Return to Task 13.5.2 Homework Activities 13.5.2.1 Task 1A: Libraries In today’s assignment you will need both the tidyverse and ez packages. library(ez) library(tidyverse) Return to Task 13.5.2.2 Task 1B: Loading in the data Remember to use read_csv() to load in the data. dat &lt;- read_csv(&quot;keysar_lin_barr_2003.csv&quot;) Return to Task 13.5.2.3 Task 2: Calculate cell means for the cell means. The code below will give the table shown. cell_means &lt;- dat %&gt;% group_by(sex, condition) %&gt;% summarise(m = mean(looktime), sd = sd(looktime)) sex condition m sd female false belief 594.5833 899.1660 female ignorance 944.6970 932.6990 male false belief 504.5833 676.7338 male ignorance 611.1111 778.0212 Return to Task 13.5.2.4 Task 3: Marginal means for sex The code below will give the table shown for the marginal means of sex. marg_sex &lt;- dat %&gt;% group_by(sex) %&gt;% summarise(m = mean(looktime), sd = sd(looktime)) sex m sd female 777.9762 911.5331 male 555.0439 707.8138 Return to Task 13.5.2.5 Task 4: Marginal means for condition The code below will give the table shown for the marginal means of condition. marg_cond &lt;- dat %&gt;% group_by(condition) %&gt;% summarise(m = mean(looktime), sd = sd(looktime)) condition m sd false belief 549.5833 775.9108 ignorance 794.5833 861.2306 Return to Task 13.5.2.6 Task 5: Interaction plot The code below will produce the shown figure. ggplot(cell_means, aes(condition, m, shape = sex, group = sex, color = sex)) + geom_line() + geom_point(size = 3) + labs(y = &quot;mean looking time (msecs)&quot;) + scale_y_continuous(limits = c(0, 1000)) + theme_bw() Figure 13.6: You should have produced a similar figure Return to Task 13.5.2.7 Task 6: Recap Question 1 We want the alternative, not the null hypothesis here. So, an acceptable hypothesis for the interaction effect of sex and condition would be: We hypothesised that there would be a significant interaction between condition and sex of participant on egocentric interference (mean looking time (msecs)). As such the correct answer is: answer_t6 &lt;- 3 Return to Task 13.5.2.8 Task 7: Recap Question 2 A good description of the marginal means for sex would be: The female participants have an average longer looking time (M = 777.98, SD = 911.53) than the male participants (M = 555.04, SD = 707.81) which may suggest a significant main effect of sex. As such the correct answer is: answer_t7 &lt;- 1 Return to Task 13.5.2.9 Task 8: Recap Question 3 A good description of the marginal means for condition would be: The participants in the false belief group had an average shorter looking time (M = 549.58, SD = 775.91) than the participants in the ignorance group (M = 749.58, SD = 861.23), which may suggest a significant main effect of condition. As such the correct answer is: answer_t8 &lt;- 2 Return to Task 13.5.2.10 Task 9: Running the factorial ANOVA The code below will produce the shown ANOVA output mod &lt;- ezANOVA(dat, dv = looktime, wid = subject, between = .(condition, sex)) knitr::kable(mod$ANOVA) Effect DFn DFd F p p&lt;.05 ges condition 1 36 0.7913759 0.3795907 0.0215098 sex 1 36 0.6405277 0.4287699 0.0174814 condition:sex 1 36 0.2130405 0.6471716 0.0058830 Return to Task 13.5.2.11 Task 10: Interpreting the ANOVA output Question A good summary of the outcome ANOVA would be: There is no significant main effect of sex or condition and there is no significant interaction between condition and sex. As such the correct answer is: answer_t10 &lt;- 3 Return to Task 13.5.2.12 Task 11: Report your results There is no definitive way to write this paragraph but essentially your findings should report both main effects and the interaction, giving appropriate F outputs, e.g. F(1, 36) = .79, p = .38, and give some interpretation/qualification of the results using the means and standard deviations above, e.g. looking time was not significantly different between the false belief task (M = X, SD = XX) or the Ignorance task (M = XX, SD = XX). Something along the following would be appropriate: A two-way between-subjects factorial ANOVA was conducted testing the main effects and interaction between sex (male vs. female) and condition (false belief vs. ignorance) on the average looking time (msecs) on a matching task. Results revealed no significant interaction (F(1, 36) = .21, p = .647) suggesting that there is no modulation of condition by sex of participant in this looking task. Furthermore, there was no significant main effect of sex (F(1, 36) = .64, p = .429) suggesting that male (M = 555.04, SD = 707.81) and female participants (M = 777.98, SD = 911.53) perform similarly in this task. Finally, there was no significant main effect of condition (F(1, 36) = .79, p = .38) suggesting that whether participants were given a false belief scenario (M = 594.58, SD = 775.91) or an ignorance scenario (M = 794.58, SD = 861.23) had no overall impact on their performance. Return to Task Chapter Complete! "],
["regression.html", "Lab 14 Regression 14.1 Overview 14.2 PreClass Activity 14.3 InClass Activity 14.4 Assignment 14.5 Solutions to Questions", " Lab 14 Regression 14.1 Overview For the past few weeks we have been looking at designs where you have categorical factors/IVs and where you want to see whether there is an effect of a given factor or an interaction between two factors on a continuous DV. And we have looked at this through decomposition matrices and through the ezANOVA package. We also briefly mentioned how this approach can be extrapolated into designs with more than two-factors such as three-way ANOVAs (three factors) and larger, but also into within-subject designs where every participant sees every stimuli, and mixed-designs where you have at least one between and one within factor. We will look in-depth at these different types of designs next year. Today, however, we want to start looking at predicting relationships from continuous variables through regression. You will already be familiar with many of the terms here from your lecture series.In addition, by looking at a practical example (relating to voice research) in the lab, and by reading about regression in Miller and Haden (2013) in the PreClass, it should start to become more concrete for you. Regression is still part of the GLM and eventually the goal will be to show you how to analyse designs that has both categorical and continuous variables as much of real data is made up like that. But for now we will just look at simple and multitple linear regression to make you more comfortable with carrying out and interpreting these analyses. The goals of this chapter are to: Introduce the concepts that underpin linear regression. Demonstrate and practice carrying out and interpreting regression analysis with one or more predictor variables. Demonstrate and practice being able to make predictions based on your regression model. 14.2 PreClass Activity As in previous chapters, the PreClass activity is to read the following chapter from Miller and Haden (2013). You may also want to try reading Chapter 14 as well on Multiple Regression but really more to add to your understanding of the general goal, as opposed to the underlying computations. Finally, reviewing your lecture on Simple Linear and Multiple Linear Regression will really help your understanding of this lab. 14.2.1 Read Chapter Read Chapter 12 of Miller and Haden (2013) and try to understand the how the GLM applies to regression. The concept of regression will be familiar to you based on the stats lectures of this semester so some of the terms will just be recapping. Some will be an expansion of your understanding and basing the analysis in terms of the GLM. Optional Read Chapter 14 of Miller and Haden (2013). This covers Multiple Linear Regression. Again the concepts will be familiar to you from the lecture series and reading this chapter would be to enhance your overall understanding, not necessarily the underlying computations. Job Done - Activity Complete! 14.3 InClass Activity You have been reading about regression in Miller and Haden (2013) and have been looking at it in the lectures. Today, to help get a practical understanding of regression, you will be working with real data and using regression to explore the question of whether there is a relationship between voice acoustics and ratings of perceived trustworthiness. The Voice The prominent theory of voice production is the source-filter theory (Fant, 1960) which suggests that vocalisation is a two step process: air is pushed through the larynx (vocal chords) creating a vibration, i.e. the source, and this is then shaped and moulded into words and utterances as it passes through the neck, mouth and nose, and depending on the shape of those structures at any given time you produce different sounds, i.e. the filter. One common measure of the source is pitch (otherwise called Fundamental Frequency or F0 (F-zero)) (Titze, 1994), which is a measure of the vibration of the vocal chords, in Hertz (Hz); males have on average a lower pitch than females for example. Likewise, one measure of the filter is called formant dispersion (measured again in Hz), and is effectively a measure of the length of someone’s vocal tract (or neck). Height and neck length are suggested to be negatively correlated with formant dispersion, so tall people tend to have smaller formant dispersion. So all in, the sound of your voice is thought to give some indication of what you look like. More recently, work has focussed on what the sound of your voice suggests about your personality. McAleer, Todorov and Belin (2014) suggested that vocal acoustics give a perception of your trustworthiness and dominance to others, regardless of whether or not it is accurate. One extension of this is that trust may be driven by malleable aspects of your voice (e.g. your pitch) but not so much by static aspects of your voice (e.g. your formant dispersion). Pitch is considered malleable because you can control the air being pushed through your vocal chords (though you have no conscious control of your vocal chords), whereas dispersion may be controlled by the structure of your throat which is much more rigid due to muscle, bone, and other things that keep your head attached. This idea of certain traits being driven by malleable features and others by static features was previously suggested by Oosterhof and Todorov (2008) and has been tested with some validation by Rezlescu, Penton, Walsh, Tsujimura, Scott and Banissy (2015). So the research question today is: can vocal acoustics, namely pitch and formant dispersion, predict perceived trustworthiness from a person’s voice? We will only look at male voices today, but you have the data for female voices as well should you wish to practice (note that in the field, tendency is to analyse male and female voices separately as they are effectively sexually dimorphic). As such, we hypothesise that a linear combination of pitch and dispersion will predict perceived vocal trustworthiness in male voices. This is what we will analyse. Let’s begin. First, to run this analysis you will need to download the data from Moodle or from here. You will see in this folder that there are two datafiles: voice_acoustics.csv - shows the VoiceID, the sex of voice, and the pitch and dispersion values voice_ratings.csv - shows the VoiceID and the ratings of each voice by 28 participants on a scale of 1 to 9 where 9 was extremely trustworthy and 1 was extremely untrustworthy Have a look at the layout of the data and familiarise yourself with it. The ratings data is rather messy and in a different layout to the acoustics but can you tell what is what? Looking at the layout of the acoustics data it appears to be in long wide tidy Looking at the layout of the ratings data it appears to be in long wide We are going to need to do some data-wrangling before we do any analysis!!! 14.3.1 Task 1: Setup Open a new script or Rmd (depending on how you like to work) and load in the tidyverse, broom, and the two CSV datasets into tibbles called ratings and acoustics. Probably best if the ratings are in ratings and the acoustics in acoustics. 14.3.2 Task 2: Restructuring the ratings Next we need to calculate a mean rating score for each voice. We are analysing the voices and not specifically what each participant rated each voice as (that is for another year) so we need to average across all participants, their ratings for each individual voice and get a mean rating for each voice. You will see in your data that the voices are identified in the VoiceID column. Recall the difference between wide and long data. In wide data, each row represents an individual case, with observations for that case in separate columns; in long data, each row represents a single observation, and the observations are grouped together into cases based on the value of a variable (for these data, the VoiceID variable). Before we calculate means, what you need to do is to restructure the ratings data into the appropriate “tidy” format; i.e., so that it looks like the table below. Table 14.1: Gather the data into Tidy format VoiceID participant rating 1 P1 7.0 2 P1 8.0 3 P1 7.5 4 P1 6.0 5 P1 4.0 6 P1 5.0 Write code to restructure the ratings data as above and store the resulting table as ratings_tidy. Only the first six rows are shown. In the table above you see the first 6 voices all rated by Participant 1. Hint for Task 2 gather(data, new_column_name, new_column_name, first-column:last-column) 14.3.3 Task 3: Calculate mean trustworthiness rating for each voice Now that you’ve gotten your ratings data into a more tidy format, the next step is to calculate the mean rating (mean_rating) for each voice. Remember that each voice is identified by the VoiceID variable. Store the resulting table in a variable named ratings_mean. Hint for Task 3 a group_by and summarise would do the trick remember if there are any NAs then na.rm = TRUE would help 14.3.4 Task 4: Joining the Data together Great! We are so hot at wrangling now we are like hot wrangling irons! Ok but, before we get ahead of ourselves, in order to perform the regression analysis we need to combine the data from ratings_mean (the mean ratings) with acoustics (the pitch and dispersion ratings). Also, as we said, we only want to analyse Male voices today. Go ahead and join the two tibbles and filter out the Female voices to keep the Male voices only. Call the resulting tibble joined. The first few rows should look like this: Table 14.2: Only the Male Voices VoiceID mean_rating sex measures value 1 4.803571 M Pitch 118.6140 1 4.803571 M Dispersion 1061.1148 2 6.517857 M Pitch 215.2936 2 6.517857 M Dispersion 1023.9048 3 5.910714 M Pitch 147.9080 3 5.910714 M Dispersion 1043.0630 Hint for Task 4 inner_join by the common column in both datasets filter to keep just Male voices 14.3.5 Task 5: Scatterplot As always, where possible, it is a good idea to visualise your data. Now that we have all of the variables in one place, reproduce the scatterplot shown below and then try to answer the following questions. According to the scatterplot, there appears to be a negative relationship positive relationship between both pitch and trustworthiness and dispersion and trustworthiness though the relationship with dispersion pitch seems stronger. Figure 14.1: Scatterplot showing the relationship between the voice measures of Dispersion (left) and Pitch (right) and Mean Trustworthiness Rating Hint for Task 5 ggplot() geom_smooth(method = “lm”) coord_cartesian or scale_y_continuous facet_wrap(scales = “free”) did you know also that you can control the number of columns and rows in a facet_wrap by adding nrow and ncol? 14.3.6 Task 6: Spreading the data Ok so we are starting to get an understanding of our data and we want to start thinking about the regression. However, the regression would be easier to work with if Pitch and Dispersion were in separate columns. Create a new tibble where Pitch and Dispersion data have been split into two columns called Pitch and Dispersion respectively. Hint for Task 6 spread() needs the data, name of the categorical column to spread, and the name of the data to spread 14.3.7 Task 7: The Regressions Excellent, we are now in a position to regress away. Oh hang on, we should probably check the correlation between Pitch and Dispersion as remember the issue of collinearity. Calculate the correlation between Pitch and Dispersion and fill in this statement. The correlation between Pitch and Dispersion is rho = .239 rho = -.239 rho = .186 rho = -.186 which would suggest that we have no issues with collinearity as our two predictors are only slightly correlated and the correlation here is not significant. Ok, let’s do some regression. The lm() function in R is the main function we will use to estimate a Linear Model (hence the function name lm). Use the lm() function to run the following three regression models. Simple Linear Regression Run the simple linear regression of predicting trustworthiness mean ratings from Pitch, and store the model in mod_pitch Run the simple linear regression of predicting trustworthiness mean ratings from Dispersion, and store the model in mod_disp Multitple Linear Regression Run the multiple linear regression of predicting trustworthiness mean ratings from Pitch and Dispersion, and store the model in mod_pitchdisp Now look at the results of each one in turn, and try to interpret them, using the function summary(), e.g. summary(mod_pitch). Hint for Task 7 Correlations You should probably use a Spearman correlation for the correlation between Pitch and Dispersion because the scales are very different although measured both in (Hz) You may need to refer back to Chapter 10 on correlations to remember how to use cor.test(). Regressions lm(dv ~ iv, data = my_data) for simple linear regression lm(dv ~ iv1 + iv2, data = my_data) for multiple linear regression 14.3.8 Task 8: Making interpretations Let’s look at the mod_pitch model together. summary(mod_pitch) ## ## Call: ## lm(formula = mean_rating ~ Pitch, data = joined_wide) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.52562 -0.30181 0.04361 0.33398 1.20492 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.921932 0.583801 5.005 2.3e-05 *** ## Pitch 0.015607 0.004052 3.852 0.000573 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6279 on 30 degrees of freedom ## Multiple R-squared: 0.3309, Adjusted R-squared: 0.3086 ## F-statistic: 14.83 on 1 and 30 DF, p-value: 0.0005732 From the output we can see that this model, when relating to the population, would predict approximately 30.8% of the variance in trustworthiness ratings (Adjusted-R^2 = .3086). We could also say that a linear regression model revealed that pitch significantly predicted perceived trustworthiness scores in male voices in that as pitch increased so does perceived trustworthiness (b = .0156, t(30) = 3.852, p &lt; .001). (Remember that these are unstandardised coefficients so the “Estimate” would mean that a one unit change in pitch would result in a .0156 unit change in perceived trust, a rather small change.) Overall we have a model of small to medium prediction but it is better than no model - or just using mean values as a prediction - as shown by the F-test being significant. Worth also pointing out here that in a simple linear regression the F-test and the t-value for the predictor are the same based on t^2 = F, as seen in Semester 2 Lab 3. Ok, based on that knowledge, answer the following questions about the two remaining models. The dispersion as a predictor by itself model would explain approximately 3% 13% 31% 33% In fact, the dispersion by itself model is not significant significant and therefor no use very useful as a model Looking at the multiple linear regression model, the explained variance is 3.05% 13.5% 30.5% 33.5%and as such explains less more variance than the pitch only model. What the above should remind you is that it is not the case that simply putting all the possible predictors into a model will make it a better model. For every predictor you add there is a penalty associated with the Adjusted-R^2 and if the explained variance attributable to the new predictor is not greater than the penalty to overall explained variance then you may actually end up with a worse model despite having more predictors. We will look at model comparison more in the coming months and years but it is always good to keep the rule of parsimony in mind! 14.3.9 Task 9: Making predictions Congratulations! You have successfully constructed a linear model relating trustworthiness to pitch and dispersion and you can think about applying this knowledge to other challenges - perhaps go look at female voices? However, one last thing you might want to do that we will quickly show you is how to make a prediction using the predict() function. One way you use this, though see solutions, is: predict(mod, newdata) where newdata is a tibble with new observations on X (e.g. pitch or dispersion) for which you want to predict the corresponding Y values (mean_rating). Make a tibble with two columns, one called Pitch and one called Dispersion - exactly as spelt in the model. Give Pitch a value of 150 Hz (quite a high voice) and give Dispersion a value of 1100 Hz - somewhere in the middle. Now put that tibble, newdata into the predict() function to run it on the mod_pitchdisp To one decimal place, what is the predicted trustworthiness rating of a person with 150 Hz Pitch and 1100 Hz Dispersion - Hint for Task 9 tibble(Pitch = Value, Dispersion = Value) WAIT! - Didn’t we just say that the mod_pitchdisp model is not as good as mod_pitch. Yep. We did. But we wanted to show you how to enter different predictors into the predict() function. So whilst this is a good teaching aid, you are 100% correct in thinking that in reality we would be better making predictions with just the mod_pitch model as it explains more variance overall. Well done for spotting that! Job Done - Activity Complete! Great! Now you know how to make predictions why not try a few more. Choose some pitch values see what you get! Go record your voice. Extract the pitch using something like PRAAT. Put it in the predict() function. Get a rating of trustworthiness for your voice. Go run a study that has your voice rated for trustworthiness and see how close the model was. Given the explained variance is not great it probably won’t be that close but you start to see how, in principle, the idea of regression and prediction of relationships works. The greater the explained variance of your predictors, the better your prediction will be for a novel participant/observation/event! You should now be ready to complete the Homework Assignment for this lab. The assignment for this Lab is summative and should be submitted through the Moodle Level 2 Assignment Submission Page no later than 1 minute before your next lab. If you have any questions, please post them on the slack forum under the channel #level2_2019. Finally, don’t forget to add any useful information to your Portfolio before you leave it too long and forget. 14.4 Assignment This is a summative assignment. Instructions as to how to access and submit your assignment will be made available during the course. 14.5 Solutions to Questions Below you will find the solutions to the questions for the Activities in this chapter. Only look at them after giving the questions a good try and speaking to the tutor about any issues. 14.5.1 InClass Ativities 14.5.1.1 Task 1 library(&quot;broom&quot;) library(&quot;tidyverse&quot;) ratings &lt;- read_csv(&quot;voice_ratings.csv&quot;) acoustics &lt;- read_csv(&quot;voice_acoustics.csv&quot;) Return to Task 14.5.1.2 Task 2 We are calling the new tibble ratings_tidy. We did not state what to call it as by now you can make that decision yourself. Just remember that when debugging your analysis paths from now on, the tibble names might not match up so, you may need to do a little bit of backtracking to see where tibbles were created. ratings_tidy &lt;- gather(ratings, participant, rating, P1:P28) Return to Task 14.5.2 Task 3 ratings_mean &lt;- ratings_tidy %&gt;% group_by(VoiceID) %&gt;% summarise(mean_rating = mean(rating)) Return to Task 14.5.3 Task 4 joined &lt;- inner_join(ratings_mean, acoustics, &quot;VoiceID&quot;) %&gt;% filter(sex == &quot;M&quot;) Return to Task 14.5.4 Task 5 Figure 14.2: Scatterplot showing the relationship between the voice measures of Dispersion (left) and Pitch (right) and Mean Trustworthiness Rating Return to Task 14.5.5 Task 6 spread() often catches people out. It is a bit like the reverse of gather() It takes in the data and you tell it which values you want spread and by which column - though in the order of data, column, value. Makes sense huh! joined_wide &lt;- joined %&gt;% spread(measures, value) Return to Task 14.5.6 Task 7 Simple Linear Regression - Pitch The pitch model would be written as such: mod_pitch &lt;- lm(mean_rating ~ Pitch, joined_wide) And give the following output: summary(mod_pitch) ## ## Call: ## lm(formula = mean_rating ~ Pitch, data = joined_wide) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.52562 -0.30181 0.04361 0.33398 1.20492 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.921932 0.583801 5.005 2.3e-05 *** ## Pitch 0.015607 0.004052 3.852 0.000573 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6279 on 30 degrees of freedom ## Multiple R-squared: 0.3309, Adjusted R-squared: 0.3086 ## F-statistic: 14.83 on 1 and 30 DF, p-value: 0.0005732 Simple Linear Regression - Dispersion The Dispersion model would be written as such: mod_disp &lt;- lm(mean_rating ~ Dispersion, joined_wide) And give the following output: summary(mod_disp) ## ## Call: ## lm(formula = mean_rating ~ Dispersion, data = joined_wide) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.87532 -0.41300 -0.02435 0.29850 1.52664 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.345300 1.982971 1.183 0.246 ## Dispersion 0.002584 0.001836 1.407 0.170 ## ## Residual standard error: 0.7434 on 30 degrees of freedom ## Multiple R-squared: 0.06191, Adjusted R-squared: 0.03064 ## F-statistic: 1.98 on 1 and 30 DF, p-value: 0.1697 Multiple Linear Regression - Pitch + Dispersion The model with both Pitch and Dispersion as predictors would be written as such: mod_pitchdisp &lt;- lm(mean_rating ~ Pitch + Dispersion, joined_wide) And give the following output: summary(mod_pitchdisp) ## ## Call: ## lm(formula = mean_rating ~ Pitch + Dispersion, data = joined_wide) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.54962 -0.36428 0.04033 0.36327 1.18915 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.444290 1.697362 0.851 0.40179 ## Pitch 0.014855 0.004142 3.586 0.00121 ** ## Dispersion 0.001470 0.001585 0.927 0.36137 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6293 on 29 degrees of freedom ## Multiple R-squared: 0.3501, Adjusted R-squared: 0.3053 ## F-statistic: 7.813 on 2 and 29 DF, p-value: 0.001931 Return to Task 14.5.7 Task 8 A brief explanation: From the models you can see that the Dispersion only model is not actually significant (F(1,30) = 1.98, p = .17) meaning that it is not actually any use as a model. This is backed up by it only explaining 3% of the variance. Looking at the multiple linear regression model which contains both pitch and dispersion we can see that it is a useful model ((F(2,29) - 7.81, P = .002) explaining 30.5% of the variance). However only pitch is a significant predictor in this model and actually the multiple regression model has smaller predictive ability than the pitch alone model. There is an arguement to be made that the pitch alone model is the best model in the current analysis. Return to Task 14.5.7.1 Task 9 Solution Version 1 newdata &lt;- tibble(Pitch = 150, Dispersion = 1100) predict(mod_pitchdisp, newdata) ## 1 ## 5.289819 Solution Version 2 predict(mod_pitchdisp, tibble(Pitch = 150, Dispersion = 1100)) ## 1 ## 5.289819 Solution Version 3 And if you want to bring it out as a single value, say for a write-up, you could do the following This does pop out a warning about a deprecated function meaning that this won’t work in future updates but for now it is ok to use. predict(mod_pitchdisp, tibble(Pitch = 150, Dispersion = 1100)) %&gt;% tidy() %&gt;% pull() %&gt;% round(1) ## Warning: &#39;tidy.numeric&#39; is deprecated. ## See help(&quot;Deprecated&quot;) ## [1] 5.3 Return to Task Chapter Complete! "],
["combining-anova-and-regression-e-g-ancovas.html", "Lab 15 Combining ANOVA and Regression (e.g. ANCOVAs) 15.1 Overview 15.2 PreClass Activity 15.3 InClass Activity 15.4 Assignment 15.5 Solutions to Questions", " Lab 15 Combining ANOVA and Regression (e.g. ANCOVAs) Note: This chapter looks at regression where you have one continuous IV and one categorical IV. More often than not this approach would be called an ANCOVA. However, it can also simply be considered as multitple regression, or the General Linear Model, as really that is what it is all about; just extended to having a mix of continuous and categorical variables. 15.1 Overview Over the last few weeks of the semester we have been really building up our skills on regression and on ANOVAs and now we’ll focus on seeing the link between them. Most places would tell you that they are separate entities but, as you will see from the reading and activities in this lab, they are related. ANOVAs to some degree are just a special type of regression where you have categorical predictors. The question you probably now have is, well, if they are related, can’t we merge them and combine categorical and continuous predictors in some fashion? Yes, yes we can! And that is exactly what we are going to do today whilst learning a little bit about screen time and well-being. The goals of this lab are: to gain a conceptual understanding of how ANOVA and regression are interlinked to get practical experience in analysing continuous and categorical variables in one design to consolidate the wrangling skills we have learnt for the past two years 15.2 PreClass Activity In this final PreClass we have two activities. The first is a very short blog by Prof. Dorothy Bishop that helps draw the links between ANOVA and Regression. The second is really the first part of the InClass activity. It is quite a long InClass, which you will be able to cope with, but we have split it across the PreClass and InClass to allow you some time to get some of the more basic wrangling steps out of the way and so you can come to the class and focus on the actual analysis. It would be worth doing most, if not all, of the activity now. 15.2.1 Read Blog Read this short blog by Prof. Dorothy Bishop on combining ANOVA and regression, and how it all fits together. ANOVA, t-tests and regression: different ways of showing the same thing 15.2.2 Activity Background: Smartphone screen time and wellbeing There is currently much debate (and hype) surrounding smartphones and their effects on well-being, especially with regard to children and teenagers. We’ll be looking at data from this recent study of English adolescents: Przybylski, A. &amp; Weinstein, N. (2017). A Large-Scale Test of the Goldilocks Hypothesis. Psychological Science, 28, 204–215. This was a large-scale study that found support for the “Goldilocks” hypothesis among adolescents: that there is a “just right” amount of screen time, such that any amount more or less than this amount is associated with lower well-being. Much like the work you have been doing, this was a huge survey study with data containing responses from over 120,000 participants! Fortunately, the authors made the data from this study openly available, which allows us to dig deeper into their results. And the question we want to expand on in this exercise is whether the relationship between screen time and well-being is modulated by partcipant’s (self-reported) sex. In other words, does screen time have a bigger impact on males or females, or is it the same for both? The dependent measure used in the study was the Warwick-Edinburgh Mental Well-Being Scale (WEMWBS). This is a 14-item scale with 5 response categories, summed together to form a single score ranging from 14-70. On Przybylski &amp; Weinstein’s page for this study on the Open Science Framework, you can find the participant survey, which asks a large number of additional questions (see page 14 for the WEMWBS questions and pages 4-5 for the questions about screen time). Within the same page you can also find the raw data; however, for the purpose of this exercise, you will be using local pre-processed copies of the data found in the accompanying zip file on Moodle or download from here. Przybylski and Weinstein looked at multiple measures of screen time, but again for the interests of this exercise we will be focusing on smartphone use, but do feel free to expand your skills after by looking at different definitions of screen time. Overall, Przybylski and Weinstein suggested that decrements in wellbeing started to appear when respondents reported more than one hour of daily smartphone use. So, bringing it back to our additional variable of sex, our question is now, does the negative association between hours of use and wellbeing (beyond the one-hour point) differ for boys and girls? Let’s think about this in terms of the variables. We have: a continuous\\(^*\\) DV, well-being; a continuous\\(^*\\) predictor, screen time; a categorical predictor, sex. Note: these variables (\\(^*\\)) are technically only quasi-continuous inasmuch as that only discrete values are possible. However, there are a sufficient number of discrete categories in our data that we can treat the data as effectively continuous. Now, in terms of analysis, what we are effectively trying to do is to estimate two slopes relating screen time to well-being, one for girls and one for boys, and then statistically compare these slopes. Sort of like running a correlation for boys, a correlation for girls, and comparing the two. Or alternatively, where you would run a regression (to estimate the slopes) but also one where you would need a t-test (to compare two groups). But the expressive power of regression allows us to do this all within a single model. Again, as we have seen building up to this lab, an independent groups t-test is just a special case of ordinary regression with a single categorical predictor; ANOVA is just a special case of regression where all predictors are categorical. But remember, although we can express any ANOVA design using regression, the converse is not true: we cannot express every regression design in ANOVA. As such people like regression, and the general linear model, as it allows us to have any combination of continuous and categorical predictors in the model. The only inconvenience with running ANOVA models as regression models is that you have to take care in how you numerically code the categorical predictors. We will use an approach called deviation coding which we will look at today later in this lab. Let’s Begin! 15.2.3 Loading in the data As always we will need to load in the tidyverse package and load in the data from the accompanying csv files, wellbeing.csv, participant_info.csv, and screen_time.csv. Create a new R Markdown file and put the csv files in the same directory with the Rmd file you’ve just created. Then load them in as follows (the solution is at the end of the chapter): pinfo stores participant_info.csv wellbeing stores wellbeing.csv screen stores screen_time.csv The Data Take a look at the resulting tibbles pinfo, wellbeing, and screen. The wellbeing tibble has information from the WEMWBS questionnaire; screen has information about screen time use on weekends (variables ending with we) and weekdays (variables ending with wk) for four types of activities: using a computer (variables starting with Comph; Q10 on the survey), playing video games (variables starting with Comp; Q9 on the survey), using a smartphone (variables starting with Smart; Q11 on the survey) and watching TV (variables starting with Watch; Q8 on the survey). If you want more information about these variables, look at the items 8-11 on pages 4-5 of the the PDF version of the survey on the OSF website. note that sex is coded as male = 1, female = 0. also, Serial is common across all datasets and is the participant ID. 15.2.4 Compute the well-being score for each participant Ok, our data is in and we need to create a well-being score for each participant on each item of the WEMWBS. To do this, and to calculate the well-being score for each participant, we simply sum all the items together for that participant. Write code to create a new tibble, called wemwbs, with two variables: Serial, and tot_wellbeing, which is the the total WEMWBS score for each participant. Hint to compute scores Step 1: reshape table from wide to long Step 2: group_by(); summarise(tot_wellbeing = …) Alternatively, mutate on the sum of all columns, then select the two needed. 15.2.5 Visualising Screen time on all technologies Great, so we have the well-being scores sorted out, we now need to think about the screen time usage and whether it is being used on a weekday or a weekend. As always, to get an idea of the data, it is often very useful to visualize the distributions of variables before proceeding with the analysis. Try recreating this figure based on the data in screen. Note that this will require some tidying of the data in screen: You’ll first need to gather the screen tibble into long format and then break apart the column names into two separate columns, one for the name of the variable (Watch, Comp, Comph, and Smart) and the other for part of the week (wk and we). This is going to take using the separate() function which we haven’t used yet but we think you can manage. Next, you’ll need to alter the values of the variables to reflect the more descriptive text that appears in the plot (e.g., &quot;Watch&quot; becomes &quot;Watching TV&quot;; &quot;wk&quot; becomes &quot;Weekday&quot;). This is a recode() issue which you have done a number of times. This is quite a tricky bit of wrangling which we think you are capable of but, do not be put off if you can’t quite get it yet. The code is at the end of the chapter for you to use once you have had a shot at it. Figure 15.1: Count of the hours of usage of different types of social media at Weekdays and Weekends Hints on Wrangling Steps 1 and 2 Step 1 gather() the data in screen into three columns: Serial, var, hours ?separate() in the console seperate(data, column_name_containing_variables_to_split, c(“column_1_to_create”,“column_2_to_create”), “character_to_split_by”) each variable (category) has an underscore in its name. Use that to split it. I.e. Comph_we will get split into Comph and we Step 2 data %&gt;% mutate(new_variable_name = recode(old_variable_name, “wk” = “Weekday”, “we” = “Weekend”)) 15.2.6 Visualising the Screen time and Well-being relationship Brilliant, that is truly excellent work and you should be really pleased with yourself. Looking at the figures, it would appear that there is not much difference between screen time use of smartphones in weekend and weekdays so we could maybe collapse that variable together later when we come to analyse it. Overall, people tend to be using all the different technologies for a peak around 3 hours, and then each distribution tails off as you get longer exposure suggesting that there are some that stay online a long time. Video games is the exception where there is a huge peak in the first hour and then a tailing off after that. But first, another visualisation. We should have a look at the relationship between screen time (for the four different technologies) and measures of well-being. This relationship looks like this shown below and the code to recreate this figure is underneath: Figure 15.2: Scatterplot showing the relationship between screen time and mean well-being across four technologies for Weekdays and Weekends At the start we said we were only going to focus on smartphones. So looking at the bottom left of the figure we could suggest that smartphone use of more than 1 hour per day is associated with increasingly negative well-being the longer screen time people have. This looks to be a similar effect for Weekdays and Weekends, though perhaps overall well-being in Weekdays is marginally lower than in Weekends (the line for Weekday is lower on the y-axis than Weekends). This makes some sense as people tend to be happier on Weekends! Sort of makes you wish we had more of them right? Job Done - Activity Complete! That is great work today and it just shows you how far you have come with your wrangling skills over the last couple of years. We will pick up from here in the lab during the week where we will start to look at the relationship in boys and girls. Don’t forget to make any notes for yourself that you think will be good to remember - rounding off your Portfolio and bank on skills that you have built up. Any questions or problems, as always post them on the forums or bring them to the lab for discussion. 15.3 InClass Activity Continued: Smartphone screen time and wellbeing We are going to jump straight into this as you will have already started the analysis in the PreClass activity but as a quick recap, there is currently much debate surrounding smartphones and their effects on well-being, especially with regard to children and teenagers. In the PreClass, and continuing today, we have been looking at data from this recent study of English adolescents: Przybylski, A. &amp; Weinstein, N. (2017). A Large-Scale Test of the Goldilocks Hypothesis. Psychological Science, 28, 204–215. This was a large-scale study that found support for the “Goldilocks” hypothesis among adolescents: that there is a “just right” amount of screen time, such that any amount more or less than this amount is associated with lower well-being. The dependent measure used in the study was the Warwick-Edinburgh Mental Well-Being Scale (WEMWBS). This is a 14-item scale with 5 response categories, summed together to form a single score ranging from 14-70, and we have been working with a version of some of the available data which can be found in the accompanying zip file on Moodle or download from here Przybylski and Weinstein looked at multiple measures of screen time, but again for the interests of this exercise we will be focusing on smartphone use only, but do feel free to expand your skills after by looking at different definitions of screen time. Overall, Przybylski and Weinstein suggested that decrements in wellbeing started to appear when respondents reported more than one hour of daily smartphone use. So, bringing it back to our additional variable of sex, our question is now, does the negative association between hours of use and wellbeing (beyond the one-hour point) differ for boys and girls? Let’s think about this in terms of the variables. We have: a continuous DV, well-being; a continuous predictor, screen time; a categorical predictor, sex. And to recap in terms of analysis, what we are effectively trying to do is to estimate two slopes relating screen time to well-being, one for girls and one for boys, and then statistically compare these slopes. Again, as we have seen building up to this lab, an independent groups t-test is just a special case of ordinary regression with a single categorical predictor; ANOVA is just a special case of regression where all predictors are categorical. But remember, although we can express any ANOVA design using regression, the converse is not true: we cannot express every regression design in ANOVA. As such people like regression, and the general linear model, as it allows us to have any combination of continuous and categorical predictors in the model. The only inconvenience with running ANOVA models as regression models is that you have to take care of how you numerically code the categorical predictors. We will use an approach called deviation coding which we will look at today later in this lab. Let’s Begin! 15.3.1 Smartphone and well-being for boys and girls Continuing from where we left on in the PreClass, so far we have been matching what the original authors suggested we would find in the data, this drop off in self-reported well-being for longer exposures of smart-phone use (i.e. &gt;1 hour). However, we said we wanted to look at this in terms of males and females, or boys and girls really, so we need to do a bit more wrangling. Also, as above we said there seemed to be only a small difference between Weekday and Weekends so, we will collapse weekday and weekend usage in smartphones. Create a new table, smarttot, that takes the information in screen2 and keeps only the smarthphone usage. Now, create an average smartphone usage for each participant, called tothours, using a group_by and summarise, regardless of whether or not it is the weekend or weekday, i.e. the mean number of hours per day of smartphone use (averaged over weekends/weekdays) for that participant. Now, create a new tibble smart_wb that only includes participants from smarttot who used a smartphone for more than one hour per day each week, and then combine this tibble with the information in the wemwbs tibble and the pinfo tibble. The finished table should look something like this (we are only showing the first 5 rows here): Serial tothours tot_wellbeing sex minority deprived 1000003 2.0 41 0 0 1 1000004 2.5 47 0 0 1 1000005 3.5 32 0 0 1 1000006 2.0 29 0 0 1 1000008 1.5 42 0 0 1 Hints for Wrangling Steps Step 1 filter(“Using Smartphone”) to keep only smartphone use Step 2 group_by(Participant) %&gt;% summarise(tothours = mean()) Step 3 filter(), inner_join() hours greater than 1 what is the common column to join each time by? Participant? 15.3.2 Visualising and Interpreting the relationship between smartphone use and wellbeing by sex Excellent! Lots of visualisation and wrangling in the PreClass and today but that is what we have been working on and building our skills on up to this point so, we are coping fine! Just a couple more visualisation and wrangles to go before we run the analysis (the easy part!) Using the data in smart_wb create the following figure. You will need to first calculate the mean wellbeing scores for each combination of sex and tothours, and then create a plot that includes separate regression lines for each sex. Next, or if you just want to look at the figure and not create it, make a brief interpretation of the figure. Think about it in terms of who has the overall lower mean wellbeing score and also are both the slopes the same or is one more negative, one more positive, etc. Figure 15.3: Scatterplot and slopes for relationships between total hours and mean wellbeing score for boys (cyan) and girls (red) 15.3.3 A side point on mean centering and deviation coding Last bit of wrangling, I promise, before the analysis. Here, we will introduce something that is worth doing to help with our interpretation. You can read up more on this later, and we will cover it in later years more in-depth, but when you have continuous variables in a regression, it is often sensible to transform them by mean centering them which has two very useful outcomes: the intercept of the model now shows the predicted value of \\(Y\\) for the mean value of the predictor variable rather than the predicted value of \\(Y\\) at the zero value of the unscaled variable as it normally would; if there are interactions in the model, any lower-order effects (e.g. main effects) can be interpreted as they would have been, had it been simply an ANOVA. These steps seem rather worthwhile in terms of interpretation and the process is really straightforward. You can mean center a continuous predictor, for example X, simply by subtracting the mean from each value of the predictor: i.e.X_centered = X - mean(X). A second very useful thing to do that aids the interpretation is to convert your categorical variables into what is called deviation coding. Again, we are going to focus more on this in L3 but it is good to hear the term in advance as you will see it from time to time. Again, all this does is to allow you to interpret the categorical predictors as if it were an ANOVA. We are going to do both of these steps, mean centering of our continuous variable and deviation coding of our categorical variable. Here is the code to do it. Copy it and run it but be sure that you understand what it is doing. totothours_c is the mean centered values of tothours sex_c is the deviation coding of the sex column (sex) where male, which was coded as 1, is now coded as .5, and female is now coded as -.5 instead of 0. The ifelse() basically says, if that column you want me to look at, says this (e.g. sex == 1), then I will put a .5, otherwise (or else) I will put a -.5. smart_wb &lt;- smart_wb %&gt;% mutate(tothours_c = tothours - mean(tothours), sex_c = ifelse(sex == 1, .5, -.5)) %&gt;% select(-tothours, -sex) 15.3.4 Estimating model parameters Superb! And now finally, after all that wrangling and visualisation, the models. Finally, we are going to see if there is statistical support for our above interpretation of the Figure 15.3, where we saw that overall, girls have lower well-being and that they are affected more by prolonged smartphone usage than boys are. Just to recap, the previous authors have already looked at smartphone usage and wellbeing but, we want to look at whether it has more of an impact in girls than boys, or boys than girls, or about the same. The multiple regression model, from the general linear model, for this analysis would be written as: \\(Y_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{3i} + e_i\\) where: \\(Y_i\\) is the wellbeing score for participant \\(i\\); \\(X_{1i}\\) is the mean-centered smartphone use predictor variable for participant \\(i\\); \\(X_{2i}\\) is gender, where we used deviation coding (-.5 = female, .5 = male); \\(X_{3i}\\) is the interaction between smartphone use and gender (\\(= X_{1i} \\times X_{2i}\\)) You have seen multiple regression models before in R and they usually take a format something like, y ~ a + b. The one for this analysis is very similar but with one difference, we need to add the interaction. To do that, instead of saying a + b we do a * b. This will return us the effects of a and b by themselves as well as the interaction of a and b. Just like you would in an ANOVA but here one of the variables is continuous and one is categorical. With that in mind, using the data in smart_wb, use the lm() function to estimate the model for this analysis where we predict tot_wellbeing from mean centered smartphone usage (tothours_c) and the deviation coded sex (sex_c) Hints on model R formulas look like this: y ~ a + b + a:b where a:b means interaction This can be written in short form of y ~ a * b Next, use the summary() function on your model output to view it. The ouput should look as follows: ## ## Call: ## lm(formula = tot_wellbeing ~ tothours_c * sex_c, data = smart_wb) ## ## Residuals: ## Min 1Q Median 3Q Max ## -36.881 -5.721 0.408 6.237 27.264 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 47.43724 0.03557 1333.74 &lt;2e-16 *** ## tothours_c -0.54518 0.01847 -29.52 &lt;2e-16 *** ## sex_c 5.13968 0.07113 72.25 &lt;2e-16 *** ## tothours_c:sex_c 0.45205 0.03693 12.24 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.135 on 71029 degrees of freedom ## Multiple R-squared: 0.09381, Adjusted R-squared: 0.09377 ## F-statistic: 2451 on 3 and 71029 DF, p-value: &lt; 2.2e-16 15.3.5 Final Interpretations Finally, just some quick interpretation questions to round off all our work! To help you, here is some info: The intercept for the male regression line can be calculated by: the Intercept + (the beta of sex_c * .5) The slope of the male regression line can be calculated by: the beta of the tothours_c + (the beta of interaction * .5) The intercept for the female regression line can be calculated by: the Intercept + (the beta of sex_c * -.5) The slope of the female regression line can be calculated by: the beta of the tothours_c + (the beta of interaction * -.5) Look at your model output in the summary() and try to answer the following questions. The solutions are below. The interaction between smartphone use and gender is shown by the variable tothours_c sex_c tothours_c:sex_c, and this interaction was significant nonsignificant at the \\(\\alpha = .05\\) level. To two decimal places, the intercept for male participants is: To two decimal places, the slope for male participants is: To two decimal places, the intercept for female participants is: To two decimal places, the slope for female participants is: As such, given the model of Y = intercept + (slope * X) where Y is wellbeing and X is total hours on smartphone, what would be the predicted wellbeing score for a male and a female who use their smartphones for 8 hours. Give your answer to two decimal places: Male: Female: And finally, what is the most reasonable interpretation of these results? smartphone use harms girls more than boys smartphone use harms boys more than girls there is no evidence for gender differences in the relationship between smartphone use and well-being smartphone use was more negatively associated with wellbeing for girls than for boys Job Done - Activity Complete! You should now be ready to complete any future Homework Assignment related to ANCOVAs. We will hold off from a formula assignment on this approach just now and we will look at it again later in the semester, but for now, if you have any questions, please post them on the slack forum under the channel #level2_2019. Finally, don’t forget to add any useful information to your Portfolio before you leave it too long and forget. 15.4 Assignment We will hold off from an assignment (formative or summative) on ANCOVAs just now and we will look at it again later in the semester, but for now, if you have any questions, please post them on the slack forum under the channel #level2_2019. Finally, don’t forget to add any useful information to your Portfolio before you leave it too long and forget. 15.5 Solutions to Questions Below you will find the solutions to the questions for the PreClass and InClass activities for this chapter. Only look at them after giving the questions a good try and speaking to the tutor about any issues. 15.5.1 PreClass Activities 15.5.1.1 Loading the Data library(&quot;tidyverse&quot;) pinfo &lt;- read_csv(&quot;participant_info.csv&quot;) wellbeing &lt;- read_csv(&quot;wellbeing.csv&quot;) screen &lt;- read_csv(&quot;screen_time.csv&quot;) Return to Task 15.5.1.2 Compute the well-being score for each participant Both of these solutions would produce the same output. Version 1 bit quicker in terms of coding and reduces chance of error by perhaps forgetting to include a specific column wemwbs &lt;- wellbeing %&gt;% gather(&quot;var&quot;, &quot;score&quot;, -Serial) %&gt;% group_by(Serial) %&gt;% summarise(tot_wellbeing = sum(score)) Version 2 this one is a bit slower wemwbs &lt;- wellbeing %&gt;% mutate(tot_wellbeing = WBOptimf + WBUseful + WBRelax + WBIntp + WBEnergy + WBDealpr + WBThkclr + WBGoodme + WBClsep + WBConfid + WBMkmind + WBLoved + WBIntthg + WBCheer) %&gt;% select(Serial, tot_wellbeing) %&gt;% arrange(Serial) Return to Task 15.5.1.3 Visualising Screen time on all technologies ## screen time screen_long &lt;- screen %&gt;% gather(&quot;var&quot;, &quot;hours&quot;, -Serial) %&gt;% separate(var, c(&quot;variable&quot;, &quot;day&quot;), &quot;_&quot;) screen2 &lt;- screen_long %&gt;% mutate(variable = recode(variable, &quot;Watch&quot; = &quot;Watching TV&quot;, &quot;Comp&quot; = &quot;Playing Video Games&quot;, &quot;Comph&quot; = &quot;Using Computers&quot;, &quot;Smart&quot; = &quot;Using Smartphone&quot;), day = recode(day, &quot;wk&quot; = &quot;Weekday&quot;, &quot;we&quot; = &quot;Weekend&quot;)) ggplot(screen2, aes(hours)) + geom_bar() + facet_grid(day ~ variable) Figure 15.4: Count of the hours of usage of different types of social media at Weekdays and Weekends Return to Task 15.5.1.4 Visualising the Screen time and Well-being relationship dat_means &lt;- inner_join(wemwbs, screen2, &quot;Serial&quot;) %&gt;% group_by(variable, day, hours) %&gt;% summarise(mean_wellbeing = mean(tot_wellbeing)) ggplot(dat_means, aes(hours, mean_wellbeing, linetype = day)) + geom_line() + geom_point() + facet_wrap(~variable, nrow = 2) Figure 15.5: Scatterplot showing the relationship between screen time and mean well-being across four technologies for Weekdays and Weekends Return to Task 15.5.2 InClass Activities 15.5.2.1 Smartphone and well-being for boys and girls Solution Steps 1 to 2 smarttot &lt;- screen2 %&gt;% filter(variable == &quot;Using Smartphone&quot;) %&gt;% group_by(Serial) %&gt;% summarise(tothours = mean(hours)) Solution Step 3 smart_wb &lt;- smarttot %&gt;% filter(tothours &gt; 1) %&gt;% inner_join(wemwbs, &quot;Serial&quot;) %&gt;% inner_join(pinfo, &quot;Serial&quot;) Return to Task 15.5.2.2 Visualise and Interpreting the relationship of smartphone use and wellbeing by sex The Figure smart_wb_gen &lt;- smart_wb %&gt;% group_by(tothours, sex) %&gt;% summarise(mean_wellbeing = mean(tot_wellbeing)) ggplot(smart_wb_gen, aes(tothours, mean_wellbeing, color = factor(sex))) + geom_point() + geom_smooth(method = &quot;lm&quot;) Figure 15.6: Scatterplot and slopes for relationships between total hours and mean wellbeing score for boys (cyan) and girls (red) A brief Interpretation Girls show lower overall well-being compared to boys. In addition, the slope for girls appears more negative than that for boys; the one for boys appears relatively flat. This suggests that the negative association between well-being and smartphone use is stronger for girls. Return to Task 15.5.2.3 Estimating model parameters This is the chunk we gave in the materials. smart_wb &lt;- smart_wb %&gt;% mutate(tothours_c = tothours - mean(tothours), sex_c = ifelse(sex == 1, .5, -.5)) %&gt;% select(-tothours, -sex) and the model would be specified as: mod &lt;- lm(tot_wellbeing ~ tothours_c * sex_c, smart_wb) or alternatively mod &lt;- lm(tot_wellbeing ~ tothours_c + sex_c + tothours_c:sex_c, smart_wb) and the output called by: summary(mod) ## ## Call: ## lm(formula = tot_wellbeing ~ tothours_c + sex_c + tothours_c:sex_c, ## data = smart_wb) ## ## Residuals: ## Min 1Q Median 3Q Max ## -36.881 -5.721 0.408 6.237 27.264 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 47.43724 0.03557 1333.74 &lt;2e-16 *** ## tothours_c -0.54518 0.01847 -29.52 &lt;2e-16 *** ## sex_c 5.13968 0.07113 72.25 &lt;2e-16 *** ## tothours_c:sex_c 0.45205 0.03693 12.24 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.135 on 71029 degrees of freedom ## Multiple R-squared: 0.09381, Adjusted R-squared: 0.09377 ## F-statistic: 2451 on 3 and 71029 DF, p-value: &lt; 2.2e-16 Return to Task 15.5.2.4 Final Interpretations The interaction between smartphone use and gender is shown by the variable tothours_c:sex_c, and this interaction was significant at the \\(\\alpha = .05\\) level, meaning that there is an significant interaction between sex and hours of smartphone usage on wellbeing To two decimal places, the intercept for male participants is: 50.01 To two decimal places, the slope for male participants is: -0.32 To two decimal places, the intercept for female participants is: 44.87 To two decimal places, the slope for female participants is: -0.77 As such, given the model of Y = intercept + (slope * X) where Y is wellbeing and X is total hours on smartphone, what would be the predicted wellbeing score for a male and a female who use their smartphones for 8 hours. Give your answer to two decimal places: Male: 47.45 Female: 38.71 And finally, the most reasonable interpretation of these results is that smartphone use was more negatively associated with wellbeing for girls than for boys. Return to Task Chapter Complete! "],
["reflection-semester-2.html", "Lab 16 Reflection - Semester 2 16.1 Overview 16.2 PreClass Activity 16.3 InClass Activity 16.4 Assignment 16.5 Solutions to Questions", " Lab 16 Reflection - Semester 2 16.1 Overview As in Semester 1 we have covered a lot of material in these labs and now would be a good time to stop, recap, and reflect on what we have learned. As such this last chapter is again about looking back at what you have learned, testing your skills, resolving issues, and looking at other cool applications of R that have not been covered in this lab series. 16.2 PreClass Activity As we are reflecting in this lab, your PreClass activities this time are: Review the labs from this semester and note any issues you have with the elements covered - both in terms of concepts and code. Post these issues on the Practical labs forum on Moodle and bring them to the lab. 16.3 InClass Activity Like in the PreClass we want to spend sometime reflecting on what we have learnt and as such this InClass is about looking at ideas, concepts, and codes, that you have had issues with and seeing if we can resolve those issues. It would be particularly worthwhile spending sometime looking at aspects of working with the GLM and decomposition matrices as this will make up much of the course next year. We will also look at some other interesting things you can do in R, should you wish to expand your own knowledge and skills. Below is the list we looked at in Semester 1 but, I am sure we have found loads more as the year has gone on, such as: New redoc package, by Ross Noam, for collaborative editing of Word documents (https://noamross.github.io/redoc/) faux for the creation of simulated data (https://debruine.github.io/faux/) (DeBruine 2019) checkpoint for timestamping the version of the packages you are using in your code mydata &lt;- read.delim(&quot;clipboard&quot;) for creating a dataframe from your copy and paste function on your keyboard. R.Online to compare codes across different versions of R (https://srv.colinfay.me:1001/) Hack Your Data Beautiful (https://psyteachr.github.io/hack-your-data/index.html) - a workshop run by a team of postgraduates in Psychology @ Glasgow that shows a range of excellent and interesting skills and applications. Previous - Popping out the Source Window to make working easier - &lt;a href=&quot;https://support.rstudio.com/hc/en-us/articles/207126217-Using-Source-Windows&quot; target = &quot;_blank&quot;&gt;Using Source Windows&lt;/a&gt; Analysing Twitter data with the rtweet package (Kearney 2019) Animating plots with the ggganimate package (Pedersen and Robinson 2019) Creating quickfire quizzes with the webex (Barr and DeBruine 2019) Make your own memes using the meme package (Yu 2019) The Hex Sticker Memory game and the background behind it Creating interactive plots using ggplot and plotly Even funkier visualtions using the ggforce package - check out facet_zoom() (Pedersen 2019) Many diverse fields are now using R and this is a good example: R for Journalists Using the knitr::read_chunk() function to call R script code through R Markdown (Xie 2019) 16.4 Assignment This is a summative assignment. Instructions as to how to access and submit your assignment will be made available during the course. 16.5 Solutions to Questions Instructions as to how to access the solution to this lab will be made available during the course. A References "],
["references.html", "A References", " A References "]
]
