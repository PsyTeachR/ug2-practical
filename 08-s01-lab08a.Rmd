# APES - Alpha, Power, Effect Sizes, Sample Size

## Overview

Up until now we have mainly spent time on data-wrangling, understanding probability, visualising our data, and more recently, running inferential tests, i.e. t-tests. In the lectures you have also started to learn about additional aspects of inferential testing and trying to reduce certain types of error in your analyses:

* **Type I error** - rejecting the null hypothesis when it is true (otherwise called **alpha** or $\alpha$). Probably better recalled as **False Positives** 
* **Type II error** - retaining the null hypothesis when it is false (otherwise called **beta** or $\beta$). Probably better recalled as **False Negatives**

Building from there we have started to discuss the idea of **power** ($1-\beta$) which, from your lectures and from the pre-class reading, you should understand is the long-run probability of correctly rejecting the null hypothesis for a fixed effect size and fixed sample size; i.e. correctly concluding there is an effect when the effect is real. The concept of power is useful for making decisions when you plan a study. **The higher the power of your planned study, the better**, with the field standard proposed as $power >= .8$ for the targeted effect size and sample size. In fact, Registered Reports are often required to have a power of at least $power >= .9$. Keep in mind that power is defined as the probability of rejecting the null for a fixed effect size and fixed sample size. For a given sample size, power will be higher for effects that you assume to be larger (they are easier to detect). For a given effect size, power will be higher for a larger sample. Because you have little control over the size of the effect, typically you can increase the power of your study by either (1) increasing the size of your sample or (2) reducing sources of noise and measurement error in your study. Because power depends on several variables, it is useful to think of power as a *function* rather than as a single fixed quantity.

Psychological research has been criticised for neglecting power during study planning, resulting in many underpowered studies. Low power, combined with undisclosed analytic flexibility and publication bias, is thought to be a key issue in the replication crisis within the field. As such there may be a large number of studies where the null hypothesis has been rejected when it should not have been; the field becomes noisy at that point and you are unsure which studies will replicate.  It is issues like this that led us to redevelop our courses and why we really want you to understand power as much as possible.

When planning a study any good researcher will consider four key elements, the **APES**:

* **alpha** - most commonly thought of as the significance level; usually set at $\alpha = .05$
* **power** - the probability of rejecting the null for a given effect size and sample size, with .8 usually cited as the minimum power you should aim for;
* **effect size** - size of the asssociation or difference you are trying to detect;
* **sample size** - the number of observations (usually, participants, but sometimes also stimuli) in your study.

And the beautiful thing is that **if you know any three of these elements then you can calculate the fourth**. The two most common calculations prior to a study would be:

1. to determine the appropriate sample size required to reject the null, with high probability, for the effect size that you are interested in. That is, you fix $\alpha$, power, and effect size, and solve for the sample size. Generally, **the smaller the assumed effect size, the more participants you will need**, assuming power and alpha are held constant at **.8** and **.05** respectively.
2. to determine the smallest effect size you can reliably detect given your sample size. That is, you know everything except the effect size. For example, say you are using an open dataset and you know they have run 100 participants, you can't add any more participants, and you want to know what is the minimum effect size I could detect in this dataset with some probability.

**Note:** Most papers would discourage you from calculating what is called Observed or Post-Hoc Power. This is where you calculate the power after running the study, based on your effect size and sample size. Similarly, this would be running an analysis on an open dataset, finding the outcome, and then calculating the power based on the outcome. Avoid this. You can read more about why, here, in your own time if you like: <a href="http://daniellakens.blogspot.com/2014/12/observed-power-and-what-to-do-if-your.html" target = "_blank"> Lakens (2014) Observed Power, and what to do if your editor asks for post-hoc power analyses</a>
